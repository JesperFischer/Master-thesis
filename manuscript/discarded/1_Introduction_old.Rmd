# Introduction

Most scientific inquiry revolves around measurements of the physical world; may that be the time it takes for a cup to fall to the ground or the time it takes for a person to react to a visual stimulus on a computer screen. These measurements will be associated with uncertainty as repeatedly measuring the same thing, will result in different measurements. This means that one of the fundamental things that scientific inquiry and theories rests on is uncertainty. It is therefore also one of the roles of science to quantify such uncertainties.

In this thesis I will investigate uncertainty handling in cognitive science, while providing ways to properly account for these uncertainties when analyses are conducted. To do this I will rely on Monte Carlo simulations which provides a robust method for accounting for uncertainties when analyses and models are deployed. In particular the thesis will introduce a partially novel way of testing and validating the parameters of models in cognitive science. The thesis will further demonstrate this, with a focus on the psychometric function, a commonly used cognitive model in cognitive science. It will be shown that the parameters of the model and their uncertainties can be reduced by several different interventions and assumptions, including optimization of the experimental design, but also by incorporating additional information already available in most experiments. The thesis will demonstrate how modeling and incorporation of such assumptions can improve estimation of parameters of already published data. Lastly using this re-analysis of published data, the thesis will highlight opportunities to conduct power analyses utilizing a novel modeling framework that can help make  power analyses for a particular model more rigorous by incorporating the propagation of uncertainty. Comparison of this novel power analysis framework will be compared to popular tools such as G\*power.

## *Uncertainties in science and examples from physics*

<!-- This section still needs work -->

Science can be thought of as a systematic way to organize knowledge in hierarchies, leading to testable hypotheses. Knowledge can be hard to define, but most often it is something that is achieved though experience. Imaging a cup being dropped, most people will have the knowledge that it will fall towards the ground and reach our foot at a particular speed, because of our previous experiences with dropping a cup. This is to say that knowledge is the relationships that we believe to be true with differing amounts of certainty. The reality is that even though we might say that we are completely certain of events, i.e. know, that the cup will fall towards the ground and reach it at a particular speed, this is in an assumption that is true only most of the time. Given that the natural world is bounded on probabilities, complete certainty is unwarranted, both in the assumption of the cup hitting our foot, but especially the speed at which it hits our foot. Most of the time the probabilistic nature of the natural world stems from the uncertainties during measurement or perhaps unseen events. The interest here is not in the unseen events, but instead in the predictability and (un)certainty of the expected. Taking the falling cup as an example, in science we would normally not be interested in the probability that the cup will hit the ground, but instead in the acceleration of the cup and the uncertainty in this estimate. What scientists have shown is that objects dropped on earth will accelerate towards the ground with an acceleration of $9.81\frac{m}{s^2}$ [@johannes_fundamentals_2009]. However, this number does not mean anything without an estimate of the uncertainty, while also accounting for the assumptions that are entailed with these numbers. The first proposition is well studied and the 95% confidence interval of the value is estimated to be $[9.78 ; 9.84]\frac{m}{s^2}$ [@johannes_fundamentals_2009]. The second proposition is also quite well studied as we know that the density of the medium that the cup is dropped in is important, but also the shape and weight of the cup if dropped outside of a vacuum. In order to estimate this acceleration, measurements have to be made. These measurements include the distance the cup travels and the time it takes. With these measures of distance and time, uncertainty is introduced and propagated to get the an estimate for the acceleration, but also the uncertainty associated with it. There are 2 main points of the example which this thesis will explore, firstly uncertainties are organized in hierarchies and are just as important as beliefs as without one, the other is meaningless. Secondly, taking these uncertainties seriously and herein estimating and propagating them should not be a choice or something that can be avoided, but a necessity of all scientific endeavors, where they can be quantified or at least approximated. The thesis will show how issues could arise in different aspects of cognitive science, if uncertainty is not properly handled. After highlighting these potential issues, the thesis will provide possible ways incorporating and minimizing these uncertainties by means of simulations. The goal of this thesis is to illuminate the often overlooked uncertainties in the data collected on human behavior and cognition while providing ways of accounting for it.

The thesis will argue that accounting for uncertainties is more important than ever, especially in research of complex systems such as humans, as computational resources have made it possible to easily develop more sophisticated analyses and models that have dependencies on lower-level analyses. The dependent structure makes the need for proper uncertainty handling even more imperative, as without propagating uncertainties the resulting estimates / beliefs will be overly confident. Furthermore, these computational resources do allow for uncertainty propagation without a deep understanding of the underlying mathematics, making it accessible to most researchers with some coding experience. To effectively communicate both the statistical models as well as the underlying uncertainties associated with doing computations on data, the thesis will start by exploring different types of uncertainty. Next the thesis will be investigating a particular cognitive model, the psychometric function, used in many sub fields of cognitive science. This model will be examined and used to show how a novel way of validating cognitive models can be used to gain more information about the structure of the uncertainty in the parameters of the model.

## *Levels of uncertainty and uncertainty propagation*

I will here broadly define 3 different types of uncertainty which are going to be used throughout the thesis, measurement, estimation, and test-retest  uncertainty see figure 1 and 2 for a visualization. These definitions are not exhaustive and will be centered around how experimental studies in cognitive science are conducted, from data collection to data analysis. Before examining these three types of uncertainty it is imperative to acknowledge that uncertainties can be defined in hierarchies and that uncertainty propagates through these hierarchies. This uncertainty propagation means that as calculations are done based on measures with uncertainty, the uncertainty propagates to the results of the calculations. In this thesis I will be using simulations to show how uncertainty propagation can be understood and handled without a need for rigorous mathematical proofs. For a more mathematical treatment see [@saccenti_corruption_2020].

The lowest level of uncertainty is in the measurements themselves i.e. measurement uncertainty. Measurement uncertainty reflects the uncertainty in how well one can for instance measure the reaction time on a computer or the time it took a falling cup to reach the ground. This level of uncertainty is often neglected in cognitive science when applying statistical models, because they are sometimes thought to be minuscule as in the case of reaction time tasks, which may or may not be true given the experiment setup [@holden_accuracy_2019; @crocetta_problem_2015; @ohyanagi_solution_2010]. This is not to say that cognitive scientists do not care about them, as moving towards more sophisticated measurement methods is an ongoing endeavor. For instance, using better and more sophisticated computers to measure reaction times commonly found in cognitive science experiments [@crocetta_problem_2015]. Minimizing this kind of uncertainty most often revolves around getting better tools to measure the variable(s) of interest. Interestingly some variables of interest in cognitive science do not have a explicit quantification of measurement uncertainty. One of the main instances coming to mind is the use of questionnaires. Many of these questionnaires try to quantify a latent construct such as mental health conditions i.e. depression or anxiety. Many of the main questionnaires used to assess these latent constructs use several questions that are then added together to give a score of said mental health condition without a quantification of the uncertainty on the latent construct because no uncertainty is quantified on each question [@xiao_psychometric_2023; @cohen_perceived_1994; @kroenke_phq-9_2001; @johnson_psychometric_2019]. 

In order to contextualize these forms of uncertainty i will consider an idealized example of a fictional researcher wanting to understand the relationship between reaction times and stress while incorporating all types of uncertainty. To do this the researcher conducts an experiment where participants are measured several times under different conditions to introduce stress. In this example both of these measures have associated uncertainty, see the individual data points in figure 1. The relationship between reaction times and stress is determined by the slope of the regression line depicted in Figure 1.

```{r figure1, fig.width = 7, fig.height = 4, warning = F, message = F, echo = F, fig.cap = "**Figure 1 Measurement and Estimation uncertainty;** the figure displays a linear regression between two measurements of for instance reaction time and stress with measurement uncertainty depicted as vertical and horizontal error bars on individual points. The mean of the regression line with and without propagated uncertainty is highlighted in grey and dark green respectively. Lastly a prediction interval is depicted as the shaded area around the mean of the regression line with and without propagated uncertainty again in grey and green respectively. The difference between the green and grey lines are therefore the difference between accounting for measurement uncertainty and not accounting for it."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_1_measurement_uncertainty.png")), scale = 1)
```

The next level of uncertainty is when a particular model is fit to some data, or more broadly when calculations are done on data with uncertainty, this uncertainty will be referred to as estimation uncertainty. Estimation uncertainty is most often quantified by the statistical model be that the standard error of a regression coefficient or the width of a posterior distribution of a parameter. Taking the example from above this is the uncertainty in the parameter estimates achieved by fitting a linear model to the data see the linear model in figure 1. Minimizing estimation uncertainty is partly what scientists care about, as inevitably most cognitive science experiments revolve around null hypothesis testing, which in most cases will involve testing whether the parameter estimate includes a particular value, mostly, 0. Therefore if there is an underlying effect i.e. the parameter is different from zero, then minimizing estimation uncertainty will reveal this effect as being statistically significant. To minimize this type of uncertainty the standard approach is to get more data, given that it comes from the same population and produces similar results. In cognitive science this might include increasing the number of trials or subjects to get a more precise estimate of interest i.e. minimizing estimation uncertainty. This way of minimizing estimation uncertainty is however not free or free of uncertainty itself. Firstly, increasing the number of trials in a cognitive task might even increase the estimation uncertainty itself. This can happen for several reasons, but boredom, habituation, fatigue and lack of engagement can become big contributors when experimental tasks become very long [@meier_is_2024; @jeong_exhaustive_2023]. Secondly for some cognitive science experiments massively increasing the number of trials could make subjects more prone to switching between cognitive strategies and if not properly accounted for in the analysis might be interpreted as additional noise by the model and its parameters. Next increasing the number of subjects included in a study will many times decrease estimation uncertainty on the population level estimates, if the sample population is homogeneous. The trade off between subjects and trials in an experiment is therefore quite important to minimize estimation uncertainty, but also minimize the overuse of resources. Beyond these traditional approaches to minimizing estimation uncertainty there are other ways, for instance changing the task design [@baldi_antognini_new_2023; @stone_using_2014]. This optimization strategy involves individualizing the task design such that each presented stimulus is the most informative it can be. This task design optimization is frequently used in psycho-physical experiments where adaptive algorithms are used to select the upcoming stimuli such that it minimizes the uncertainty in the estimated parameter values. See for example algorithms like PSI, QUEST and ADOPY [@watson_quest_2017; @yang_adopy_2021; @prins_psi-marginal_2013]. From the example of the researcher investigating reaction time and stress this might involve selecting interventions that will produce varying levels of stress to minimize estimation uncertainty of the parameters of the model. The thesis will explore how these algorithms work and can be implemented to minimize estimation uncertainty. 

The next level, and here last level of uncertainty stems from the fact that the parameter estimates will vary over time, as humans vary over time. This variation stems from both behavioral factors like learning, but also psychological factors such as mood and arousal [@schurr_dynamic_2024]. This type of uncertainty will be referred to as test-retest uncertainty. Participants in the researchers study on reaction times and stress might be tested twice on different days to understand how stable the relationship is over time. As the relationship is measured by the parameters of the model the stability of the relationship is measured by the stability of the parameters. One might imagine that the amount of sleep acquired before the experimental day could influence both measures of the task i.e. reaction time and susceptibility to stress and perhaps even their relationship. Figure 2 displays how the parameter estimates of the same model as presented in Figure 1 with and without accounting for uncertainty propagation change based on the propagation of uncertainty. As can be seen from Figure 1 accounting for the measurement uncertainty does not change much the prediction made by the model, however when propagating these extra uncertainties into the next analysis of the parameters i.e. from session to session in Figure 2 the change in results become more pronounced. The main effect for the current linear model is that the residual variance (sigma) and the intercept is underestimated without error propagation and the slope parameter is overestimated.



```{r figure2, fig.width = 7, fig.height = 4, warning = F, message = F, echo = F, fig.cap = "**Figure 2 Test retest uncertainty.** Diplays the results of fitting the linear regression in Figure 1 twice, with and without accounting for measurement uncertainty. Each facet represents one of the three parameters of the linear model, the intercept the residual uncertainty (sigma) and the slope respectively from left to right. Colors represented weather the measurement uncertainty was proporgated or not."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_2_test_retest.png")), scale = 1)
```

The main message here is that to get reliable estimates and in the end, to make reliable inference one needs to account for sorts of uncertainties and the lower in the hierarchy you move the more fundamental and important they become, when doing higher level calculations with them. Having a parameter estimate that is stable over time will not matter if you cannot estimate it, or measure it, reliably in the first place. For a complete set of scripts and parameters used for above demonstration see [github](https://github.com/JesperFischer/Master-thesis/blob/main/plot%20scripts/Measurement%20error%20visualization.Rmd).
