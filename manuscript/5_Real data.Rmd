# Experimental data

Having rigorously investigated how the PF, both in terms of the interaction between parameter values, but also in terms of how estimation uncertainty of the parameters are influenced by various factors, the thesis now turns to a re-analysis of data collected on participants. This section introduces the published data-set that will re-analysed utilizing the psychometric functions introduced. The goal with this re-analysis is 2-fold. Firstly, it reiterates the fact that making assumptions about the structure of the data can make big differences in the parameter estimates and their uncertainties. Secondly, it will serve as a starting point to understanding why the internal model validity steps are helpful as a metrics to gauge how trials and subjects interact on the statistical power of a model to reject a hypothesis. This last aspect of testing hypotheses will tie together how the validity steps above can help determine the ability of a particular model to do hypothesis testing. The last point of the thesis is going revolve around conducting a thorough power analysis of the current model, utilizing the published data-set described below. In the regard of conducting a power analyses I will again highlight where uncertainty creeps in and how we can deal with and account for it, as common practices are insufficient.

## *Heart rate discrimination task*

The article introducing the data-set is @legrand_heart_2022 and is an interoceptive task, meaning participants were instructed to attend to their internal bodily states. Here the authors collected 223 participants who came in twice to complete the heart rate discrimination (HRD) task with 6 weeks between visits. The HRD task has participants internalize their own heart rate for 5 seconds, meanwhile the participant's heart rate is monitored and calculated in real time. Next, based on the observed heart rate participants will hear five auditory tones with a given frequency (not the internal frequency of the tone, but the frequency of how fast the tones is presented) that is either faster or slower than their own objective heart rate. The amount this auditory tone's frequency is faster or slower is determined by the PSI ADO algorithm introduced in the "Adaptive design optimizing" section. This means that the stimulus value for the PF for this experiment is the difference between the external tone's frequency and the observed heart rate of the participant. The responses provided by the participants are therefore either faster or slower with faster referring to the belief that the individual heart rate was faster than the tone provided. This means that a participant might have a heart rate of 50 beats per minute (BPM) at a particular trial and then hear tones in a frequency of 40 BPM and are asked to respond whether they think this 40 BPM tone is slower or faster than their own heart rate. The authors of the experiment, ran a single participant level model of each subject, for each session, and then correlated the slope and threshold of the PF. They found a medium correlation between the threshold r = 0.5 p \< .001 between sessions and a negligible correlation r = 0.1, p = .15 for the slope. In the next section I will show how this reliability might change given different assumptions of the structure of the data as well as employing different models by incorpurating additional information in terms of reaction times and confidence ratings.

## *The models*

In this section the models fit to the test-retest data-set are described, in order to examine the influence of the type of model fit on the correlation between session one and two of the PF parameters here the threshold and slope. The single fit model is going to be the references and going to be the same as the original authors did. That is estimating each individual for each of the sessions individually without a lapse rate (i.e. a two parameter PF) and then post hoc correlating the estimates between session one and two. adding the propagated uncertainty to these estimates will serve as the next model. Next, the same model as above with a lapse rate with be tested, in order to understand the influence of this parameter in this particular data-set. Two types of hierarchical models are going to be fit, one with a single layer amounting to modeling the two sessions from the same multivariate normal distribution with hierarchical priors for each session. This model directly models the correlation between sessions as its included in the variance - covariance matrix of the multivariate normal distribution. This model would amount to the model displayed in figure 5 if the participant level distributions were removed. The last type of model is the nested hierarchical model identical to the model presented when introducing the ICC, see Figure 5. This model assumes that all subjects have a mean level parameter which is drawn from the same multivariate normal distribution, each parameter for each session is then drawn from a subject level distribution. For this last model the ICC is the statistical metric estimated by the model itself (i.e. what has been described as $ICC_1$), and the correlation will afterwards be calculated. In addition to testing the influence of the data structure in the fitted models the reaction times for each trial is also going to be included in the analysis in the same vein as described in the section about increasing information in cognitive models. finally a full model is going to be fit which will also utilize the continous confidence rating readily available in the data-set. This full model will not only incorporate the reaction times on a trial-by-trial basis, but also these confidence ratings for each trial. Confidence ratings were included in the task of the original experiment to examine the participants' interoceptive metacognitive abilities, but here they will be used to inform the parameters of the underlying psychometric function just as with the reaction times. The confidence ratings are going to be modeled in close resemblance to the reaction times, just inverted. This inversion is because in the middle of the psychometric function (at the threshold) the uncertainty about the stimulus representation is the highest and therefore reaction times should be their highest as well, but confidence should be at the lowest. Another difference between the reaction times and the confidence ratings is their range of possible values and therefore the probability density function used to describe them. The confidence ratings in the task were bounded between 0 and 100 ranging from complete uncertainty and certainty. A natural probability density function for such kind of double bounded variables is the beta distribution as its bounded between 0 and 1 [@geissinger_case_2022]. The problem with using beta distribution in this case is the edge cases of 0 and 1's which for the confidence ratings are 0 and 100 when deviding each confidence rating with 100. One approach to circumvent this is to model these edge cases separately using a zero-one-inflated beta distribution. This approach, however, models these edge values as separate processes which does not make sense in this case as the confidence ratings are meant to represent a continuous measure of confidence. For reasons of simplicity the thesis therefore subtracts a small number i.e. 0.001 from the 1 ratings and adds 0.001 to the 0 ratings making it possible to use the beta distribution for the full range of confidence ratings. This approach of modeling the bounded ratings between 0 and 100 is tenuous and new methods are slowly being developed for a more holistic approach see @kubinec_ordered_2023. Reaction times of the responses were at maximum 8 seconds and can therefore still be modeled by the shifted log normal distribution introduced above.

## *Results*

Table 3 displays the correlation coefficient between the first and second session for the threshold and slope for each model when uncertainty has been propagated using bootstrapping. For a full table of all parameters of all models as well as with and without uncertainty propagation see [Supplementary table 1](https://github.com/JesperFischer/Master-thesis/blob/main/Supplementary%20tables/Supplementary%20table1.xlsx) which is linked to the github of the thesis.

\newpage

```{r table3, fig.width = 7, fig.height = 7, warning = F, message = F, echo = F}
table3 = read.csv(here::here("tables","table3.csv")) %>% mutate(X = NULL)

table3 = flextable::flextable(table3) %>% width(c(1,2), width = 1.7)%>% width(5, width = 1.1)%>% width(4, width = 1)


table3 = set_caption(table3,
  caption ="Table 3. Results from reanalysis of legrand (2022). Table showing the correlation between sessions of the threshold and slope parameter of the psychometric function using different model fomulations as well as hierarchical model structures.")



table3
```

\linebreak

Table 3 highlights the fact that the additional assumptions of the hierarchical models both nested and unnested increases the session-by-session correlation of the slope of the psychometric function. Additionally including the reactions times increases the correlation even more, however with overlaying 95% credibility intervals. The models with included confidence ratings perform worse than the models with only the added reaction time in the session by session correlation. This might allude to the fact that another process is present, or that the particular model used for the confidence ratings, might be inappropriate. The main difference in session-by-session correlation between the two hierarchical models can be found in the threshold as the nested hierarchical model outperforms the non-nested hierarchical model in this regard. A concern of this approach of just looking the correlation coefficients is of cause that a model with a high session by session correlation might not fit the data at all. Therefore an examination of the comparison of model fit is crucial to ensure that the nested hierarchical model also fits the data better than the unnested hierarchical model. One approach would therefore be to examine model fit using common metrics such as leave one out cross validation, information criterion etc. The difficulty here is that most of the models are incompatible. This stems from the fact that they have been fit to differing amounts of subjects in the case of hierarchical vs single fit models, and to differing amounts of dependent variables in the case of within model architectures. Another consideration for not conducting model comparison is that the difference between the models that are comparable i.e. the hierarchical and nested hierarchical with the same number of dependent variables is that the difference between these model lie in the assumption of the data and is therefore something that should have been decided before modeling the data. Therefore given that it is known that each subject was accessed twice (and not a new participant was tested), and that the nested Hierarchical model captures this assumption, one should be inclined to choose this model regardless of the session by session correlation. Therefore instead of directly comparing the comparable models, one might look at posterior predictive checks. These checks serves to investigate whether the model predictions align with the data. These posterior predictive checks were performed for the most complicated models to ensure that the models are capturing some of the underlying patterns in the collected data. For these posterior predictive checks on both group level and single subject level see supplementary Figure 8-11 together with Supplementary Note 5.
