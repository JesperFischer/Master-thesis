## *Investigating measurement uncertainty*

To keep a consistent theme, I will throughout the thesis be demonstrating how computational resources have made the need for analytic solutions involving tedious assumptions sometimes irrelevant. This is highly relevant as closed-form-problems where an analytic solution is known or even attainable are becoming less and less frequent with the surge in popularity of more and more complex models, see section about modeling definitions for further elaboration. In order to explore measurement uncertainty in examples related to cognitive science the thesis will here investigate the relationship between correlation coefficients and measurement uncertainty. This will be done, as a non trivial part of the published litterature in cognitive science revovles around conducting correlational analyses on measures such as questionaires [@de_berker_computations_2016; @luijcks_influence_2015].

To demonstrate how and by how much adding uncertainty into questionnaires will change interpretations, I will here focus on the perceived stress scale (PSS) [@cohen_perceived_1994]. Which is a questionnaire of 10 items that are scored on a 5-point Likert scale from 0 (never) to 4 (very often), with 1 (almost never) 2 (sometime) 3 (fairly often). Trying to avoid response bias and random selection of questions, the last 5 items' scores are reversed and then the sum of the scores is calculated to give a perceived stress score with predefined intervals of scores of 0-13 14-26 27-40, indicating low stress, medium stress and high perceived stress respectively. This questionnaire has been used in various papers [@de_berker_computations_2016; @luijcks_influence_2015],indicating some acceptance across scientific fields [@treadway_perceived_2013; @kuiper_global_1986]. In this section I will demonstrate how using simulations to both understand and explore how adding measurement uncertainty to this particular questionnaire will change the interpretation of doing correlational analyses of such scores with other variables such as behavioral measures like reaction times, computational parameters like learning rates or structural properties of the brain like the myelination or grey matter volume in a region of interest [@wu_neurobiological_2021]. In order to use simulations to include measurement uncertainty firstly an understanding of the uncertainty of the correlation coefficient itself is needed. Analytical solutions exist to calculate the uncertainty of such statistics, which is incorporated in most statistical softwares [@R-correlation], however another way to find and understand this uncertainty comes from resampling. Below is firstly an explanation of using resampling to evalute uncertainty in a general case and thereafter in the specific case of the questionaire described above.

The way to estimate the uncertainty in the correlation coefficient is to re sample the collected data with replacement i.e. bootstrapping  and then recalculate the test statistic of interest [@efron_estimating_1983]. Iterating this process gives a distribution of test statistics which with enough iterations will converge towards the analytic solution with recommendations of having at least 30 data points to begin with [@efron_estimating_1983; @efron_introduction_1994; @wu_jackknife_1986]. For the simplest case of recalculating the correlation coefficient and its uncertainty might seem somewhat tedious compared to taking the direct analytic solution, as this is already implemented in most statistical softwares and packages [@correlationPackage], however once setup and understood this approach allows for adding not only measurement uncertainty, but a more general way of thinking about the uncertainty of statistical metrics. One of the advantages of having an analytic solution to this simple case of recalculating the uncertainty of the correlation coefficient is to ensure that the code and scripts are properly set up. This therefore serves as a validation step before exploring territories where analytic solutions are scarce or nonexistent.

The first step is therefore to show that the two approaches of simulating and analytically estimating the uncertainty of the correlation coefficient is identical across different ranges of correlations and sample sizes. To do this, I've simulated data from a multivariate normal distribution with the following parameters.

$$
\begin{equation} 
  \tag{Eq. 1}
  \begin{pmatrix}
  x_i \\
  y_i 
  \end{pmatrix} \sim \mathcal{N}\left(
  \begin{pmatrix}
  \mu_x \\
  \mu_y 
  \end{pmatrix}, \begin{bmatrix}
  \sigma_x^2 & \sigma_x \cdot \sigma_y \cdot \rho_{xx} \\
  \sigma_x \cdot \sigma_y \cdot \rho_{xx} & \sigma_y^2 
  \end{bmatrix} 
  \right)
\end{equation} 
$$

Where

$$
\mu_x = 50, \quad \mu_y = 100, \quad \Sigma = \begin{bmatrix}
10^2 & 10 \cdot 10 \cdot \rho_{xx} \\
10 \cdot 10 \cdot \rho_{xx} & 10^2 
\end{bmatrix}
$$

The multinormal distribution produces random variables with a means $μ_x$ a standard deviation $σ_x$ and crucially with a correlation coefficient between all random variables $ρ_{xx}$, here the subscript indicates that there are x random variables being sampled together. This distribution is perfect for understanding how the correlation coefficient changes as it is a parameter of the distribution. Now demonstrating that bootstrapping and the analytic solution implemented in R are identical, I simulate correlation coefficients ranging from -0.9 to 0.9 in increments of 0.1 with the total number of samples per random variable being between 50 and 500 in increments of 50 [@correlationPackage; @R2024]. See supplementary material and supplementary Figure 1 for demonstration of the simularity of these two approaches. 

Having shown that the two approaches are identical (or close to) we can add measurement uncertainty to each observation. This has again been analytically solved and solutions exist to calculate the correlation coefficient under these circumstances [@saccenti_corruption_2020]. To add measurement uncertainty to the measurements we can instead of randomly re sampling pairs of data points from the original data, as done for the simplest case above, one re samples these pairs as means of an error distribution where the uncertainty (standard deviation) of this distribution is the measurement uncertainty. A mindless choice of error distribution would be the normal distribution which would reflect the fact that the directionality of the uncertainty is assumed to be bidirectional i.e. with no preferred direction. Of note here is that one might re sample the original data from other error distributions for instance if values are strictly positive or bounded in other ways then simulating from a truncated normal or strictly positive distributions like a lognormal, would be preferred to avoid sampling values that cannot be obtained i.e. negative questionnaire values. For the first demonstration of adding measurement uncertainties to observed data, normally distributed noise is simulated, which means simulating new "observed values" from a normal distribution with a mean of the observed observation and a standard deviation $\sigma$ equal to the measurement uncertainty. This can be seen in Figure 3, here uncertainty is added to just the x values in increasing amounts (A), with the resulting correlation coefficient distribution obtained by bootstrapping displayed in (B). It should be noted that the correlation coefficient simulated in this case was 0.8, as indicated by the vertical line in figure 3 (B). it is clear the estimated correlation coefficient using bootstrapping is being attenuated in size but also that the width of the correlation coefficient distribution is increasing with increasing measurement uncertainty, mimicking what can be shown using the analytical solutions [@saccenti_corruption_2020].

```{r figure3, fig.width = 7, fig.height = 7, warning = F, message = F, echo = F, fig.cap = "**Figure 3 Measurement uncertainty on correlation coefficient**  (A) Displays a scatterplot with varying amounts of measurement uncertainty. (B) displays how the correlation coefficient distribution, obtained through bootstrapping changes with increasing measurement uncertainty. Vertical line is the simulated correlation coeficient without uncertainty."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_3_measurement_uncertainty.png")), scale = 1)
```

Moving to a concrete example of using the PSS questionnaire, one might borrow ideas from physics of adding a small uncertainty to each of the 10 questions of the questionnaire and then propagate the uncertainty using error propagation, as shown above in figure 3 (A) to the x-values, which is the same way physicists would add 0.5mm uncertainty to a measurement of length using a ruler.
$$
dz = \sqrt{(dx^2 + dy^2 + ...)}
$$

Where $dz$ is the uncertainty on the overall score and $dx$, $dy$ ... are the uncertainties of the individual questions. Assuming the uncertainty on each question is identical, then this expression simplifies to:

$$
dz = \sqrt{(n*dc^2)} = \sqrt{n} * dc
$$

Where n is the number of questions in the questionnaire and $dc$ is the uncertainty associated with each question and $dz$ being the uncertainty associated with the PSS. A couple of key things should be noted, the independence of questionnaire scores is surely unjustified as we expect the first 5 questions to be highly positively correlated and these 5 being negatively correlated with the last 5, as these have been reversed in meaning. Next the error distribution for each question and therefore also the resulting questionnaire score cannot have a normal distribution as the error distribution because we know that the minimum and maximum score of each question cannot exceed 0 or 4 and the maximum score is bounded between 0 and 40. One might therefore first try a truncated norm distribution between 0 and 4 as the error distribution for each question with the standard deviation of this distribution being the amount of error we assume to be present for each question.
