## *Investigating measurement uncertainty in questionnaires*

To keep a consistent theme, I will throughout the thesis be demonstrating how computational resources have made the need for analytic solutions involving tedious assumptions sometimes irrelevant. This is highly relevant as closed-form-problems where an analytic solution is known or even attainable are becoming less and less frequent with the surge in popularity of more and more complex models, see section about modeling definitions for further elaboration.

To demonstrate how and by how much adding uncertainty into questionnaires will change interpretations, I will here focus on the perceived stress scale (PSS) [@cohen_perceived_1994]. Which is a questionnaire of 10 items that are scored on a 5-point Likert scale from 0 (never) to 4 (very often), with 1 (almost never) 2 (sometime) 3 (fairly often). Trying to avoid response bias and random selection of questions, the last 5 items' scores are reversed and then the sum of the scores is calculated to give a perceived stress score with predefined intervals of scores of 0-13 14-26 27-40, indicating low stress, medium stress and high perceived stress respectively. This questionnaire has been used in various papers [@de_berker_computations_2016; @luijcks_influence_2015],indicating widespread acceptance across scientific fields [@treadway_perceived_2013; @kuiper_global_1986]. In this section I will demonstrate how we can use simulations to both understand and explore how adding measurement uncertainty to this particular questionnaire will change the interpretation of doing correlational analyses of such questionnaire score with other variables such as behavioral measures like reaction times, computational parameters like learning rates or structural properties of the brain like the myelination or grey matter volume in a region of interest [@wu_neurobiological_2021].

Firstly it will be shown how uncertainty estimates of statistics (here persons correlation coefficient) can be obtained using simulations instead of analytically solutions as implemented in various software. The way to do this is to re sample the collected data with replacement i.e. bootstrapping the data and then recalculate the test statistic of interest [@efron_estimating_1983]. Iterating this process gives a distribution of test statistics which with enough iterations will converge towards the analytic solution with recommendations of having at least 30 data points to begin with [@efron_estimating_1983; @efron_introduction_1994; @wu_jackknife_1986]. For the simplest case of recalculating the correlation coefficient and its uncertainty might seem somewhat tedious compared to taking the direct analytic solution, as this is already implemented in most statistical softwares and packages [@correlationPackage], however once setup and understood this approach allows for adding not only measurement uncertainty, but a more general way of thinking about the uncertainty of statistical metrics. One of the advantages of having an analytic solution to this simple case of recalculating the uncertainty of the correlation coefficient is to ensure that the code and scripts are properly set up. This therefore serves as a validation step before exploring territories where analytic solutions are scarce or nonexistent.

The first step is therefore to show that the two approaches of simulating and analytically estimating the uncertainty of the correlation coefficient is identical across different ranges of correlations and sample sizes. To do this, I've simulated data from a multivariate normal distribution with the following parameters.

$$
\begin{equation} 
  \tag{Eq. 1}
  \begin{pmatrix}
  x_i \\
  y_i 
  \end{pmatrix} \sim \mathcal{N}\left(
  \begin{pmatrix}
  \mu_x \\
  \mu_y 
  \end{pmatrix}, \begin{bmatrix}
  \sigma_x^2 & \sigma_x \cdot \sigma_y \cdot \rho_{xx} \\
  \sigma_x \cdot \sigma_y \cdot \rho_{xx} & \sigma_y^2 
  \end{bmatrix} 
  \right)
\end{equation} 
$$

Where

$$
\mu_x = 5, \quad \mu_y = 10, \quad \Sigma = \begin{bmatrix}
10^2 & 1.5 \cdot \rho_{xx} \\
1.5 \cdot \rho_{xx} & 20^2 
\end{bmatrix}
$$

The multinormal distribution (Eq. 1) produces random variables with a means $μ_x$ a standard deviation $σ_x$ and crucially with a correlation coefficient between all random variables $ρ_{xx}$, here the subscript indicates that there are x random variables being sampled together. This distribution is perfect for understanding how the correlation coefficient changes as it is a parameter of the distribution. Now demonstrating that using bootstrapping and the analytic solution implemented in r are identical, I simulate correlation coefficients ranging from -0.9 to 0.9 in increments of 0.1 with the total number of samples per random variable being between 50 and 500 in increments of 50 [@correlationPackage]. Supplementary Figure XX shows how the mean correlation coefficient changes based on the number of simulations and the approach used to obtain it as well as how the uncertainty of that estimate changes due to the same factors. 
Having shown that the two approaches are identical we can add measurement uncertainty to each observation. This has again been analytically solved and solutions exist to calculate the correlation coefficient under these circumstances [@saccenti_corruption_2020]. To add measurement uncertainty to the measurements we can instead of randomly re sampling pairs of data points from the original data, as done for the simplest case above, one re samples these pairs as means of an error distribution where the uncertainty (variance or standard deviation) of this distribution is the measurement uncertainty. A mindless choice of error distribution would be the normal distribution which would reflect the fact that the directionality of the uncertainty is assumed to be bidirectional i.e. with no preferred direction. Of note here is that one might re sample the original data from other error distributions for instance if values are strictly positive or bounded in other ways then simulating from a truncated normal or strictly positive distributions like a lognormal, would be preferred to avoid sampling values that cannot be obtained i.e. negative questionnaire values. 
For the first demonstration of adding measurement uncertainties to observed data. We simulate normally distributed noise which means simulating new "observed values" from a normal distribution with a mean of the observed observation and a standard deviation $\sigma$. This can be seen in Figure 3, here uncertainty is added to just the x values in increasing amounts (A), with the resulting correlation coefficient distribution obtained by bootstrapping displayed in (B). As is clear the estimated correlation coefficient using bootstrapping is being attenuated in size and with increasing width of the distribution with increasing measurement uncertainty.

```{r figure3, fig.width = 7, fig.height = 7, warning = F, message = F, echo = F, fig.cap = "**Figure 3 Measurement uncertainty on correlation coefficient**  (A) Displays a scatterplot with varying amounts of measurement uncertainty. (B) displays how the correlation coefficient distribution, obtained through bootstrapping changes with increasing measurement uncertainty. Vertical line is the simulated correlation coeficient without uncertainty."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_3_measurement_uncertainty.png")), scale = 1)
```

Moving to the example of the PSS questionnaire, one might borrow ideas from physics of adding a small uncertainty to each of the 10 questions of the questionnaire and then propagate the uncertainty using error propagation, the same way physicists would add 0.5mm uncertainty to a measurement of length using a ruler. If we assume that the measurements / question entries are independent, this becomes one of the simplest cases of error propagation and the additional uncertainty from each added question to the questionnaire score will add the following uncertainty to the overall score:

$$
dz = \sqrt{(dx^2 + dy^2 + ...)}
$$ 

Where $dz$ is the uncertainty on the overall score and $dx$, $dy$ ... are the uncertainties of the individual questions. Assuming the uncertainty on each question is identical, then this expression simplifies to: 

$$
dz = \sqrt{(n*dc^2)} = \sqrt{n} * dc
$$ 

Where n is the number of questions in the questionnaire and $dc$ is the uncertainty associated with each question and $dz$ being the uncertainty associated with the PSS. A couple of key things should be noted, the independence of questionnaire scores is surely unjustified as we expect the first 4 questions to be highly positively correlated and these 4 being negatively correlated with the last 4, as these have been reversed in meaning. Next the error distribution for each question and therefore also the resulting questionnaire score cannot have a normal distribution as the error distribution because we know that the minimum and maximum score of each question cannot exceed 0 or 4 and the maximum score is bounded between 0 and 40. One might therefore first try a truncated norm distribution between 0 and 4 as the error distribution for each question with the standard deviation of this distribution being the amount of error we assume to be present for each question.
