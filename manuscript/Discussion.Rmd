# Discussion

The current thesis has investigated how the handling of uncertainty in the field of cognitive science and especially in the developing field of cognitive modeling can be improved. The thesis has done this by demonstrating that using computational resources in the form of simulations, a deep mathematical understanding with rigorous closed end solutions is not necessary to get a deeper understanding of how uncertainties on each level can and will influence every statistical metric. The thesis outlined three critical types of uncertainty; measurement uncertainty being the lowest level of uncertainty that is often completely neglected in the field, even though it influences the resulting statistical metrics in very unpredictable ways if the data has influential data points which might be associated with particularly high measurement uncertainty. Researchers should firstly be aware that measurement uncertainty is always present and examining the extent to which it can be safely ignored in their statistical models must be determined. Even in measures like reaction times which is commonly used in cognitive science [@sternberg_memory-scanning_1969; @macleod_training_1988; @pirolli_role_1985], there are measurement uncertainties, which depending on the soft and hard-ware the experiment might be a huge factor [@holden_accuracy_2019; @crocetta_problem_2015; @ohyanagi_solution_2010].

The thesis pointed to one aspect of cognitive science literature where measurement uncertainty is of difficulty, i.e. questionnaires, note however that the arguments laid out can be applied to any type of measure. Quite a literature exists on doing correlational or testing differences in populations groups from the results of questionnaires, which begs a question of the certainty of these questionnaire scores. This is especially true when considering some quite important aspects in handling these statistical problems. Firstly, questionnaires are easy, fast, and cheap to conduct when performing a behavioral experiment, subjecting many questionnaires to be collected in order "to see if something is there". This curiosity is sometimes what derives sciences forwards, however in cases like these it will inevitably lead to false positive findings that cannot be replicated as the pressure for publishing results might hide the multiple comparison correction that should have been made when finalizing a manuscript. Including some uncertainty into the questionnaires would serve to push the significance barrier higher and make it harder to find significant results in these types of analyses. Perhaps a reasonable comprise would be that the added uncertainty on questionnaire scores should be proportional or just in general related to the internal consistency, measured by ICC, Cronbach alpha, correlation coefficient etc. of the questionnaire itself i.e. its test re-test reliability uncertainty.

Estimation uncertainty was introduced as the uncertainty associated with doing computations and is often displayed as standard error of statistical metrics. The main focus of the thesis was to use this understanding of uncertainties in the field of cognitive modeling and revise some of the methods and metrics used to validate these cognitive models. It was shown that using correlations, which has been used in many previous studies [@schurr_dynamic_2024], between simulated and recovered parameters values was not a particularly sensible metric to determine the extent of internal model validity. Two important things about the correlation coefficient made it insensible for internal model validity, the decision of choosing what size and uncertainty of correlation coefficient to deem model parameters sensible is not straightforward, because the interpretation of the correlation coefficient itself in the regard of model validation is not straightforward. This is particularly true when highlighting that the correlation coefficient is invariant of a linear transformations. The other reason was that in instances where the simulated and recovered parameter values did show good dependency the correlation coefficient rapidly approached the asymptote at 1 even when more information could be gained by increasing the number of trials, due to its limited inclusion of estimation uncertainty. The study therefore suggests that using a variant of the intra class correlation coefficient (ICC) as the statistical metric for examining internal model validity which has recently been suggested in the literature [@schurr_dynamic_2024]. The thesis found that this metric was much more sensitive to estimation uncertainty in the parameters, with a sensible interpretation of the ratio between desirable to undesirable variance / uncertainty. With this new metric the thesis explored ways to decrease the undesirable variance and thereby increase the ICC metric, by incorporating smart experimental designs that are optimized for each individual on a trial-by-trial basis. Furthermore, the study showed how thinking generatively about the origins of the responses given in an experiment can increase the ICC metric without the need for extra trials, the concrete example given in the thesis was incorporating reaction times into the cognitive model describing how stimulus intensities are transformed to binary forced choices. This highlights an idea of jointly modeling several dependent variables and their interactions, that has been around for quite some time but only now is slowly gaining traction in cognitive science literature [@stone_using_2014; @pedersen_drift_2017; @hess_bayesian_2024]. The thesis highlights how all the above implementations and considerations do not necessarily have to rest on heavy mathematical understandings and proofs as computational resources and especially simulations has made it possible for people with coding experience to gain these insights by the power of (re) sampling; some of the implications of this will be discussed below. Lastly the thesis investigated and used data from a test-retest reliability study and showed that a reanalyzed could achieve better test-retest reliability by incorporating knowledge about the structure of how the data was gathered together with incorporating information already represent in the data. This data set was then used as an example of how power analyses of cognitive models could be conducted. This was done by first simulating and then fitting the cognitive model to many different simulated effect sizes in different trials and subject combinations. This approach allowed modelling the latent power curve that relates observed effect size trials and subjects to the probability of rejecting a null hypothesis in an experiment. Using posterior predictive checks and leave one out cross validation a particular power law related the parameters of the power curve to subjects and trials with good predictive abilities. Using this equation together with the many times overlooked aspect of sampling variability in the observed effect sizes when conducting power analyses, it was shown that incorporating sampling variability greatly increased the need for more subjects to achieve the same amount of power. This section also highlighted why and where the test re-test reliability of these metric matters as increasing test re-test reliability shrinks the influence of sampling variability in the observed effect sizes. Lastly the complete uncertainty propagated power analysis was compared to not accounting for sampling variability or estimation uncertainty on the parameters by using the widely used statistical software tool G\*power in a concrete example [@faul_gpower_2007; @ioannidis_why_2005; @aarts_estimating_2015] . This comparison showed how G\*power estimation of sample size was equivalent to having close to infinitely many trials and not accounting for sampling variability.

## **Power analyses, subjectivity and replication crisis.**

In recent years many scientific fields, and especially psychology, social science and medicine has been under scrutiny due to a lack of and failure of replication of previous studies. [@wiggins_replication_2019] (REF; <https://www.sciencedirect.com/science/article/pii/B9780323909693000037>)

Many contributing factors has been laid out such as publication bias, questionable research practices such as doing statistical analyses untill significant (p-hacking) or hypothesizing after the results are known (HARKING).

<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4359000/>

<https://journals.sagepub.com/doi/abs/10.1207/s15327957pspr0203_4?casa_token=9PTjzwM5EwUAAAAA:-qLZ9IR25eO_t-Kr-7hzBDUGgiiimIeZJUp7oP4EoV_5wuDJQbkS4w13PH6tUNkRTzEVRrJCFW91>

However a quite paradoxical aspect of the interaction between the replication crisis and the use of power analyses is that many times it is advised as one of the ways to increase the replicability of studies, as analyses of power of detecting small to medium effect size in social sciences have been found to be low to very low [@felix_singleton_statistical_2023]. It is therefore argued that a reason for such low replicability, might also be due to very low probability of being able to detect the underlying effect i.e. statistical power. This argument is sound as long as the analysis of power is accurate or accurate to a certain degree. What this thesis has highlighted is that the use of very popular tools like G\*power for conducting these types of power analyses will underestimate the number of subjects by a large margin. The problem is therefore that with the confidence of having done a power analysis there will be a false sense of certainty, just like the false sense of certainty about measurements assumed in these popular softwares for power analyses. Therefore instead of increasing replicability and certainty in the effects observed from cognitive science experiments utilizing these tools might paradoxically decrease them, as researchers might be tricked into conducting less powered studies due to the recommendations of the software.

Interestingly, quite a large number of scientists have suggested that perhaps moving the arbitrary statistical significance threshold to 0.005 instead of the commonly used 0.05 could be one of the approaches used to combat this replication crisis [@benjamin_redefine_2018]. Interestingly, lowing of the statistical threshold for significance would in practice lead to the conclusions drawn from this thesis of including and propagating uncertainty. Two approaches however have a very different reason to making these adjustments as this lowering of the threshold would be a means to an end instead of addressing the underlying problem, which the authors do also acknowledge [@benjamin_redefine_2018].

Another interesting idea that coincides with the general theme of the thesis to combat the replication crisis is that of preregistration, registered reports, and blind analysis, which also has been suggested as ways to combat the replication crisis. [@maccoun_blind_2015; @klein_blind_2005; @evans_improving_2023; @chambers_past_2022]. What all these types of interventions have in common is that they acknowledge the subjectivity in not only the data collection but also in the data analysis pipeline of scientific inquiry. This subjectivity is both what introduces biases into what is published, but also what derives novel ideas, and a tradeoff between exploration and exploitation might be necessary to fully guard against unwanted subjectivity. What these interventions try to do is to have the analysis pipeline either fixed before data collection or have the data scrambled such that the results of the analyses are not known when producing the analysis pipeline. Hopefully it is clear that the checking and validation introduced in this thesis is not at stake with these interventions but facilitate them. Everything up until the power analysis was done on simulated data, meaning that all model checking, and validation can be done before collecting and therefore analyzing the experimental data. However, there are still considerations when analyzing the experimental data especially on the model convergence side, where in or excluding covariate or reparameterization of models might be necessary. This is where the blind analysis intervention might be a valuable insight from physics where experimental data is scrambled in various ways such that models, analysis pipelines can be done on data that resembles the collected data, but without being able to know the results before the data is unblinded. Decisions are therefore made on scientific justifications instead of completely subjective decisions to either make the experimental results fit a research paradigm or perhaps even worse produce significant results. The distinction between decision based on scientific justification and subjective nonsensical rationale is fuzzy and narrow, however keeping incentives, such as publishing pressure, fitting into a hypothesis or research paradigm, out of the equation can help with this distinction. This might even give rise to more rigorous methods and analysis pipelines because you do not necessarily stop when the results fit the preconceived notions of the scientific paradigm, but you stop the building and testing process when you are satisfied with the assumptions made. This process might also help researchers understand the uncertainty that is associated with many of the methods or practices commonly used in the literature, which are taken as either ground truths or good approximations when they are at best noisy estimates. This could for instance be taking a sampled observed effect size from a previous study instead of relying on some further scientifically justified assumptions that the researchers hold in their domain that might be a much better approximation for the size of the underlying latent effect.

## *Why and how computational tools are vital in science.*

Perhaps cognitive or even computational modeling is the fresh start that is needed in sciences that have notoriously been relying on statistical models such as linear or generalized linear models. These more sophisticated models might be the steppingstone to engage in more theoretically driven modeling, however for this movement to not fall into the same traps and pitfalls as what statistical modeling has, it is essential that rigorous metrics are enforced from the beginning such that those models without any even provable, in principle, parameters or behaviors are discarded from the beginning. Example might arise where rigorous mathematical formulation of theories is developed but that in practice this formulation is not tractable from a computational perspective, it would be a shame to spent years investigating this model and its assumption in field of research just to discover that it in fact is intractable in practice [@ho_cognitive_2021; @mcclelland_place_2009; @zuidema_five_2020]. One might think that a necessity of these more complicated models is a need for deeper mathematical understanding in the people using them. This might be a valid concern however the field of cognitive science has rapidly picked up on sophisticated hierarchical / multi-level models without a need for a deeper understanding of the mathematical machinery. This is not to say that a better understanding of the machinery itself wouldn't be helpful for researchers of various fields, but perhaps that instead of giving researchers and students a flowchart of when to use a particular statistical model, they should be getting the tools to understand and reflect on these statistical models and therefore also the tools to understand when they break. In the same way that a good scientific program does not teach students the right theories or hypotheses, it teaches them to think in a scientific way such that the individual can decide and test these themselves. Here the tools for understanding reflecting and experimenting with statistical models and concepts could be programming experience in statistics to conduct the types of data simulations presented in the current thesis. This would allow researchers to understand the assumptions that are being made when they go and try and stimulate the data generating process, which might even help spark new scientific ideas. This approach would have researchers more closely engaged in the statistical process of analyzing the data, instead of just picking an off the shelf model from a flowchart.

## *Nothing comes without a cost.*

All of the cognitive models used in the current paper were fitted using stan using full Bayesian statistical inference with Markov chain monte carlo (MCMC) sampling. As described in the introduction section about modeling definitions fitting and building models in this framework is extremely flexible however this does come with a computational cost of resources and time as these models are not fit at the same time as a linear or multilevel models in packages such as lme4, lmertest or GAMLSS to name a few quite flexible models fitting packages in R [@stasinopoulos_generalized_2008; @kuznetsova_lmertest_2017; @bates_fitting_2015]. This added time for doing the optimization to get estimate of parameters has drawbacks in a need for access to bigger machines to necessitate the need for parallelization of the computational burden, which is something that is growing in accessibility and already available to many universities or centers of research and has been correlated with research competitiveness [@apon_high_2010].

\newpage
