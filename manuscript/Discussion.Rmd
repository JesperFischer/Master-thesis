# Discussion

The current thesis has investigated how the handling of uncertainty in the field of cognitive science and especially in the developing field of cognitive modeling can be improved. The thesis has done this by demonstrating that using computational resources in the form of simulations, a deep mathematical understanding with rigorous closed end solutions is not necessary to get a deeper understanding of how uncertainties on each level can and will influence every statistical metric. The thesis outlined three critical types of uncertainty; measurement uncertainty being the lowest level of uncertainty that is often completely neglected in the field, even though it influences the resulting statistical metrics in very unpredictable ways. Researchers should firstly be aware that measurement uncertainty is always present and examining the extent to which it can be safely ignored in their statistical models. Even in measures like reaction times which is commonly used in cognitive science [@sternberg_memory-scanning_1969; @macleod_training_1988; @pirolli_role_1985], measurement uncertainties are present, which depending on the soft and hard-ware the experiment might be a factor to account for [@holden_accuracy_2019; @crocetta_problem_2015; @ohyanagi_solution_2010].

<!-- The thesis pointed to one aspect of cognitive science literature where measurement uncertainty is of difficulty, i.e. questionnaires, note however that the arguments laid out can be applied to any type of measure. Quite a literature exists on doing correlational or testing differences in populations groups from the results of questionnaires, which begs a question of the certainty of these questionnaire scores. This is especially true when considering some quite important aspects in handling these statistical problems. Firstly, questionnaires are easy, fast, and cheap to conduct when performing a behavioral experiment, subjecting many questionnaires to be collected in order "to see if something is there". This curiosity is sometimes what derives sciences forwards, however in cases like these it will inevitably lead to false positive findings that cannot be replicated as the pressure for publishing results might hide the multiple comparison correction that should have been made when finalizing a manuscript. Including some uncertainty into the questionnaires would serve to push the significance barrier higher and make it harder to find significant results in these types of analyses. Perhaps a reasonable comprise would be that the added uncertainty on questionnaire scores should be proportional or just in general related to the internal consistency, measured by ICC, Cronbach alpha, correlation coefficient etc. of the questionnaire itself i.e. its test re-test reliability uncertainty. -->

Estimation uncertainty was introduced as the uncertainty associated with doing computations and is often displayed as the standard error of statistical metrics. The main focus of the thesis was to use this understanding of uncertainties in the field of cognitive modeling and revise some of the methods and metrics used to validate these models. This was done using a psychometric function that maps stimulus (x) values to probabilities (p) by three parameters the threshold ($\alpha$), slope ($\beta$) and ($\lambda$). It was argued that using correlations, which has been used in many previous studies [@schurr_dynamic_2024], between simulated and recovered parameters values was not a particularly sensible metric to determine the extent of internal model validity. Two important things about the correlation coefficient made it insensible for internal model validity, the decision of choosing what size of correlation coefficient to deem model parameters sensible is not straightforward, because the interpretation of the correlation coefficient itself in the regard of model validation is not straightforward. This is particularly true when highlighting that the correlation coefficient is invariant of a linear transformations. The other reason was that in instances where the simulated and recovered parameter values did show good dependency the correlation coefficient rapidly approached the asymptote at 1. This was even the case when more information could be gained by increasing the number of trials, due to its limited inclusion of estimation uncertainty. The study therefore suggested that using a variant of the intra class correlation coefficient (ICC) as the statistical metric for examining internal model validity which has recently been suggested in the literature [@schurr_dynamic_2024]. The thesis found that this metric was much more sensitive to estimation uncertainty in the parameters, with a sensible interpretation of the ratio between desirable to undesirable variance / uncertainty as its interpretation. With this new metric the thesis explored ways to decrease the undesirable variance and thereby increase the ICC metric for the particular model of the psychometric function. 

Two particular ways was investigated which revolved around, either by incorporating smart experimental designs that are optimized for each individual on a trial-by-trial basis or incorporating reaction times into the cognitive model describing how stimulus intensities are transformed to binary forced choices. These ways are neither mutually exclusive or incompatible and could easily be implemented in various experiments  to decrease estimation uncertainty in the parameters of the psychometric function without a need for more trials in the experiment itself. The second approach of jointly modeling several dependent variables and their interactions, has been around for quite some time but only now is slowly gaining traction in cognitive science literature [@stone_using_2014; @pedersen_drift_2017; @hess_bayesian_2024]. What these two ways to optimizing either experimental design and or analysis have in common is that they do not increase the efficiency of the study by  mindlessly increasing trials as this could in principle have troublesome consequences in non-obvious ways. The obvious problems with increasing the number of trials are resource costs, both in terms of money, but also in the time spent for both the participant and the experimenter. From an ethical perspective this is especially true of the time investment from the participants' side, and perhaps even more so if patient populations are investigated. However, the most problematic aspect becomes more obvious if we take a step back and think carefully about what we are studying. We are studying a complex system that has its own goals, desires and motivations and it is not trivial to know how this participant will behave if the task is double the length. Firstly, will the participant employ a different strategy knowing that the experiment is going to take X time longer, or will they halfway through employ a different strategy because of boredom. Even if the participant keeps the same underlying cognitive strategy that we are trying to model, then one reasonable assumption would be that attentional lapses and engagement in the task will decreasing, making each additional trial after a certain point less informative.

The thesis highlights how the above implementations and considerations do not necessarily have to rest on heavy mathematical understandings and proofs as computational resources and especially simulations has made it possible for people with coding experience to gain these insights by the power of (re) sampling; some of the implications of this will be discussed below. The thesis went on to investigate data from a test-retest reliability study and showed that a reanalyzis could achieve better test-retest reliability by incorporating knowledge about the structure of how the data was gathered together with incorporating information already represent in the data i.e. reaction times. This data set was then used as an example of how power analyses of cognitive models could be conducted. This was done by first simulating and then fitting the cognitive model to many different simulated effect sizes in different trials and subject combinations. This approach allowed modelling the latent power curve that relates observed effect size trials and subjects to the probability of rejecting a null hypothesis in an experiment. Using posterior predictive checks and leave one out cross validation a particular power law related the parameters of the power curve to subjects and trials with good predictive abilities. Using this equation together with sampling variability in the observed effect sizes, it was shown that incorporating sampling variability greatly increased the need for more subjects to achieve the same amount of power. This section also highlighted why and where the test re-test reliability of these metric matters as increasing test re-test reliability shrinks the influence of sampling variability in the observed effect sizes. Lastly the complete uncertainty propagated power analysis was compared to not accounting for sampling variability or estimation uncertainty on the parameters by using the widely used statistical software tool G\*power in a concrete example [@faul_gpower_2007; @ioannidis_why_2005; @aarts_estimating_2015]. This comparison showed how G\*power's estimation of sample size was equivalent to having infinitely many trials (i.e. no estimation uncertainty on the parameters) and not accounting for sampling variability i.e. disregarding much of the uncertainty inherent in the experiment.

## **Power analyses, certainty and replication crisis.**

In recent years many scientific fields, and especially psychology, social science and medicine has been under scrutiny due to a lack of and failure of replication of previous studies. [@wiggins_replication_2019; @forbes_chapter_2023]. Many contributing factors has been laid out such as publication bias, questionable research practices like doing statistical analyses until significant (p-hacking) or hypothesizing after the results are known (HARKING) [@head_extent_2015; @kerr_harking_1998]. A quite paradoxical aspect of this replication crisis and the use of power analyses is that many times it is advised as one of the ways to increase the replicability of studies, as (meta) analyses of power of detecting small to medium effect size in social sciences have been found to be low to very low [@felix_singleton_statistical_2023]. It is therefore argued that a reason for such low replicability in these fields, might partly be due to very low probability of being able to detect the underlying effect in the first place i.e. low statistical power. The argument is sound as long as the analysis of power is accurate or accurate to a certain degree. What this thesis has highlighted is that the use of very popular tools like G\*power for conducting these types of power analyses will underestimate the number of subjects by a large margin. However if this assumption is not true, then a problem might therefore be that researcher could have confidence in their experiment due to having comnducted a power analysis and there will have a false sense of certainty in their results. This mimics the false sense of certainty about the measurements assumed by these popular softwares or measurements in cognitive science in general. Therefore instead of increasing replicability and certainty in the effects observed, utilizing these tools might paradoxically decrease them, as researchers might be tricked into conducting less powered studies due to the recommendations of the software.

### *Ways of combacting the replication crisis.*

Interestingly, quite a large number of scientists have suggested that moving the arbitrary statistical significance threshold to 0.005 instead of the commonly used 0.05 could be an approach used to combat the replication crisis [@benjamin_redefine_2018]. Interestingly, lowing of the statistical threshold for significance would in practice lead to the conclusions drawn from this thesis of including and propagating uncertainty in most cases. This of cause depends on the structure and uncertainty measures of the data, however in most cases including and propagating uncertainties will lower the resulting statistic and therefore increase the resulting p-value of a particular test. These two approaches, i.e. increase the statistical significance threshold or properly propergating uncertinty, however have a very different reason to making these adjustments. The lowering of the significance threshold would be a means to an end, instead of addressing the underlying problem, which the authors also do acknowledge [@benjamin_redefine_2018].

Another interesting idea that coincides with the general theme of the thesis to combat the replication crisis is that of preregistration, registered reports, and blind analyses [@maccoun_blind_2015; @klein_blind_2005; @evans_improving_2023; @chambers_past_2022]. What all these types of interventions have in common is that they acknowledge the subjectivity in not only the data collection but also in the data analysis pipeline of the scientific inquiry. This subjectivity is both what introduces biases, but also what drives novel ideas, and therefore is a tradeoff between exploration and exploitation, that needs to be addressed to partly guard against unwanted subjectivity. What these interventions try to do is to have the analysis pipeline either fixed before data collection or have the data scrambled such that the results of the analyses are not known when producing the analysis pipeline. The rigorous checking, testing and validating of cognitive models the thesis outlined is not at stake with these interventions but facilitate them as they are build on simulations. However, there are still considerations when analyzing the experimental data especially on the model convergence side, where in or excluding covariate or reparameterization of the models might be necessary. This is where the blind analysis intervention might be a valuable insight from physics, where experimental data is scrambled in various ways such that models and analysis pipelines can be done on data that resembles the collected data, but without being able to know the results before the data is unblinded. Decisions are therefore made on scientific justifications instead of completely subjective decisions to either make the experimental results fit a research paradigm or perhaps even worse, produce significant results. The distinction between decisions based on scientific justification and subjective nonsensical rationale is fuzzy and narrow, however keeping incentives, such as publishing pressure, fitting into a hypothesis or research paradigm, out of the equation can help with this distinction [@quaia_finding_2022]. This might even give rise to more rigorous methods and analysis pipelines because it hinders arbitrarily stopping the development of the pipeline when the results fit the preconceived notions of the scientific paradigm. Instead it forces researchers to stop when they are satisfied with the assumptions and implementations made in the analysis pipeline. This process might also help researchers understand the uncertainty that is associated with many of the methods or practices commonly used in the literature, which are taken as either ground truths or good approximations when they are at best noisy estimates. This could for instance be the difference between taking an observed effect size from a previous study instead of relying or even incorporating some further scientifically justified assumptions that the researchers hold in their domain, that might give a much better approximation for the size of the underlying effect.

## *Why and how computational tools are becoming vital in science.*

Perhaps cognitive or even computational modeling is the fresh start that is needed in the sciences that has had trouble with replication. These more sophisticated models, compared to the general linear models employed in statistical modeling, might be the steppingstone to engage in more theoretically driven analyses, hopefully reducing the number of unreplicable studies. However for this movement to succeed, it is essential that rigorous metrics are enforced from the beginning such that those models without any even provable, in principle, parameters or behaviors are discarded from the beginning. Example might arise where rigorous mathematical formulation of theories are developed but that in practice the formulation is not tractable from a computational perspective. It would be a shame to spent years investigating this model and its implications in a field of research, just to discover that it in fact is intractable in practice [@ho_cognitive_2021; @mcclelland_place_2009; @zuidema_five_2020]. One might think that a necessity of these more complicated models is a need for deeper mathematical understanding. What this thesis has argued is that this is not necessarily the case as simulations allow researchers to observe the implications of their assumptions as well as investigate when they break. A counter argument could also be the increase in adaption of sophisticated hierarchical / multi-level models which are mathematically much more complex than single level models in cognitive science [@dedrick_multilevel_2009]. This is not to say that a better understanding of the machinery itself would not be helpful for researchers of various fields, but perhaps instead of giving researchers and students a flowchart of when to use a particular statistical model; they should be getting the tools to understand and reflect on these statistical models and therefore also the tools to understand when they break. In the same way that a good scientific program does not teach students the right theories or hypotheses, it teaches them to think in a scientific way such that the individual can decide and test these themselves. Here the tools for understanding reflecting and experimenting with statistical models and concepts would be programming experience in statistics to conduct the types of data simulations presented in the current thesis. This would allow the researcher to understand the assumptions that are being made and examine what happens when they are broken in various ways. This approach also requires to think more generatively about the process of how the data has been generated, because in order to simulate it is necessarily to have a model of how it was generated, which might even help spark new scientific ideas. This approach would have researchers more closely engaged in the statistical process of analyzing the data, instead of just picking an off the shelf model from a flowchart.

## *Standing on the shoulders of giants*

All of the models used in the current paper were fitted using Stan with the cmdstanr interface, which uses full Bayesian statistical inference with Markov chain monte carlo (MCMC) sampling [@R-cmdstanr]. As described in the introduction section about modeling definitions, fitting and building models in this framework is extremely flexible as the sampling algorithm essentially serve as the optimizing process for the parameters of interest. This also means that essentially the code for simulating the generative process is close to identical in nature, to the code that specifies the model, making it easy for users with a generative framework to code up these types of models. The additional benefits to using especially Stan and the its Hamiltonian Monte Carlo (HMC) algorithm is that when issues arise the algorithm will complain and let you know, reducing the risk for erroneous inference due to the sampling algorithm or typos in the code [@vehtari_rank-normalization_2021]. 

The thesis used Bayesian inference and Stan, mainly due to its flexibility in model formulation and not because of the inherent differences between Bayesian and frequentist statistics. However Bayesian inference does allow for a more optimistic way to interpret the replication crisis discussion above. This interpretation is that perhaps instead of starting each experimental analysis from the perspective that nothing or very little is known about the parameters of interest, perhaps incorporating information from previous studies would be beneficial. This is in essences what science is about, a hierarchical organization of knowledge, where each step rests on the step below, i.e. on auxiliary assumptions as put by the Duhem--Quine thesis [@ariew_duhem_1984]. This view on science also matches that of uncertainties as these are also hierarchically organized and when doing analyses on data with uncertainties this uncertainty has to be accounted for. So in the same way that the results of a scientific theory is only as strong as its auxiliary assumptions; the strength of an analysis is also only as strong as the certainty with which the data is measured with. What the Bayesian framework of inference allows, is that prior information from similar studies can be used in the modelling allowing for researchers to not start their scientific studies from scratch, but pick up where others left off. This would essentially mean that instead of having to collect a larger number of subjects to achieve the actual desired power of the study, this could be done by two independent laboratories, the second using the information provided by the first. This essentially is already what is being done when conducting meta-analyses of different fields or sub fields. This approach incentives publications of all types of finding as they serve as the stepping stones for the next researcher, making the problem of publication bias where null findings are unpublished less incentivized [@laitin_reporting_2021].

## *Limitations*

The main focus of current thesis was investigating how the correlation coefficient is an inappropriate metric to examine internal model validation of cognitive models. The modified intra class correlation (ICC) was purposed as a more sensible metric. As is clearly the case in Figure 11, the correlation coefficient quickly becomes asymptotic with quickly diminishing uncertainties whereas there is more granularity in the ICC. It might be argued that this increased granularity however is not important as the shape of the curve is similar, i.e. with increasing trials both the correlation and ICC asymptotes at 1. This interpretation is sensible and comparing Figure 11 (with simulated beta of 2 as the population mean in the power analysis) and the power analysis results in Figure 22, it seems like the pattern for the correlation coefficient better follows the shape of the curve of the power analysis. However without a straightforward way to quantitatively compare these two analyses its impossible to say. One reason to suspect that the ICC metric would fair better in such link is both the demonstration that it is highly senisble to estimation uncertainty, but also that it is variant to linear transformation. This invariance of the correlation coefficient makes it non sensical from a theoretical standpoint to access how well a model can recover simulated parameter values. Future investigations could examine this link as a clear link would make the need for conducting a power analysis superfluous as the information would be contained in the ICC analysis. A thorough investigation of this link would mean that the somewhat arbitrary choice of trials when designing an experiment would no longer be arbitrary, but informed by how estimation uncertainty in the parameters of interest change based on the number of trials [@miller_how_2024].

Another limitation of the current study is the quite limited power analysis conducted. Ideally the thesis would have investigated how other parameters of the psychometric function behaves as a function of trials and subjects, a particular interest would be on the slope of the psychometric function as changes in this parameter changes the estimation uncertainty of all the other parameters. One might suspect that an intervention that increases the steepness of the slope would also make it easier to detect a change in the threshold as both the correlation coefficient and ICC metric showed that with increased steepness of the function less estimation uncertainty was present in the estimation of the threshold and slope itself. This highlights how the parameters of the model interacts which can be accounted for by performing these simulations. therefore future investigations should expand upon this power analysis for the other parameters and especially the slope of the psychometric function. The thesis also did not investigate how incorporating the reaction times into the power analysis would change the number of subject and trials needed for observering a particular effectsize. Future studies investigating this would not only help elucidate the question posed above about the relationship between the internal model validation metric, trials and power, but could also give an estimate of the increased efficiency by incorpurating information already present in most experiments. The reasoning for only conducting the single power analysis on the threshold in the current thesis highlights one of the main hurdles of the framework purposed, computational resources. Firstly fitting models using HMC and Bayesian inference is both more time and computational resource intensive compared to frequentist inference in packages such as lme4, lmertest or GAMLSS to name a few quite flexible models fitting packages in R [@stasinopoulos_generalized_2008; @kuznetsova_lmertest_2017; @bates_fitting_2015]. This added time for doing the optimization of posterior distributions of parameters has drawbacks in a need for access to bigger machines to necessitate the need for parallelization of the computational burden especially when several chains are needed to ensure convergence. Fortunately the access to bigger machine both privately but also on an institutional level is something that is growing in accessibility and already available to many universities or centers of research and has been correlated with research competitiveness [@apon_high_2010]. The current thesis was supported by Ucloud (see acknowledgement).

\newpage


