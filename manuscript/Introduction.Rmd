# Introduction

Imagine measuring the length of an A4 paper with a ruler and then using a well validated questionnaire to determine your anxiety level. Which of these two measures, i.e. the length of the A4 paper or your anxiety level, would have the greatest faith in? In this thesis I will investigate shortcomings in uncertainty handling in the cognitive science literature, while also providing ways to account for these uncertainties when statistical or computational models are applied. To do this I will rely on Monte Carlo simulations and low-level mathematics to demonstrate that the utility of simulations. To provide novel ways of demonstrating uncertainties in cognitive models the thesis will extend on the current framework for parameter recovery. To do this the thesis will focus on the psychometric function and show how this extension of the parameter recovery framework can be used to examine how uncertainties in the parameters of the psychometric function can be minimized by experimental design, but also by incorporating additional information available in most experiments like reaction times. To further reiterate the utility of jointly modeling dependent variables the thesis will re-analyze published data using a psychometric function. Here it will also be demonstrated how carefully using the known structure of the data can greatly improve session by session correlation between parameters, i.e. minimize test-retest uncertainty. Lastly using this reanalysis, the thesis will highlight opportunities to conduct power analyses utilizing novel modeling framework that account for the uncertainty in model parameters as well as sampling variability of the effect investigated. Comparison will be made to popular tools such as G\*power highlighting the need for more rigorous methods, when conducting power analyses. associated with our statistical models.

## *Uncertainties in science and examples from physics*

<!-- This section still needs work -->

Science is a systematic way to organize knowledge in hierarchies, with a foundation in testable hypotheses. Knowledge can be hard to define, but most often it is something that is achieved though experience, one might imagine a dropped cup we have the knowledge that it will fall towards the ground, because of our previous experiences. This is to say that knowledge is the relationships that we believe to be true with differing amounts of certainty. The reality is that even though we might say that we are completely certain of events, i.e. know, that the cup will fall towards the ground, this is still an assumption that is true most of the time but given that the natural world is bounded on probabilities, complete certainty is unwarranted. Most of the time this probabilistic nature of the natural world stems from the uncertainties during measurement or perhaps unseen events. The interest here is not in the unseen events but instead in the predictability and (un)certainty of the expected. Taking the falling cup as an example, we would normally not be interested in the probability that the cup will hit the ground and shatter, but instead in the acceleration of the cup and the uncertainty in this estimate. What scientists have shown is that objects dropped on earth will accelerate towards the ground with an acceleration of $9.81\frac{m}{s^2}$ [@johannes_fundamentals_2009]. However, this number does not mean anything without an estimate of the uncertainty, while also accounting for the assumptions that are entailed with these numbers. The first proposition is well studied and the 95% confidence interval of the value of $[9.78 ; 9.84]\frac{m}{s^2}$ [@johannes_fundamentals_2009]. The second proposition is also quite well studied as we know that the density of the medium that the cup is dropped in is quite important and the shape and weight of the cup if dropped outside a vacuum.

There are 2 main points of the example which this thesis will explore, firstly uncertainties are organized in hierarchies and are just as important as beliefs as without one, the other is meaningless. Secondly, taking these uncertainties seriously and herein estimating and propagating them should not be a choice or something that can be avoided, but a necessity of all scientific endeavors, where they can be quantified or at least approximated. Taking its outset in the published literature, the thesis will establish issues regarding uncertainty handling and propagation and use simulations to highlight the problems with neglecting a proper account of uncertainty in statistical models. After highlighting these potential issues, the thesis will provide possible ways of dealing with the shortcomings by means of simulations. The goal of this thesis is to illuminate the often-overlooked uncertainties in the data collected on human behavior and cognition while providing ways of dealing with it, such that the uncertainty reported in the published literature more accurately reflects the (un)certainty we should have in the results. The thesis will argue that accounting for uncertainties is more important than ever, especially in research of complex systems such as humans as computational resources have made it possible to easily develop more sophisticated analyses and models that have dependencies on lower-level analyses, which makes the need for proper uncertainty handling even more imperative. Furthermore, these computational resources do allow for uncertainty propagation without understanding the underlying mathematics, making it accessible to most researchers with some coding experience. To effectively communicate both the statistical models as well as the underlying uncertainties associated with doing computations on data, the thesis will start by exploring different types of uncertainty in cognitive science and examine which parts of the literature might be more susceptible to overconfident findings by neglecting uncertainty propagation. Next the thesis will be investigating a particular cognitive model used in many subfields of cognitive science and examine how validation of such cognitive models have been done and how proper uncertainty handling can improve these validation steps.

## *Levels of uncertainty and uncertainty propagation*

I will here broadly define 3 different types of uncertainty, measurement, estimation, and test-retest reliability uncertainty see figure 1 and 2 for a visualization. These definitions are not exhaustive and will be centered around how experimental studies in cognitive science are conducted, from data collection to data analysis. The first aspect of uncertainty is to acknowledge that uncertainties can be defined in hierarchies as uncertainty propagates these hierarchies. This uncertainty propagation means that as you do calculations based on measures with uncertainty, the uncertainty propagates to the results of the calculations. In this thesis I will be using simulations to show how uncertainty propagation can be understood and handled with a need for rigorous mathematical proofs for a more mathematical treatment see [@saccenti_corruption_2020].

The lowest level of uncertainty is in the measurements themselves i.e. measurement uncertainty. Measurement uncertainty reflects the uncertainty in how well one can for instance measure the reaction time on a computer or the time it took the falling cup to reach the ground. This level of uncertainty is often neglected in cognitive science when applying statistical models, because they are thought to be minuscule as in the case of reaction time tasks, which may or may not be true given the experiment setup [@holden_accuracy_2019; @crocetta_problem_2015; @ohyanagi_solution_2010]. This is not to say that cognitive scientists do not care about them, as moving towards more sophisticated measurement methods is an ongoing endeavor. For instance, moving from lower to higher magnetic fields or more electrodes when using functional magnetic resonance imaging or electroencephalography respectively [@glover_overview_2011]. Minimizing this kind of uncertainty most often revolves around getting better tools to measure the variable(s) of interest. However, there might also be other avenues where a more explicit quantification of measurement uncertainty might be appropriate. One of the main instances coming to mind is the use of questionnaires, that try to quantify how mental health conditions such as depression or anxiety correlate with cognitive measures or parameters. Many of the main questionnaires used to assess depression, anxiety, stress etc. use several questions that are then added together to give a score of the mental health condition without a quantification of the uncertainty [@xiao_psychometric_2023; @cohen_perceived_1994; @kroenke_phq-9_2001; @johnson_psychometric_2019].

```{r figure1, fig.width = 7, fig.height = 4, warning = F, message = F, echo = F, fig.cap = "**Figure 1 Measurement and Estimation uncertainty;** displays a linear regression between two measurements with measurement uncertainty depicted as vertical and horizontal error bars on individual points. The mean of the regression line with and without propagated uncertainty is highlighted in grey and dark green respectively. Lastly a prediction interval is depicted as the shaded area around the mean of the regression line with and without propagated uncertainty again in grey and green respectively."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_1_measurement_uncertainty.png")), scale = 1)
```

The next level of uncertainty is when a particular model is fit to some data, or more broadly when calculations are done on data with uncertainty. In cognitive science we achieve parameter estimates from our collected data and these parameter estimates have uncertainty associated with them, this uncertainty will be referred to as estimation uncertainty. Estimation uncertainty is most often quantified by the statistical model be that the standard error of a regression coefficient or the width of a posterior distribution of a parameter in a Bayesian framework. Minimizing this estimation uncertainty is what most scientists care about, as inevitably most cognitive science experiments revolve around null hypothesis testing, which in most cases will involve testing whether the parameter estimate includes a particular value mostly, 0. To minimize this type of uncertainty the standard approach is to get more data, given they are from the same population and behave similarly. In cognitive science this might include achieving more trials or subjects to get a more precise estimate of interest i.e. minimizing estimation uncertainty. In cognitive science the minimizing of estimation uncertainty is however not free or free of uncertainty itself. Firstly, both approaches, increasing trials and subjects, utilize more resources, but more importantly increasing the number of trials in a cognitive task might even increase the estimation uncertainty itself. This can happen for several reasons, but boredom, habituation, fatigue and lack of engagement can become big contributors when experimental tasks become very long [@meier_is_2024; @jeong_exhaustive_2023]. Other contributors might be that subjects switch between cognitive strategies and if not properly accounted for in the analysis might be interpreted as additional noise by the model. Next increasing the number of subjects included in a study will many times decrease estimation uncertainty on population level estimates, if the sample population is homogeneous. The trade off between subjects and trials in an experiment is therefore quite important to minimize estimation uncertainty, but also minimize the overuse of resources. However, there are many times also other ways to minimize estimation uncertainty [@baldi_antognini_new_2023; @stone_using_2014]. For instance changing the task design such that responses will give more information on parameter values of interest. This optimization strategy involves individualizing the task design such that each presented stimulus is the most informative. This task design optimization is frequently used in psychophysical experiments where adaptive algorithms are used to select the upcoming stimuli such that it minimizes the uncertainty in the estimated parameter values. See for example algorithms like PSI, QUEST and ADOPY [@watson_quest_2017; @yang_adopy_2021; @prins_psi-marginal_2013]. The next level of uncertainty stems from the fact that these parameter estimates will vary over time, as humans vary over time both in terms of behavioral factors like learning, but also psychological factors such as mood and arousal [@schurr_dynamic_2024]. This type of uncertainty will be referred to as test-retest uncertainty.

To fully appreciate the relationship between these types of uncertainty and why they are hierarchically organized, one might think of a researcher that want to conduct an experiment on how total time spent awake (sleep deprivation) influences reaction times in an experimental paradigm. Now imagine that the total time spent awake is measured with uncertainty either because of the participants' accuracy in estimating when they fell asleep the previous day or in the device that measures it, this is one type of measurement uncertainty in the experiment. Now each reaction time of the participant also have measurement uncertainty, but more importantly because the reaction times are measured several times the mean reaction time is going to have estimation uncertainty associated with it, here the standard error of the mean. And this standard error of the mean is going to be influenced by the measurement uncertainty on each individual reaction time. Lastly reaction times fluctuate even in the same person from day to day highlighting test-retest uncertainty, meaning that if the test was repeated several times additional test-retest uncertainty is added.

```{r figure2, fig.width = 7, fig.height = 4, warning = F, message = F, echo = F, fig.cap = "**Figure 2 Test retest uncertainty;** displays the results of fitting the linear regression in Figure 1 twice with and without accounting for measurement uncertainty. Each facet represents one of the three parameters of the linear model, the intercept the residual uncertainty and the slope respectively from left to right. Colors represented weather the measurement uncertainty was proporgated or not."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_2_test_retest.png")), scale = 1)
```

The main message here is that to get reliable estimates and, in the end, to make reliable inference one needs to account for all these sorts of uncertainties and the lower in the hierarchy you move the more fundamental and important they become. Having a parameter estimate that is stable over time won't matter if you cannot estimate it or measure it reliably in the first place.
