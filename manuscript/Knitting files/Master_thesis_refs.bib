
@article{meier_is_2024,
	title = {Is boredom a source of noise and/or a confound in behavioral science research?},
	volume = {11},
	copyright = {2024 The Author(s)},
	issn = {2662-9992},
	url = {https://www.nature.com/articles/s41599-024-02851-7},
	doi = {10.1057/s41599-024-02851-7},
	abstract = {Behavioral researchers tend to study behavior in highly controlled laboratory settings to minimize the effects of potential confounders. Yet, while doing so, the artificial setup itself might unintentionally introduce noise or confounders, such as boredom. In this perspective, we draw upon theoretical and empirical evidence to make the case that (a) some experimental setups are likely to induce boredom in participants, (b) the degree of boredom induced might differ between individuals as a function of differences in trait boredom, (c) boredom can impair participants’ attention, can make study participation more effortful, and can increase the urge to do something else (i.e., to disengage from the study). Most importantly, we argue that some participants might adjust their behavior because they are bored. Considering boredom’s potential for adding noise to data, or for being an unwanted confound, we discuss a set of recommendations on how to control for and deal with the occurrence and effects of boredom in behavioral science research.},
	language = {en},
	number = {1},
	urldate = {2024-04-15},
	journal = {Humanities and Social Sciences Communications},
	author = {Meier, Maria and Martarelli, Corinna S. and Wolff, Wanja},
	month = mar,
	year = {2024},
	note = {Publisher: Palgrave},
	keywords = {Psychology, Language and linguistics},
	pages = {1--8},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\CQEHC28B\\Meier et al. - 2024 - Is boredom a source of noise andor a confound in .pdf:application/pdf},
}

@misc{holden_accuracy_2019,
	title = {Accuracy of different modalities of reaction time testing: {Implications} for online cognitive assessment tools},
	copyright = {© 2019, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	shorttitle = {Accuracy of different modalities of reaction time testing},
	url = {https://www.biorxiv.org/content/10.1101/726364v1},
	doi = {10.1101/726364},
	abstract = {Reaction time testing is widely used in computerized cognitive assessments, and clinical studies have repeatedly shown it to be a sensitive indicator of cognitive function. Typically, the reaction time test is administered by presenting a subject with a visual stimulus on a computer monitor and prompting the individual to respond (via keypad or computer mouse) as quickly as possible. The individual’s reaction time is calculated as the interval between presentation of the stimulus and the time recorded from the mechanical response. However, there are many inherent latencies and variabilities that may be introduced to the measure by both hardware (computer monitor and mouse) and software (operating system). Because of these delays, we hypothesized that a comparison of hardware protocols (excluding human response) would demonstrate significant differences in the resulting reaction time measures. To simulate the delays of various components of the common systems used to obtain reaction time, we conducted a simple experiment in which either a visual or tactile stimulus evoked a movement from a mechanical transducer to respond to a computer peripheral or a dedicated response device. In the first condition, a simulated visual reaction time test was conducted by flashing a visual stimulus on a computer monitor. The stimulus was detected by a dedicated light sensor, and a linear actuator delivered the mechanical response via computer mouse. The second test condition employed a mobile device as the medium for the visual stimulus, and the mechanical response was delivered to the mobile device’s touchscreen. The third and fourth test conditions simulated tactile reaction time tests in which the stimulus was generated by a dedicated hardware device. The third condition simulated a tactile stimulus, which was detected by a mechanical switch, and again a hardware device delivered the response via computer mouse. The fourth condition also simulated a tactile stimulus, but the response was delivered by a dedicated hardware device designed to store the interval between stimulus delivery and stimulus response. There were significant differences in the range of responses recorded from the four different conditions with the reaction time collected from a visual stimulus on a mobile device being the worst and the device with dedicated hardware designed for the task being the best. The results suggest that some of the commonly used visual tasks on consumer grade computers could be introducing significant errors for reaction time testing and that dedicated hardware designed for the reaction time task is needed to minimize testing errors.},
	language = {en},
	urldate = {2024-04-15},
	publisher = {bioRxiv},
	author = {Holden, Jameson and Francisco, Eric and Lensch, Rachel and Tommerdahl, Anna and Kirsch, Bryan and Zai, Laila and Dennis, Robert and Tommerdahl, Mark},
	month = aug,
	year = {2019},
	note = {Pages: 726364
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\N55SFM42\\Holden et al. - 2019 - Accuracy of different modalities of reaction time .pdf:application/pdf},
}

@article{ohyanagi_solution_2010,
	title = {A solution for measuring accurate reaction time to visual stimuli realized with a programmable microcontroller},
	volume = {42},
	issn = {1554-3528},
	doi = {10.3758/BRM.42.1.242},
	abstract = {This article presents a new solution for measuring accurate reaction time (SMART) to visual stimuli. The SMART is a USB device realized with a Cypress Programmable System-on-Chip (PSoC) mixed-signal array programmable microcontroller. A brief overview of the hardware and firmware of the PSoC is provided, together with the results of three experiments. In Experiment 1, we investigated the timing accuracy of the SMART in measuring reaction time (RT) under different conditions of operating systems (OSs; Windows XP or Vista) and monitor displays (a CRT or an LCD). The results indicated that the timing error in measuring RT by the SMART was less than 2 msec, on average, under all combinations of OS and display and that the SMART was tolerant to jitter and noise. In Experiment 2, we tested the SMART with 8 participants. The results indicated that there was no significant difference among RTs obtained with the SMART under the different conditions of OS and display. In Experiment 3, we used Microsoft (MS) PowerPoint to present visual stimuli on the display. We found no significant difference in RTs obtained using MS DirectX technology versus using the PowerPoint file with the SMART. We are certain that the SMART is a simple and practical solution for measuring RTs accurately. Although there are some restrictions in using the SMART with RT paradigms, the SMART is capable of providing both researchers and health professionals working in clinical settings with new ways of using RT paradigms in their work.},
	language = {eng},
	number = {1},
	journal = {Behavior Research Methods},
	author = {Ohyanagi, Toshio and Sengoku, Yasuhito},
	month = feb,
	year = {2010},
	pmid = {20160303},
	keywords = {Humans, Reaction Time, Visual Perception, Psychology},
	pages = {242--253},
	file = {Full Text:C\:\\Users\\au645332\\Zotero\\storage\\AWJYU8WQ\\Ohyanagi and Sengoku - 2010 - A solution for measuring accurate reaction time to.pdf:application/pdf},
}

@article{crocetta_problem_2015,
	title = {{THE} {PROBLEM} {OF} {MEASURING} {REACTION} {TIME} {USING} {SOFTWARE} {AND} {HARDWARE}: {A} {SYSTEMATIC} {REVIEW}},
	volume = {24},
	shorttitle = {{THE} {PROBLEM} {OF} {MEASURING} {REACTION} {TIME} {USING} {SOFTWARE} {AND} {HARDWARE}},
	abstract = {Reaction time (RT) is a sensitive measure and has been applied in modern Psychology and in Psychology of Sport and Exercise. This study aimed to identify potential delays in time measurement with the use of computers in RT research through the analysis of scientific production from articles published in the electronic database Web of Science (R) powered by Thomson Reuters Web of Knowledge(SM). The publications analyzed delays generated in the use of a CRT/LCD monitor, keyboard or mouse, finding significant differences. DOS, Mac, Linux and Windows Operating systems were investigated. The results of the studies reported delays ranging from 0.0006 to 80ms in the keyboards evaluated; 8 to 52ms in 'mice'; and 0.0005 to 68ms in the monitors. Although computers are widely used for RT measurements, the reliability of time measurement is directly related to the technical characteristics of the accessories used and the configuration of the operating system. We conclude that studies with computer-measured RT tests must be carefully analyzed, since the effectiveness of the time measurements is directly related to the type and quality of the equipment used, as well as the operating systems and software developed. A discussion regarding the scientific literature on the problem of using computers to perform tests involving time measurement, possible errors involving the use of software and hardware, as well as the solutions that have already been found to decrease their impact on time measurement is important for Psychology of Sport and Exercise, because it will enable us to suggest the care needed to obtain a reliable measurement.},
	journal = {Revista de Psicologia del Deporte},
	author = {Crocetta, Tania and Andrade, Alexandro},
	month = jan,
	year = {2015},
	pages = {341--349},
}

@article{glover_overview_2011,
	title = {Overview of {Functional} {Magnetic} {Resonance} {Imaging}},
	volume = {22},
	issn = {1042-3680},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3073717/},
	doi = {10.1016/j.nec.2010.11.001},
	abstract = {Blood Oxygen Level Dependent (BOLD) functional magnetic resonance imaging (fMRI) depicts changes in deoxyhemoglobin concentration consequent to task-induced or spontaneous modulation of neural metabolism. Since its inception in 1990, this method has been widely employed in thousands of studies of cognition for clinical applications such as surgical planning, for monitoring treatment outcomes, and as a biomarker in pharmacologic and training programs. Technical developments have solved most of the challenges of applying fMRI in practice. These challenges include low contrast to noise ratio of BOLD signals, image distortion, and signal dropout. More recently, attention is turning to the use of pattern classification and other statistical methods to draw increasingly complex inferences about cognitive brain states from fMRI data. This paper reviews the methods, some of the challenges and the future of fMRI.},
	number = {2},
	urldate = {2024-04-15},
	journal = {Neurosurgery clinics of North America},
	author = {Glover, Gary H.},
	month = apr,
	year = {2011},
	pmid = {21435566},
	pmcid = {PMC3073717},
	pages = {133--139},
	file = {PubMed Central Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\J7FGV7U4\\Glover - 2011 - Overview of Functional Magnetic Resonance Imaging.pdf:application/pdf},
}

@article{xiao_psychometric_2023,
	title = {Psychometric validation of the {Perceived} {Stress} {Scale} ({PSS}-10) among family caregivers of people with schizophrenia in {China}},
	volume = {13},
	issn = {2044-6055},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10689371/},
	doi = {10.1136/bmjopen-2023-076372},
	abstract = {Background
The 10-item Perceived Stress Scale (PSS-10) is a widely used measure of perceived stress that has been validated in various populations, yet with inconsistent results on its factor structure. The present study examines the reliability and validity of the PSS-10 in a population not previously examined: Chinese family caregivers of persons with schizophrenia, with a focus on factor analysis.

Methods
A sample of 449 family caregivers of persons with schizophrenia was recruited for psychometric testing of the scale. The factor structure of PSS-10 was tested by randomly dividing the sample into two groups for both exploratory factor analysis (EFA) and confirmatory factor analysis (CFA). The scale was further tested for internal consistency, test–retest reliability, convergent validity, discriminant validity and concurrent validity.

Results
EFA extracted two factors: perceived helplessness with six negative phrasing items and perceived efficacy with four positive phrasing items. CFA confirmed the structure of two factors with satisfactory model fit indices. Convergent validity was supported by high standard regression weight (0.78–0.92), average variance extracted (AVE=0.79–0.81) and composite reliability (0.88–0.94), while discriminant validity was confirmed by higher AVE estimates than the squared interconstruct correlations. The PSS-10 showed good internal consistency and test–retest reliability, with Cronbach’s alpha of 0.79 and intraclass correlation coefficient of 0.91, respectively. Concurrent validity was demonstrated by its significant positive correlations with stigma, depression and anxiety, as well as significant negative correlations with social support, family functioning and positive caregiving experiences.

Conclusion
The two-factor PSS-10 has good psychometric characteristics assessing the perceived stress of family caregivers of people with schizophrenia. The findings indicate that the PSS-10 can be used to measure perceived stress in future research and practice among caregivers of people with schizophrenia, and potentially, other caregiving samples.},
	number = {11},
	urldate = {2024-04-15},
	journal = {BMJ Open},
	author = {Xiao, Tao and Zhu, Feng and Wang, Dan and Liu, Xiang and Xi, Shi-Jun and Yu, Yu},
	month = nov,
	year = {2023},
	pmid = {38035751},
	pmcid = {PMC10689371},
	pages = {e076372},
	file = {PubMed Central Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\2M3X4XZQ\\Xiao et al. - 2023 - Psychometric validation of the Perceived Stress Sc.pdf:application/pdf},
}

@article{cohen_perceived_1994,
	title = {{PERCEIVED} {STRESS} {SCALE}},
	language = {en},
	author = {Cohen, Sheldon},
	month = jan,
	year = {1994},
	file = {Cohen - PERCEIVED STRESS SCALE.pdf:C\:\\Users\\au645332\\Zotero\\storage\\7CZXRAWE\\Cohen - PERCEIVED STRESS SCALE.pdf:application/pdf},
}

@article{kroenke_phq-9_2001,
	title = {The {PHQ}-9},
	volume = {16},
	issn = {0884-8734},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1495268/},
	doi = {10.1046/j.1525-1497.2001.016009606.x},
	abstract = {OBJECTIVE
While considerable attention has focused on improving the detection of depression, assessment of severity is also important in guiding treatment decisions. Therefore, we examined the validity of a brief, new measure of depression severity.

MEASUREMENTS
The Patient Health Questionnaire (PHQ) is a self-administered version of the PRIME-MD diagnostic instrument for common mental disorders. The PHQ-9 is the depression module, which scores each of the 9 DSM-IV criteria as “0” (not at all) to “3” (nearly every day). The PHQ-9 was completed by 6,000 patients in 8 primary care clinics and 7 obstetrics-gynecology clinics. Construct validity was assessed using the 20-item Short-Form General Health Survey, self-reported sick days and clinic visits, and symptom-related difficulty. Criterion validity was assessed against an independent structured mental health professional (MHP) interview in a sample of 580 patients.

RESULTS
As PHQ-9 depression severity increased, there was a substantial decrease in functional status on all 6 SF-20 subscales. Also, symptom-related difficulty, sick days, and health care utilization increased. Using the MHP reinterview as the criterion standard, a PHQ-9 score ≥10 had a sensitivity of 88\% and a specificity of 88\% for major depression. PHQ-9 scores of 5, 10, 15, and 20 represented mild, moderate, moderately severe, and severe depression, respectively. Results were similar in the primary care and obstetrics-gynecology samples.

CONCLUSION
In addition to making criteria-based diagnoses of depressive disorders, the PHQ-9 is also a reliable and valid measure of depression severity. These characteristics plus its brevity make the PHQ-9 a useful clinical and research tool.},
	number = {9},
	urldate = {2024-04-15},
	journal = {Journal of General Internal Medicine},
	author = {Kroenke, Kurt and Spitzer, Robert L and Williams, Janet B W},
	month = sep,
	year = {2001},
	pmid = {11556941},
	pmcid = {PMC1495268},
	pages = {606--613},
	file = {Full Text:C\:\\Users\\au645332\\Zotero\\storage\\CRHL6KX2\\Kroenke et al. - 2001 - The PHQ-9.pdf:application/pdf},
}

@article{johnson_psychometric_2019,
	title = {Psychometric {Properties} of the {General} {Anxiety} {Disorder} 7-{Item} ({GAD}-7) {Scale} in a {Heterogeneous} {Psychiatric} {Sample}},
	volume = {10},
	issn = {1664-1078},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6691128/},
	doi = {10.3389/fpsyg.2019.01713},
	abstract = {The GAD-7 is commonly used as a measure of general anxiety symptoms across various settings and populations. However, there has been disagreement regarding the factor structure of the GAD-7, and there is a need for larger studies investigating the psychometric properties of the measure. Patients undergoing treatment (N = 1201), both inpatient and outpatient patients, completed the GAD-7 at pre- and post-treatment. Measures of depression, well-being, and other anxiety measures were also completed, making it possible to investigate convergent and divergent validity. Internal consistency and convergent validity were excellent for the total sample, and there was acceptable variation related to treatment groups. We conducted an exploratory factor analysis (EFA) on a random sample (50\%) of the patients at intake and then conducted a confirmatory factor analysis (CFA) to confirm the factor structure in the other part of the sample at intake. The EFA indicated a clear one-factor solution, but the one-factor solution with CFA provided a poor fit to the data. Correlating the residuals among items assessing somatic symptoms led to a good fit in a respecified CFA solution. The GAD-7 has excellent internal consistency, and the one-factor structure in a heterogeneous clinical population was supported.},
	urldate = {2024-04-15},
	journal = {Frontiers in Psychology},
	author = {Johnson, Sverre Urnes and Ulvenes, Pål Gunnar and Øktedalen, Tuva and Hoffart, Asle},
	month = aug,
	year = {2019},
	pmid = {31447721},
	pmcid = {PMC6691128},
	pages = {1713},
	file = {PubMed Central Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\F3W4CPGZ\\Johnson et al. - 2019 - Psychometric Properties of the General Anxiety Dis.pdf:application/pdf},
}

@article{jeong_exhaustive_2023,
	title = {Exhaustive or exhausting? {Evidence} on respondent fatigue in long surveys},
	volume = {161},
	issn = {0304-3878},
	shorttitle = {Exhaustive or exhausting?},
	url = {https://www.sciencedirect.com/science/article/pii/S0304387822001341},
	doi = {10.1016/j.jdeveco.2022.102992},
	abstract = {Living standards measurement surveys require sustained attention for several hours. We quantify survey fatigue by randomizing the order of questions in 2–3 hour-long in-person surveys. An additional hour of survey time increases the probability that a respondent skips a question by 10\%–64\%. Because skips are more common, the total monetary value of aggregated categories such as assets or expenditures declines as the survey goes on, and this effect is sizeable for some categories: for example, an extra hour of survey time lowers food expenditures by 25\%. We find similar effect sizes within phone surveys in which respondents were already familiar with questions, suggesting that cognitive burden may be a key driver of survey fatigue.},
	urldate = {2024-04-15},
	journal = {Journal of Development Economics},
	author = {Jeong, Dahyeon and Aggarwal, Shilpa and Robinson, Jonathan and Kumar, Naresh and Spearot, Alan and Park, David Sungho},
	month = mar,
	year = {2023},
	keywords = {Measurement, Design of experiments, Survey fatigue, Survey methodology},
	pages = {102992},
	file = {ScienceDirect Snapshot:C\:\\Users\\au645332\\Zotero\\storage\\R97YGB55\\S0304387822001341.html:text/html},
}

@article{baldi_antognini_new_2023,
	title = {New insights into adaptive enrichment designs},
	volume = {64},
	issn = {1613-9798},
	url = {https://doi.org/10.1007/s00362-023-01433-0},
	doi = {10.1007/s00362-023-01433-0},
	abstract = {The transition towards personalized medicine is happening and the new experimental framework is raising several challenges, from a clinical, ethical, logistical, regulatory, and statistical perspective. To face these challenges, innovative study designs with increasing complexity have been proposed. In particular, adaptive enrichment designs are becoming more attractive for their flexibility. However, these procedures rely on an increasing number of parameters that are unknown at the planning stage of the clinical trial, so the study design requires particular care. This review is dedicated to adaptive enrichment studies with a focus on design aspects. While many papers deal with methods for the analysis, the sample size determination and the optimal allocation problem have been overlooked. We discuss the multiple aspects involved in adaptive enrichment designs that contribute to their advantages and disadvantages. The decision-making process of whether or not it is worth enriching should be driven by clinical and ethical considerations as well as scientific and statistical concerns.},
	language = {en},
	number = {4},
	urldate = {2024-04-15},
	journal = {Statistical Papers},
	author = {Baldi Antognini, Alessandro and Frieri, Rosamarie and Zagoraiou, Maroussa},
	month = aug,
	year = {2023},
	keywords = {Continuous biomarker, Personalized medicine, Predictive biomarker, Stratified medicine, Subgroup identification},
	pages = {1305--1328},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\9Q9TWZ9Q\\Baldi Antognini et al. - 2023 - New insights into adaptive enrichment designs.pdf:application/pdf},
}

@article{stone_using_2014,
	title = {Using reaction times and binary responses to estimate psychophysical performance: an information theoretic analysis},
	volume = {8},
	issn = {1662-453X},
	shorttitle = {Using reaction times and binary responses to estimate psychophysical performance},
	url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2014.00035/full},
	doi = {10.3389/fnins.2014.00035},
	abstract = {{\textless}p{\textgreater}As the strength of a stimulus increases, the proportions of correct binary responses increases, which define the psychometric function. Simultaneously, mean reaction times (RT) decrease, which collectively define the chronometric function. However, RTs are traditionally ignored when estimating psychophysical parameters, even though they may provide additional Shannon information. Here, we extend Palmer et al's ({\textless}xref ref-type="bibr" rid="B6"{\textgreater}2005{\textless}/xref{\textgreater}) proportional-rate diffusion model (PRD) by: (a) fitting individual RTs to an inverse Gaussian distribution, (b) including lapse rate, (c) point-of-subjective-equality (PSE) parameters, and, (d) using a two-alternative forced choice (2AFC) design based on the proportion of times a variable comparison stimulus is chosen. Maximum likelihood estimates of mean RT values (from fitted inverse Gaussians) and binary responses were fitted both separately and in combination to this extended PRD (EPRD) model, to obtain psychophysical parameter values. Values estimated from binary responses alone (i.e., the psychometric function) were found to be similar to those estimated from RTs alone (i.e., the chronometric function), which provides support for the underlying diffusion model. The EPRD model was then used to estimate the mutual information between binary responses and stimulus strength, and between RTs and stimulus strength. These provide conservative bounds for the average amount of Shannon information the observer gains about stimulus strength on each trial. For the human experiment reported here, the observer gains between 2.68 and 3.55 bits/trial. These bounds are monotonically related to a new measure, the {\textless}italic{\textgreater}Shannon increment{\textless}/italic{\textgreater}, which is the expected value of the smallest change in stimulus strength detectable by an observer.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-04-15},
	journal = {Frontiers in Neuroscience},
	author = {Stone, James V.},
	month = mar,
	year = {2014},
	note = {Publisher: Frontiers},
	keywords = {Reaction Time, 2AFC, chronometric function, diffusion model, mutual information., point of subjective equality, PSE, psychometric function, Shannon information, threshold},
	file = {Full Text:C\:\\Users\\au645332\\Zotero\\storage\\MHZ3VQ3N\\Stone - 2014 - Using reaction times and binary responses to estim.pdf:application/pdf},
}

@article{watson_quest_2017,
	title = {{QUEST}+: {A} general multidimensional {Bayesian} adaptive psychometric method},
	volume = {17},
	issn = {1534-7362},
	shorttitle = {{QUEST}+},
	url = {https://doi.org/10.1167/17.3.10},
	doi = {10.1167/17.3.10},
	abstract = {QUEST+ is a Bayesian adaptive psychometric testing method that allows an arbitrary number of stimulus dimensions, psychometric function parameters, and trial outcomes. It is a generalization and extension of the original QUEST procedure and incorporates many subsequent developments in the area of parametric adaptive testing. With a single procedure, it is possible to implement a wide variety of experimental designs, including conventional threshold measurement; measurement of psychometric function parameters, such as slope and lapse; estimation of the contrast sensitivity function; measurement of increment threshold functions; measurement of noise-masking functions; Thurstone scale estimation using pair comparisons; and categorical ratings on linear and circular stimulus dimensions. QUEST+ provides a general method to accelerate data collection in many areas of cognitive and perceptual science.},
	number = {3},
	urldate = {2024-04-15},
	journal = {Journal of Vision},
	author = {Watson, Andrew B.},
	month = apr,
	year = {2017},
	pages = {10},
	file = {Full Text:C\:\\Users\\au645332\\Zotero\\storage\\IZTMT7TH\\Watson - 2017 - QUEST+ A general multidimensional Bayesian adapti.pdf:application/pdf},
}

@article{yang_adopy_2021,
	title = {{ADOpy}: a python package for adaptive design optimization},
	volume = {53},
	issn = {1554-3528},
	shorttitle = {{ADOpy}},
	url = {https://doi.org/10.3758/s13428-020-01386-4},
	doi = {10.3758/s13428-020-01386-4},
	abstract = {Experimental design is fundamental to research, but formal methods to identify good designs are lacking. Advances in Bayesian statistics and machine learning offer algorithm-based ways to identify good experimental designs. Adaptive design optimization (ADO; Cavagnaro, Myung, Pitt, \& Kujala, 2010; Myung, Cavagnaro, \& Pitt, 2013) is one such method. It works by maximizing the informativeness and efficiency of data collection, thereby improving inference. ADO is a general-purpose method for conducting adaptive experiments on the fly and can lead to rapid accumulation of information about the phenomenon of interest with the fewest number of trials. The nontrivial technical skills required to use ADO have been a barrier to its wider adoption. To increase its accessibility to experimentalists at large, we introduce an open-source Python package, ADOpy, that implements ADO for optimizing experimental design. The package, available on GitHub, is written using high-level modular-based commands such that users do not have to understand the computational details of the ADO algorithm. In this paper, we first provide a tutorial introduction to ADOpy and ADO itself, and then illustrate its use in three walk-through examples: psychometric function estimation, delay discounting, and risky choice. Simulation data are also provided to demonstrate how ADO designs compare with other designs (random, staircase).},
	language = {en},
	number = {2},
	urldate = {2024-04-15},
	journal = {Behavior Research Methods},
	author = {Yang, Jaeyeong and Pitt, Mark A. and Ahn, Woo-Young and Myung, Jay I.},
	month = apr,
	year = {2021},
	keywords = {Bayesian adaptive experimentation, Cognitive modeling, Delay discounting, Optimal experimental design, Psychometric function estimation, Risky choice},
	pages = {874--897},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\5LR6IE4B\\Yang et al. - 2021 - ADOpy a python package for adaptive design optimi.pdf:application/pdf},
}

@article{prins_psi-marginal_2013,
	title = {The psi-marginal adaptive method: {How} to give nuisance parameters the attention they deserve (no more, no less)},
	volume = {13},
	issn = {1534-7362},
	shorttitle = {The psi-marginal adaptive method},
	doi = {10.1167/13.7.3},
	abstract = {Adaptive testing methods serve to maximize the information gained regarding the values of the parameters of a psychometric function (PF). Such methods typically target only one or two ("threshold" and "slope") of the PF's four parameters while assuming fixed values for the "nuisance" parameters ("guess rate" and "lapse rate"). Here I propose the "psi-marginal" adaptive method, which can target nuisance parameters but only when this is the most optimal manner in which to reduce uncertainty in the value of the parameters of primary interest. The method is based on Kontsevich and Tyler's (1999) psi-method. However, in the proposed method a posterior distribution defined across all parameters of unknown value is maintained. Each of the parameters is specified either as a parameter of primary interest whose estimation should be optimized or as a nuisance parameter whose estimation should be subservient to the estimation of the parameters of primary interest. Critically, selection of stimulus intensities is based on the expected information gain in the marginal posterior distribution defined across the parameters of interest only. The appeal of this method is that it will target nuisance parameters adaptively and only when doing so maximizes the expected information gain regarding the values of the parameters of interest. Simulations indicate that treating the lapse rate as a nuisance parameter in the psi-marginal method results in smaller bias and higher precision in threshold and slope estimates compared to the original psi method. The method is highly flexible and various other uses are discussed.},
	language = {eng},
	number = {7},
	journal = {Journal of Vision},
	author = {Prins, Nicolaas},
	month = jun,
	year = {2013},
	pmid = {23750016},
	keywords = {Attention, Data Interpretation, Statistical, Psychophysics, Visual Perception, Psychometrics, Sensory Thresholds, Bayesian adaptive method, Regression Analysis, Computer Simulation, psychometric function, alternative forced choice, maximum likelihood, yes/no task},
	pages = {3},
}

@article{de_berker_computations_2016,
	title = {Computations of uncertainty mediate acute stress responses in humans},
	volume = {7},
	copyright = {2016 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/ncomms10996},
	doi = {10.1038/ncomms10996},
	abstract = {The effects of stress are frequently studied, yet its proximal causes remain unclear. Here we demonstrate that subjective estimates of uncertainty predict the dynamics of subjective and physiological stress responses. Subjects learned a probabilistic mapping between visual stimuli and electric shocks. Salivary cortisol confirmed that our stressor elicited changes in endocrine activity. Using a hierarchical Bayesian learning model, we quantified the relationship between the different forms of subjective task uncertainty and acute stress responses. Subjective stress, pupil diameter and skin conductance all tracked the evolution of irreducible uncertainty. We observed a coupling between emotional and somatic state, with subjective and physiological tuning to uncertainty tightly correlated. Furthermore, the uncertainty tuning of subjective and physiological stress predicted individual task performance, consistent with an adaptive role for stress in learning under uncertain threat. Our finding that stress responses are tuned to environmental uncertainty provides new insight into their generation and likely adaptive function.},
	language = {en},
	number = {1},
	urldate = {2024-04-15},
	journal = {Nature Communications},
	author = {de Berker, Archy O. and Rutledge, Robb B. and Mathys, Christoph and Marshall, Louise and Cross, Gemma F. and Dolan, Raymond J. and Bestmann, Sven},
	month = mar,
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Stress and resilience},
	pages = {10996},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\U2PKT3BX\\de Berker et al. - 2016 - Computations of uncertainty mediate acute stress r.pdf:application/pdf},
}

@article{luijcks_influence_2015,
	title = {The {Influence} of {Perceived} {Stress} on {Cortical} {Reactivity}: {A} {Proof}-{Of}-{Principle} {Study}},
	volume = {10},
	issn = {1932-6203},
	shorttitle = {The {Influence} of {Perceived} {Stress} on {Cortical} {Reactivity}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4475054/},
	doi = {10.1371/journal.pone.0129220},
	abstract = {The aim of this study was to investigate how perceived stress may affect electroencephalographical (EEG) activity in a stress paradigm in a sample of 76 healthy participants. EEG activity was analyzed using multilevel modeling, allowing estimation of nested effects (EEG time segments within subjects). The stress paradigm consisted of a 3-minute pre-stimulus stress period and a 2-minute post-stimulus phase. At t=3 minutes, a single electrical stimulus was delivered. Participants were unaware of the precise moment of stimulus delivery and its intensity level. In the EEG time course of alpha activity, a stronger increase was observed during the post-stimulus period as compared to the pre-stimulus period. An opposite time course effect was apparent for gamma activity. Both effects were in line with a priori expectations and support the validity of this experimental EEG-stress paradigm. Secondly, we investigated whether interaction effects of stress and coping, as measured with the Perceived Stress Scale-10 questionnaire (PSS-10), could be demonstrated. A higher perceived stress score was accompanied by a greater increase in delta- and theta-activity during the post-stimulus phase, compared to low scores. In contrast, low coping capacity was associated with a stronger decrease in slow beta, fast beta and gamma activity during the post-stimulus phase. The results of the present article may be interpreted as proof-of-principle that EEG stress-related activity depends on the level of subjectively reported perceived stress. The inclusion of psychosocial variables measuring coping styles as well as stress-related personality aspects permits further examination of the interconnection between mind and body and may inform on the process of transformation from acute to chronic stress.},
	number = {6},
	urldate = {2024-04-15},
	journal = {PLoS ONE},
	author = {Luijcks, Rosan and Vossen, Catherine J. and Hermens, Hermie J. and van Os, Jim and Lousberg, Richel},
	month = jun,
	year = {2015},
	pmid = {26090882},
	pmcid = {PMC4475054},
	pages = {e0129220},
	file = {PubMed Central Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\SRGAZG5Y\\Luijcks et al. - 2015 - The Influence of Perceived Stress on Cortical Reac.pdf:application/pdf},
}

@article{treadway_perceived_2013,
	title = {Perceived stress predicts altered reward and loss feedback processing in medial prefrontal cortex},
	volume = {7},
	issn = {1662-5161},
	url = {https://www.frontiersin.org/articles/10.3389/fnhum.2013.00180},
	doi = {10.3389/fnhum.2013.00180},
	abstract = {Stress is significant risk factor for the development of psychopathology, particularly symptoms related to reward processing. Importantly, individuals display marked variation in how they perceive and cope with stressful events, and such differences are strongly linked to risk for developing psychiatric symptoms following stress exposure. However, many questions remain regarding the neural architecture that underlies inter-subject variability in perceptions of stressors. Using functional magnetic resonance imaging (fMRI) during a monetary incentive delay paradigm, we examined the effects of self-reported perceived stress levels on neural activity during reward anticipation and feedback in a sample of healthy individuals. We found that subjects reporting more uncontrollable and overwhelming stressors displayed blunted neural responses in medial prefrontal cortex (mPFC) following feedback related to monetary gains as well monetary losses. This is consistent with preclinical models that implicate the mPFC as a key site of vulnerability to the noxious effects of uncontrollable stressors. Our data help translate these findings to humans, and elucidate some of the neural mechanisms that may underlie stress-linked risk for developing reward-related psychiatric symptoms.},
	language = {English},
	urldate = {2024-04-15},
	journal = {Frontiers in Human Neuroscience},
	author = {Treadway, Michael T. and Buckholtz, Joshua W. and Zald, David},
	month = may,
	year = {2013},
	note = {Publisher: Frontiers},
	keywords = {insula, medial prefrontal cortex (mPFC), monetary incentive delay task, perceived stress, reward processing},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\JTP5IAJG\\Treadway et al. - 2013 - Perceived stress predicts altered reward and loss .pdf:application/pdf},
}

@article{kuiper_global_1986,
	title = {Global perceived stress level as a moderator of the relationship between negative life events and depression},
	volume = {12},
	issn = {0097-840X},
	doi = {10.1080/0097840X.1986.9936781},
	abstract = {The present research investigated the proposal that global perceived stress level moderates the degree of relationship between negative life events and depression. Accordingly, subjects in this study completed the Beck Depression Inventory (BDI), the Life Experiences Survey (LES), and the Perceived Stress Scale (PSS). The PSS provides a measure of global perceived stress level, or the general tendency to view one's life as being unpredictable, out of control, and overwhelming. Consistent with past research, the findings revealed an increase in depression level as negative life change scores increased. Of special importance, however, was the finding that global level of stress significantly moderated the relationship between depression and negative life events. For those low on perceived stress, negative life changes had only a minimal impact on depression level. In contrast, for those high on perceived stress, the relationship was more pronounced. These findings were then discussed with regard to the possible role of cognitive appraisals in enhancing the symptoms of psychopathology experienced by individuals high on global level of perceived stress.},
	language = {eng},
	number = {4},
	journal = {Journal of Human Stress},
	author = {Kuiper, N. A. and Olinger, L. J. and Lyons, L. M.},
	year = {1986},
	pmid = {3559198},
	keywords = {Humans, Adult, Male, Female, Psychological Tests, Depressive Disorder, Adaptation, Psychological, Adjustment Disorders, Life Change Events, Risk},
	pages = {149--153},
}

@article{wu_neurobiological_2021,
	title = {Neurobiological effects of perceived stress are different between adolescents and middle-aged adults},
	volume = {15},
	issn = {1931-7557},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8032601/},
	doi = {10.1007/s11682-020-00294-7},
	abstract = {Stress is an inevitable element of everyday living. Developmental studies suggested that adolescents are more vulnerable and sensitive to the effect of stress due to their developing brains, especially in areas related to stress perception and processing. This voxel-based morphometry study examined the association between various neurobiological markers and the level of perceived stress experienced by adolescents (n = 26) and middle-aged adults (n = 26). Our findings indicated that differences existed in the relationships between perceived stress and the structural volume of the orbitofrontal cortex (OFC) extending to the insula and amygdala. Specifically, the levels of perceived stress and the grey matter volume of the orbitofrontal cortex, the insula, and the amygdala were positively related in adolescents but negatively related for adults. Furthermore, a significant negative correlation between perceived stress and cortisol levels was observed in adults, whereas the relationship between perceived stress and cortisol levels was not significant for adolescents. Perceived stress measurement may be better than cortisol levels in terms of reflecting the emotional states of adolescents. In sum, the relationships between perceived stress and neurobiological markers were different between adolescents and middle-aged adults and thus appeared to be age dependent.},
	number = {2},
	urldate = {2024-04-15},
	journal = {Brain Imaging and Behavior},
	author = {Wu, Jingsong and Tong, Horace and Liu, Zhongwan and Tao, Jing and Chen, Lidian and Chan, Chetwyn C. H. and Lee, Tatia M. C.},
	year = {2021},
	pmid = {32737826},
	pmcid = {PMC8032601},
	pages = {846--854},
	file = {PubMed Central Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\ER62NCPY\\Wu et al. - 2021 - Neurobiological effects of perceived stress are di.pdf:application/pdf},
}

@book{ivanova_beyond_2022,
	title = {Beyond linear regression: mapping models in cognitive neuroscience should align with research goals},
	shorttitle = {Beyond linear regression},
	abstract = {Many cognitive neuroscience studies use large feature sets to predict and interpret brain activity patterns. Feature sets take many forms, from human stimulus annotations to representations in deep neural networks. Of crucial importance in all these studies is the mapping model, which defines the space of possible relationships between features and neural data. Until recently, most encoding and decoding studies have used linear mapping models. Increasing availability of large datasets and computing resources has recently allowed some researchers to employ more flexible nonlinear mapping models instead; however, the question of whether nonlinear mapping models can yield meaningful scientific insights remains debated. Here, we discuss the choice of a mapping model in the context of three overarching desiderata: predictive accuracy, interpretability, and biological plausibility. We show that, contrary to popular intuition, these desiderata do not map cleanly onto the linear/nonlinear divide; instead, each desideratum can refer to multiple research goals, each of which imposes its own constraints on the mapping model. Moreover, we argue that, instead of categorically treating the mapping models as linear or nonlinear, we should instead aim to estimate the complexity of these models. We show that, in many cases, complexity provides a more accurate reflection of restrictions imposed by various research goals. Finally, we outline several complexity metrics that can be used to effectively evaluate mapping models.},
	author = {Ivanova, Anna and Schrimpf, Martin and Anzellotti, Stefano and Zaslavsky, Noga and Fedorenko, Evelina and Isik, Leyla},
	month = aug,
	year = {2022},
	doi = {10.48550/arXiv.2208.10668},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\KECKLGAA\\Ivanova et al. - 2022 - Beyond linear regression mapping models in cognit.pdf:application/pdf},
}

@article{wilson_ten_2019,
	title = {Ten simple rules for the computational modeling of behavioral data},
	volume = {8},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.49547},
	doi = {10.7554/eLife.49547},
	abstract = {Computational modeling of behavior has revolutionized psychology and neuroscience. By fitting models to experimental data we can probe the algorithms underlying behavior, find neural correlates of computational variables and better understand the effects of drugs, illness and interventions. But with great power comes great responsibility. Here, we offer ten simple rules to ensure that computational modeling is used with care and yields meaningful insights. In particular, we present a beginner-friendly, pragmatic and details-oriented introduction on how to relate models to data. What, exactly, can a model tell us about the mind? To answer this, we apply our rules to the simplest modeling techniques most accessible to beginning modelers and illustrate them with examples and code available online. However, most rules apply to more advanced techniques. Our hope is that by following our guidelines, researchers will avoid many pitfalls and unleash the power of computational modeling on their own data.},
	urldate = {2024-04-16},
	journal = {eLife},
	author = {Wilson, Robert C and Collins, Anne GE},
	editor = {Behrens, Timothy E},
	month = nov,
	year = {2019},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {computational modeling, model fitting, reproducibility, validation},
	pages = {e49547},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\FEUESJGK\\Wilson and Collins - 2019 - Ten simple rules for the computational modeling of.pdf:application/pdf},
}

@article{courtin_spatial_2023,
	title = {Spatial summation of cold and warm detection: {Evidence} for increased precision when brisk stimuli are delivered over larger area},
	volume = {797},
	issn = {0304-3940},
	shorttitle = {Spatial summation of cold and warm detection},
	url = {https://www.sciencedirect.com/science/article/pii/S0304394023000046},
	doi = {10.1016/j.neulet.2023.137050},
	abstract = {Cold and warm stimuli delivered over a larger skin area tend to be more easily detected/elicit stronger sensations, a phenomenon referred to as spatial summation. The aim of the present study was to clarify how stimulation area affects thermal detection processes by evaluating whether increasing the stimulation area simply reduces the detection threshold or also reduces the uncertainty of the detection process. Psychometric functions were fitted to the detection performance of 16 healthy subjects. Stimuli (duration: 200 ms; rate of change: 300 °C/s) were delivered to the volar forearm using a Peltier-effect contact thermode and three different stimulation surfaces (23 mm2, 69 mm2, and 116 mm). Stimulation intensities were selected trial-by-trial by the psi marginal method to optimize estimation of slope and threshold parameters of the psychometric function. The raw data (100 stimulus–response pairs per subject per surface and per modality) was used to fit group-level hierarchical models of cold and warm detection, allowing to assess the effect of stimulation surface and account for inter-individual variability. Increasing stimulation area led to a compression of the psychometric function towards baseline skin temperature (reduced threshold and steeper slope), suggesting that spatial summation reflects a change in the precision of the neural representation of the stimulus which in turn influences the ability of the nervous system to distinguish true stimuli from sensory noise. Regardless of area, with the stimulation settings used in this study, cold detection appeared easier than warm detection, possibly because of structural and functional differences between cold- and warm-sensitive afferents.},
	urldate = {2024-04-16},
	journal = {Neuroscience Letters},
	author = {Courtin, Arthur S. and Delvaux, Aurore and Dufour, Arthur and Mouraux, André},
	month = feb,
	year = {2023},
	keywords = {Spatial summation, Psychometric function, Cold detection, Precision weighting, Warm detection},
	pages = {137050},
	file = {ScienceDirect Snapshot:C\:\\Users\\au645332\\Zotero\\storage\\W8FUIEN2\\S0304394023000046.html:text/html},
}

@article{sternberg_memory-scanning_1969,
	title = {Memory-{Scanning}: {Mental} {Processes} {Revealed} by {Reaction}-{Time} {Experiments}},
	volume = {57},
	issn = {0003-0996},
	shorttitle = {Memory-{Scanning}},
	url = {https://www.jstor.org/stable/27828738},
	number = {4},
	urldate = {2024-04-16},
	journal = {American Scientist},
	author = {Sternberg, Saul},
	year = {1969},
	note = {Publisher: Sigma Xi, The Scientific Research Society},
	pages = {421--457},
}

@article{macleod_training_1988,
	title = {Training and {Stroop}-like interference: {Evidence} for a continuum of automaticity},
	volume = {14},
	issn = {1939-1285},
	shorttitle = {Training and {Stroop}-like interference},
	doi = {10.1037/0278-7393.14.1.126},
	abstract = {Three experiments varied the extent of practice in an analog of the Stroop color-word task. The experiments involved 4 phases: baseline naming of 4 familiar colors; training in consistently naming 4 novel shapes by using the names of the same 4 colors; naming the colors when they appeared in the form of the shapes; and naming the shapes when they appeared in color. In Exp 1, with up to 2 hr of training in shape naming, colors were named much faster than shapes. Interference was observed only in Phase 4. In Exp 2 (5 hr of training) shape naming sped up, but was still slower than color naming. There was symmetrical interference in Phases 3 and 4 that persisted 3 mo without further training. Exp 3 extended practice to 20 hr, by which time shape and color naming were equally rapid. After 20 hr, interference appeared only in Phase 3, reversing the original asymmetry. The overall pattern is inconsistent with a simple speed of processing account of interference. Implications of the alternative idea of a continuum of automaticity—a direct consequence of training—are considered. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {1},
	journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {MacLeod, Colin M. and Dunbar, Kevin},
	year = {1988},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Interference (Learning), Color Perception, Form and Shape Perception, Naming, Stroop Effect},
	pages = {126--135},
	file = {Snapshot:C\:\\Users\\au645332\\Zotero\\storage\\MF46PECL\\1989-07414-001.html:text/html},
}

@article{pirolli_role_1985,
	title = {The role of practice in fact retrieval},
	volume = {11},
	issn = {1939-1285},
	doi = {10.1037/0278-7393.11.1.136},
	abstract = {Investigated in 4 experiments, each conducted with 8–24 undergraduates, the relationship among massive practice, speed-up of memory retrieval, and the reduction of long-term memory interference. The present experiments employed a multiday fact recognition procedure. Interference was varied by a fan manipulation: Concepts could appear in more than 1 fact (fan) or only 1 fact (no fan). Results show that fact retrieval speeded up as a power function of days of practice, but the number of daily repetitions beyond 4 produced little or no impact on RT. Interference decreased in proportion to the degree of practice but did not disappear even with 25 days of practice. Practice on specific facts and practice on the general task had multiplicative effects in reducing recognition time. General task practice decreased interference, suggesting that general central processes speeded up. It is noted that most but not all of these practice effects and their interaction with interference are predicted by the model of fact retrieval developed by the present 2nd author (see record 1982-27252-001). (40 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {1},
	journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Pirolli, Peter L. and Anderson, John R.},
	year = {1985},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Reaction Time, Human Information Storage, Interference (Learning), Practice, Long Term Memory},
	pages = {136--153},
	file = {Snapshot:C\:\\Users\\au645332\\Zotero\\storage\\69LP2CDY\\1986-00316-001.html:text/html},
}

@article{kubinec_ordered_2023,
	title = {Ordered {Beta} {Regression}: {A} {Parsimonious}, {Well}-{Fitting} {Model} for {Continuous} {Data} with {Lower} and {Upper} {Bounds}},
	volume = {31},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Ordered {Beta} {Regression}},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/abs/ordered-beta-regression-a-parsimonious-wellfitting-model-for-continuous-data-with-lower-and-upper-bounds/89F4141DA16D4FC217809B5EB45EEE83},
	doi = {10.1017/pan.2022.20},
	abstract = {I propose a new model, ordered Beta regression, for continuous distributions with both lower and upper bounds, such as data arising from survey slider scales, visual analog scales, and dose–response relationships. This model employs the cut point technique popularized by ordered logit to fit a single linear model to both continuous (0,1) and degenerate [0,1] responses. The model can be estimated with or without observations at the bounds, and as such is a general solution for these types of data. Employing a Monte Carlo simulation, I show that the model is noticeably more efficient than ordinary least squares regression, zero-and-one-inflated Beta regression, rescaled Beta regression, and fractional logit while fully capturing nuances in the outcome. I apply the model to a replication of the Aidt and Jensen (2014, European Economic Review 72, 52–75) study of suffrage extensions in Europe. The model can be fit with the R package ordbetareg to facilitate hierarchical, dynamic, and multivariate modeling.},
	language = {en},
	number = {4},
	urldate = {2024-04-16},
	journal = {Political Analysis},
	author = {Kubinec, Robert},
	month = oct,
	year = {2023},
	keywords = {Bayesian statistics, limited dependent variables, regression modeling},
	pages = {519--536},
	file = {Submitted Version:C\:\\Users\\au645332\\Zotero\\storage\\T23IQLWQ\\Kubinec - 2023 - Ordered Beta Regression A Parsimonious, Well-Fitt.pdf:application/pdf},
}

@article{legrand_heart_2022,
	title = {The heart rate discrimination task: {A} psychophysical method to estimate the accuracy and precision of interoceptive beliefs},
	volume = {168},
	issn = {0301-0511},
	shorttitle = {The heart rate discrimination task},
	url = {https://www.sciencedirect.com/science/article/pii/S0301051121002325},
	doi = {10.1016/j.biopsycho.2021.108239},
	abstract = {Interoception - the physiological sense of our inner bodies - has risen to the forefront of psychological and psychiatric research. Much of this research utilizes tasks that attempt to measure the ability to accurately detect cardiac signals. Unfortunately, these approaches are confounded by well-known issues limiting their validity and interpretation. At the core of this controversy is the role of subjective beliefs about the heart rate in confounding measures of interoceptive accuracy. Here, we recast these beliefs as an important part of the causal machinery of interoception, and offer a novel psychophysical “heart rate discrimination“ method to estimate their accuracy and precision. By applying this task in 223 healthy participants, we demonstrate that cardiac interoceptive beliefs are more biased, less precise, and are associated with poorer metacognitive insight relative to an exteroceptive control condition. Our task, provided as an open-source python package, offers a robust approach to quantifying cardiac beliefs.},
	urldate = {2024-04-16},
	journal = {Biological Psychology},
	author = {Legrand, Nicolas and Nikolova, Niia and Correa, Camile and Brændholt, Malthe and Stuckert, Anna and Kildahl, Nanna and Vejlø, Melina and Fardo, Francesca and Allen, Micah},
	month = feb,
	year = {2022},
	keywords = {Psychophysics, Heart rate discrimination, Heartbeat tracking, Interoception, Metacognition},
	pages = {108239},
	file = {Full Text:C\:\\Users\\au645332\\Zotero\\storage\\RC7UDV8H\\Legrand et al. - 2022 - The heart rate discrimination task A psychophysic.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\au645332\\Zotero\\storage\\RNLRA6ER\\S0301051121002325.html:text/html},
}

@article{yao_using_2018,
	title = {Using {Stacking} to {Average} {Bayesian} {Predictive} {Distributions} (with {Discussion})},
	volume = {13},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-13/issue-3/Using-Stacking-to-Average-Bayesian-Predictive-Distributions-with-Discussion/10.1214/17-BA1091.full},
	doi = {10.1214/17-BA1091},
	abstract = {Bayesian model averaging is flawed in the M-open setting in which the true data-generating process is not one of the candidate models being fit. We take the idea of stacking from the point estimation literature and generalize to the combination of predictive distributions. We extend the utility function to any proper scoring rule and use Pareto smoothed importance sampling to efficiently compute the required leave-one-out posterior distributions. We compare stacking of predictive distributions to several alternatives: stacking of means, Bayesian model averaging (BMA), Pseudo-BMA, and a variant of Pseudo-BMA that is stabilized using the Bayesian bootstrap. Based on simulations and real-data applications, we recommend stacking of predictive distributions, with bootstrapped-Pseudo-BMA as an approximate alternative when computation cost is an issue.},
	number = {3},
	urldate = {2024-04-16},
	journal = {Bayesian Analysis},
	author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
	month = sep,
	year = {2018},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Stan, Bayesian model averaging, model combination, predictive distribution, proper scoring rule, stacking},
	pages = {917--1007},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\JINFKN5M\\Yao et al. - 2018 - Using Stacking to Average Bayesian Predictive Dist.pdf:application/pdf},
}

@article{vehtari_practical_2017,
	title = {Practical {Bayesian} model evaluation using leave-one-out cross-validation and {WAIC}},
	volume = {27},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-016-9696-4},
	doi = {10.1007/s11222-016-9696-4},
	abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
	language = {en},
	number = {5},
	urldate = {2024-04-16},
	journal = {Statistics and Computing},
	author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
	month = sep,
	year = {2017},
	keywords = {Stan, Bayesian computation, K-fold cross-validation, Leave-one-out cross-validation (LOO), Pareto smoothed importance sampling (PSIS), Widely applicable information criterion (WAIC)},
	pages = {1413--1432},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\E8PM6R7D\\Vehtari et al. - 2017 - Practical Bayesian model evaluation using leave-on.pdf:application/pdf},
}

@article{faul_gpower_2007,
	title = {G*{Power} 3: {A} flexible statistical power analysis program for the social, behavioral, and biomedical sciences},
	volume = {39},
	copyright = {http://www.springer.com/tdm},
	issn = {1554-351X, 1554-3528},
	shorttitle = {G*{Power} 3},
	url = {http://link.springer.com/10.3758/BF03193146},
	doi = {10.3758/BF03193146},
	language = {en},
	number = {2},
	urldate = {2024-04-16},
	journal = {Behavior Research Methods},
	author = {Faul, Franz and Erdfelder, Edgar and Lang, Albert-Georg and Buchner, Axel},
	month = may,
	year = {2007},
	pages = {175--191},
	file = {Faul et al. - 2007 - GPower 3 A flexible statistical power analysis p.pdf:C\:\\Users\\au645332\\Zotero\\storage\\LMTGMSZM\\Faul et al. - 2007 - GPower 3 A flexible statistical power analysis p.pdf:application/pdf},
}

@article{saccenti_corruption_2020,
	title = {Corruption of the {Pearson} correlation coefficient by measurement error and its estimation, bias, and correction under different error models},
	volume = {10},
	copyright = {2020 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-57247-4},
	doi = {10.1038/s41598-019-57247-4},
	abstract = {Correlation coefficients are abundantly used in the life sciences. Their use can be limited to simple exploratory analysis or to construct association networks for visualization but they are also basic ingredients for sophisticated multivariate data analysis methods. It is therefore important to have reliable estimates for correlation coefficients. In modern life sciences, comprehensive measurement techniques are used to measure metabolites, proteins, gene-expressions and other types of data. All these measurement techniques have errors. Whereas in the old days, with simple measurements, the errors were also simple, that is not the case anymore. Errors are heterogeneous, non-constant and not independent. This hampers the quality of the estimated correlation coefficients seriously. We will discuss the different types of errors as present in modern comprehensive life science data and show with theory, simulations and real-life data how these affect the correlation coefficients. We will briefly discuss ways to improve the estimation of such coefficients.},
	language = {en},
	number = {1},
	urldate = {2024-04-19},
	journal = {Scientific Reports},
	author = {Saccenti, Edoardo and Hendriks, Margriet H. W. B. and Smilde, Age K.},
	month = jan,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biomarkers, Statistics, Biochemical networks},
	pages = {438},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\XHVXF83X\\Saccenti et al. - 2020 - Corruption of the Pearson correlation coefficient .pdf:application/pdf},
}

@article{wichmann_psychometric_2001,
	title = {The psychometric function: {I}. {Fitting}, sampling, and goodness of fit},
	volume = {63},
	issn = {1532-5962},
	shorttitle = {The psychometric function},
	url = {https://doi.org/10.3758/BF03194544},
	doi = {10.3758/BF03194544},
	abstract = {The psychometric function relates an observer’s performance to an independent variable, usually some physical quantity of a stimulus in a psychophysical task. This paper, together with its companion paper (Wichmann \& Hill, 2001), describes an integrated approach to (1) fitting psychometric functions, (2) assessing the goodness of fit, and (3) providing confidence intervals for the function’s parameters and other estimates derived from them, for the purposes of hypothesis testing. The present paper deals with the first two topics, describing a constrained maximum-likelihood method of parameter estimation and developing several goodness-of-fit tests. Using Monte Carlo simulations, we deal with two specific difficulties that arise when fitting functions to psychophysical data. First, we note that human observers are prone to stimulus-independent errors (orlapses). We show that failure to account for this can lead to serious biases in estimates of the psychometric function’s parameters and illustrate how the problem may be overcome. Second, we note that psychophysical data sets are usually rather small by the standards required by most of the commonly applied statistical tests. We demonstrate the potential errors of applying traditionalX2 methods to psychophysical data and advocate use of Monte Carlo resampling techniques that do not rely on asymptotic theory. We have made available the software to implement our methods.},
	language = {en},
	number = {8},
	urldate = {2024-04-19},
	journal = {Perception \& Psychophysics},
	author = {Wichmann, Felix A. and Hill, N. Jeremy},
	month = nov,
	year = {2001},
	keywords = {Deviance Residual, Influential Observation, Psychometric Function, Psychophysical Data, Root Mean Square},
	pages = {1293--1313},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\J5PN4VT6\\Wichmann and Hill - 2001 - The psychometric function I. Fitting, sampling, a.pdf:application/pdf},
}

@article{mathys_bayesian_2011,
	title = {A {Bayesian} {Foundation} for {Individual} {Learning} {Under} {Uncertainty}},
	volume = {5},
	issn = {1662-5161},
	url = {https://www.frontiersin.org/articles/10.3389/fnhum.2011.00039},
	doi = {10.3389/fnhum.2011.00039},
	abstract = {Computational learning models are critical for understanding mechanisms of adaptive behavior. However, the two major current frameworks, reinforcement learning (RL) and Bayesian learning, both have certain limitations. For example, many Bayesian models are agnostic of inter-individual variability and involve complicated integrals, making online learning difficult. Here, we introduce a generic hierarchical Bayesian framework for individual learning under multiple forms of uncertainty (e.g., environmental volatility and perceptual uncertainty). The model assumes Gaussian random walks of states at all but the first level, with the step size determined by the next higher level. The coupling between levels is controlled by parameters that shape the influence of uncertainty on learning in a subject-specific fashion. Using variational Bayes under a mean field approximation and a novel approximation to the posterior energy function, we derive trial-by-trial update equations which (i) are analytical and extremely efficient, enabling real-time learning, (ii) have a natural interpretation in terms of RL, and (iii) contain parameters representing processes which play a key role in current theories of learning, e.g., precision-weighting of prediction error. These parameters allow for the expression of individual differences in learning and may relate to specific neuromodulatory mechanisms in the brain. Our model is very general: it can deal with both discrete and continuous states and equally accounts for deterministic and probabilistic relations between environmental events and perceptual states (i.e., situations with and without perceptual uncertainty). These properties are illustrated by simulations and analyses of empirical time series. Overall, our framework provides a novel foundation for understanding normal and pathological learning that contextualizes RL within a generic Bayesian scheme and thus connects it to principles of optimality from probability theory.},
	language = {English},
	urldate = {2024-04-19},
	journal = {Frontiers in Human Neuroscience},
	author = {Mathys, Christoph and Daunizeau, Jean and Friston, Karl J. and Stephan, Klaas Enno},
	month = may,
	year = {2011},
	note = {Publisher: Frontiers},
	keywords = {Neuromodulation, decision-making, variational Bayes, volatility, Dopamine, Acetylcholine, Hierarchical Models, Serotonin},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\AE7NKVBZ\\Mathys et al. - 2011 - A Bayesian Foundation for Individual Learning Unde.pdf:application/pdf},
}

@article{mathys_uncertainty_2014,
	title = {Uncertainty in perception and the {Hierarchical} {Gaussian} {Filter}},
	volume = {8},
	issn = {1662-5161},
	url = {https://www.frontiersin.org/articles/10.3389/fnhum.2014.00825},
	doi = {10.3389/fnhum.2014.00825},
	abstract = {In its full sense, perception rests on an agent’s model of how its sensory input comes about and the inferences it draws based on this model. These inferences are necessarily uncertain. Here, we illustrate how the hierarchical Gaussian filter (HGF) offers a principled and generic way to deal with the several forms that uncertainty in perception takes. The HGF is a recent derivation of one-step update equations from Bayesian principles that rests on a hierarchical generative model of the environment and its (in)stability. It is computationally highly efficient, allows for online estimates of hidden states, and has found numerous applications to experimental data from human subjects. In this paper, we generalize previous descriptions of the HGF and its account of perceptual uncertainty. First, we explicitly formulate the extension of the HGF’s hierarchy to any number of levels; second, we discuss how various forms of uncertainty are accommodated by the minimization of variational free energy as encoded in the update equations; third, we combine the HGF with decision models and demonstrate the inversion of this combination; finally, we report a simulation study that compared four optimization methods for inverting the HGF/decision model combination at different noise levels. These four methods (Nelder-Mead simplex algorithm, Gaussian process-based global optimization, variational Bayes and Markov chain Monte Carlo sampling) all performed well even under considerable noise, with variational Bayes offering the best combination of efficiency and informativeness of inference. Our results demonstrate that the HGF provides a principled, flexible, and efficient - but at the same time intuitive - framework for the resolution of perceptual uncertainty in behaving agents.},
	language = {English},
	urldate = {2024-04-19},
	journal = {Frontiers in Human Neuroscience},
	author = {Mathys, Christoph D. and Lomakina, Ekaterina I. and Daunizeau, Jean and Iglesias, Sandra and Brodersen, Kay H. and Friston, Karl J. and Stephan, Klaas E.},
	month = nov,
	year = {2014},
	note = {Publisher: Frontiers},
	keywords = {Learning, uncertainty, decision-making, volatility, Bayesian inference, filtering, free energy, hierarchical modeling},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\A8B7LLVE\\Mathys et al. - 2014 - Uncertainty in perception and the Hierarchical Gau.pdf:application/pdf},
}

@article{schurr_dynamic_2024,
	title = {Dynamic computational phenotyping of human cognition},
	copyright = {2024 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-024-01814-x},
	doi = {10.1038/s41562-024-01814-x},
	abstract = {Computational phenotyping has emerged as a powerful tool for characterizing individual variability across a variety of cognitive domains. An individual’s computational phenotype is defined as a set of mechanistically interpretable parameters obtained from fitting computational models to behavioural data. However, the interpretation of these parameters hinges critically on their psychometric properties, which are rarely studied. To identify the sources governing the temporal variability of the computational phenotype, we carried out a 12-week longitudinal study using a battery of seven tasks that measure aspects of human learning, memory, perception and decision making. To examine the influence of state effects, each week, participants provided reports tracking their mood, habits and daily activities. We developed a dynamic computational phenotyping framework, which allowed us to tease apart the time-varying effects of practice and internal states such as affective valence and arousal. Our results show that many phenotype dimensions covary with practice and affective factors, indicating that what appears to be unreliability may reflect previously unmeasured structure. These results support a fundamentally dynamic understanding of cognitive variability within an individual.},
	language = {en},
	urldate = {2024-04-19},
	journal = {Nature Human Behaviour},
	author = {Schurr, Roey and Reznik, Daniel and Hillman, Hanna and Bhui, Rahul and Gershman, Samuel J.},
	month = feb,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cognitive neuroscience, Human behaviour},
	pages = {1--15},
	file = {Full Text:C\:\\Users\\au645332\\Zotero\\storage\\6UIQPNF8\\Schurr et al. - 2024 - Dynamic computational phenotyping of human cogniti.pdf:application/pdf},
}

@article{gold_how_2013,
	title = {How mechanisms of perceptual decision-making affect the psychometric function},
	volume = {103},
	issn = {0301-0082},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3445702/},
	doi = {10.1016/j.pneurobio.2012.05.008},
	abstract = {Psychometric functions are often interpreted in the context of Signal Detection Theory, which emphasizes a distinction between sensory processing and non-sensory decision rules in the brain. This framework has helped to relate perceptual sensitivity to the “neurometric” sensitivity of sensory-driven neural activity. However, perceptual sensitivity, as interpreted via Signal Detection Theory, is based on not just how the brain represents relevant sensory information, but also how that information is read out to form the decision variable to which the decision rule is applied. Here we discuss recent advances in our understanding of this readout process and describe its effects on the psychometric function. In particular, we show that particular aspects of the readout process can have specific, identifiable effects on the threshold, slope, upper asymptote, time dependence, and choice dependence of psychometric functions. To illustrate these points, we emphasize studies of perceptual learning that have identified changes in the readout process that can lead to changes in these aspects of the psychometric function. We also discuss methods that have been used to distinguish contributions of the sensory representation versus its readout to psychophysical performance.},
	urldate = {2024-04-19},
	journal = {Progress in neurobiology},
	author = {Gold, Joshua I. and Ding, Long},
	month = apr,
	year = {2013},
	pmid = {22609483},
	pmcid = {PMC3445702},
	pages = {98--114},
	file = {PubMed Central Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\BK8RQL6M\\Gold and Ding - 2013 - How mechanisms of perceptual decision-making affec.pdf:application/pdf},
}

@article{bates_fitting_2015,
	title = {Fitting {Linear} {Mixed}-{Effects} {Models} {Using} \textbf{lme4}},
	volume = {67},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v67/i01/},
	doi = {10.18637/jss.v067.i01},
	abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-eﬀects models can be determined using the lmer function in the lme4 package for R. As for most model-ﬁtting functions in R, the model is described in an lmer call by a formula, in this case including both ﬁxed- and random-eﬀects terms. The formula and data together determine a numerical representation of the model from which the proﬁled deviance or the proﬁled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the proﬁled deviance or REML criterion, and the structure of classes or types that represents such a model. Suﬃcient detail is included to allow specialization of these structures by users who wish to write functions to ﬁt specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
	language = {en},
	number = {1},
	urldate = {2024-04-21},
	journal = {Journal of Statistical Software},
	author = {Bates, Douglas and Mächler, Martin and Bolker, Ben and Walker, Steve},
	year = {2015},
	file = {Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf:C\:\\Users\\au645332\\Zotero\\storage\\6AA9VLZ4\\Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf:application/pdf},
}

@article{myung_tutorial_2013,
	title = {A {Tutorial} on {Adaptive} {Design} {Optimization}},
	volume = {57},
	issn = {0022-2496},
	doi = {10.1016/j.jmp.2013.05.005},
	abstract = {Experimentation is ubiquitous in the field of psychology and fundamental to the advancement of its science, and one of the biggest challenges for researchers is designing experiments that can conclusively discriminate the theoretical hypotheses or models under investigation. The recognition of this challenge has led to the development of sophisticated statistical methods that aid in the design of experiments and that are within the reach of everyday experimental scientists. This tutorial paper introduces the reader to an implementable experimentation methodology, dubbed Adaptive Design Optimization, that can help scientists to conduct "smart" experiments that are maximally informative and highly efficient, which in turn should accelerate scientific discovery in psychology and beyond.},
	language = {eng},
	number = {3-4},
	journal = {Journal of Mathematical Psychology},
	author = {Myung, Jay I. and Cavagnaro, Daniel R. and Pitt, Mark A.},
	month = jun,
	year = {2013},
	pmid = {23997275},
	pmcid = {PMC3755632},
	pages = {53--67},
	file = {Accepted Version:C\:\\Users\\au645332\\Zotero\\storage\\FFA3E5GH\\Myung et al. - 2013 - A Tutorial on Adaptive Design Optimization.pdf:application/pdf},
}

@article{kwon_adaptive_2023,
	series = {Reliability of {Neurocognitive} {Measures} for {Mental} {Health}},
	title = {Adaptive {Design} {Optimization} as a {Promising} {Tool} for {Reliable} and {Efficient} {Computational} {Fingerprinting}},
	volume = {8},
	issn = {2451-9022},
	url = {https://www.sciencedirect.com/science/article/pii/S245190222200338X},
	doi = {10.1016/j.bpsc.2022.12.003},
	abstract = {A key challenge in understanding mental (dys)functions is their etiological and functional heterogeneity, and several multidimensional assessments have been proposed for their comprehensive characterization. However, such assessments require lengthy testing, which may hinder reliable and efficient characterization of individual differences due to increased fatigue and distraction, especially in clinical populations. Computational modeling may address this challenge as it often provides more reliable measures of latent neurocognitive processes underlying observed behaviors and captures individual differences better than traditional assessments. However, even with a state-of-the-art hierarchical modeling approach, reliable estimation of model parameters still requires a large number of trials. Recent work suggests that Bayesian adaptive design optimization (ADO) is a promising way to address these challenges. With ADO, experimental design is optimized adaptively from trial to trial to extract the maximum amount of information about an individual’s characteristics. In this review, we first describe the ADO methodology and then summarize recent work demonstrating that ADO increases the reliability and efficiency of latent neurocognitive measures. We conclude by discussing the challenges and future directions of ADO and proposing development of ADO-based computational fingerprints to reliably and efficiently characterize the heterogeneous profiles of psychiatric disorders.},
	number = {8},
	urldate = {2024-04-21},
	journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
	author = {Kwon, Mina and Lee, Sang Ho and Ahn, Woo-Young},
	month = aug,
	year = {2023},
	keywords = {Computational modeling, Computational psychiatry, Adaptive design optimization, Clinical assessment, Computational fingerprint, Reliability paradox},
	pages = {798--804},
	file = {ScienceDirect Snapshot:C\:\\Users\\au645332\\Zotero\\storage\\VEI2NNF3\\S245190222200338X.html:text/html;Submitted Version:C\:\\Users\\au645332\\Zotero\\storage\\3G3A2H74\\Kwon et al. - 2023 - Adaptive Design Optimization as a Promising Tool f.pdf:application/pdf},
}

@misc{zhang_pathfinder_2022,
	title = {Pathfinder: {Parallel} quasi-{Newton} variational inference},
	shorttitle = {Pathfinder},
	url = {http://arxiv.org/abs/2108.03782},
	abstract = {We propose Pathfinder, a variational method for approximately sampling from differentiable log densities. Starting from a random initialization, Pathfinder locates normal approximations to the target density along a quasi-Newton optimization path, with local covariance estimated using the inverse Hessian estimates produced by the optimizer. Pathfinder returns draws from the approximation with the lowest estimated Kullback-Leibler (KL) divergence to the true posterior. We evaluate Pathfinder on a wide range of posterior distributions, demonstrating that its approximate draws are better than those from automatic differentiation variational inference (ADVI) and comparable to those produced by short chains of dynamic Hamiltonian Monte Carlo (HMC), as measured by 1-Wasserstein distance. Compared to ADVI and short dynamic HMC runs, Pathfinder requires one to two orders of magnitude fewer log density and gradient evaluations, with greater reductions for more challenging posteriors. Importance resampling over multiple runs of Pathfinder improves the diversity of approximate draws, reducing 1-Wasserstein distance further and providing a measure of robustness to optimization failures on plateaus, saddle points, or in minor modes. The Monte Carlo KL divergence estimates are embarrassingly parallelizable in the core Pathfinder algorithm, as are multiple runs in the resampling version, further increasing Pathfinder's speed advantage with multiple cores.},
	urldate = {2024-04-21},
	publisher = {arXiv},
	author = {Zhang, Lu and Carpenter, Bob and Gelman, Andrew and Vehtari, Aki},
	month = may,
	year = {2022},
	note = {arXiv:2108.03782 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 46 pages, 21 figures},
	file = {arXiv.org Snapshot:C\:\\Users\\au645332\\Zotero\\storage\\V5PQKPKH\\2108.html:text/html;Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\8QPJZEFL\\Zhang et al. - 2022 - Pathfinder Parallel quasi-Newton variational infe.pdf:application/pdf},
}

@article{kontsevich_bayesian_1999,
	title = {Bayesian adaptive estimation of psychometric slope and threshold},
	volume = {39},
	issn = {0042-6989},
	doi = {10.1016/s0042-6989(98)00285-5},
	abstract = {We introduce a new Bayesian adaptive method for acquisition of both threshold and slope of the psychometric function. The method updates posterior probabilities in the two-dimensional parameter space of psychometric functions and makes predictions based on the expected mean threshold and slope values. On each trial it sets the stimulus intensity that maximizes the expected information to be gained by completion of that trial. The method was evaluated in computer simulations and in a psychophysical experiment using the two-alternative forced-choice (2AFC) paradigm. Threshold estimation within 2 dB (23\%) precision requires less than 30 trials for a typical 2AFC detection task. To get the slope estimate with the same precision takes about 300 trials.},
	language = {eng},
	number = {16},
	journal = {Vision Research},
	author = {Kontsevich, L. L. and Tyler, C. W.},
	month = aug,
	year = {1999},
	pmid = {10492833},
	keywords = {Humans, Psychophysics, Bayes Theorem, Visual Perception, Psychometrics, Sensory Thresholds, Contrast Sensitivity, Mathematics, Microcomputers},
	pages = {2729--2737},
}

@article{alshkaki_six_2021,
	title = {A {Six} {Parameters} {Beta} {Distribution} with {Application} for {Modeling} {Waiting} {Time} of {Muslim} {Early} {Morning} {Prayer}},
	volume = {8},
	issn = {2198-5812},
	url = {https://doi.org/10.1007/s40745-020-00282-0},
	doi = {10.1007/s40745-020-00282-0},
	abstract = {Beta distribution is a well-known and widely used distribution for modeling and analyzing lifetime data, due to its interesting characteristics. In this paper, a six parameters beta distribution is introduced as a generalization of the two (standard) and the four parameters beta distributions. This distribution is closed under scaling and exponentiation, and has reflection symmetry property, has some well-known distributions as special cases, such as, the two and four parameters beta, generalized modification of the Kumaraswamy, generalized beta of the first kind, the power function, Kumaraswamy power function, Minimax, exponentiated Pareto, and the generalized uniform distributions. Its moments about the origin, moment generating function, incomplete moments, mean deviations, are derived. The maximum likelihood estimation method is used for estimating its parameters and applied to estimate the parameters of the six different simulated data sets of this distribution, in order to check the performance of the estimation method through the estimated parameters mean squares errors computed from the different simulated sample sizes. Finally, two real life data sets, represent the waiting period of Muslim worshipers from the time of entering the mosque till the actual time of starting Alfajir pray in two different mosques, were used to illustrate the usefulness and the flexibility of this distribution, as well as, presents better fitting than the other gamma, exponential, the four parameters beta, and the generalized beta of the first kind distributions},
	language = {en},
	number = {1},
	urldate = {2024-04-21},
	journal = {Annals of Data Science},
	author = {Alshkaki, Rafid S. A.},
	month = mar,
	year = {2021},
	keywords = {60E05, 62E15, 65C05, Applications, Beta distribution, Maximum likelihood estimator, Moments, Simulation study},
	pages = {57--90},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\ILAI4KIK\\Alshkaki - 2021 - A Six Parameters Beta Distribution with Applicatio.pdf:application/pdf},
}

@article{goulet-pelletier_review_2018,
	title = {A review of effect sizes and their confidence intervals, {Part} {I}: {The} {Cohen}'s d family},
	volume = {14},
	issn = {2292-1354},
	shorttitle = {A review of effect sizes and their confidence intervals, {Part} {I}},
	url = {http://www.tqmp.org/RegularArticles/vol14-4/p242},
	doi = {10.20982/tqmp.14.4.p242},
	abstract = {Effect sizes and conﬁdence intervals are important statistics to assess the magnitude and the precision of an effect. The various standardized effect sizes can be grouped in three categories depending on the experimental design: measures of the difference between two means (the d family), measures of strength of association (e.g., r, R2, η2, ω2), and risk estimates (e.g., odds ratio, relative risk, phi; Kirk, 1996). Part I of this study reviews the d family, with a special focus on Cohen’s d and Hedges’ g for two-independent groups and two-repeated measures (or paired samples) designs. The present paper answers questions concerning the d family via Monte Carlo simulations. First, four different denominators are often proposed to standardize the mean difference in a repeated measures design. Which one should be used? Second, the literature proposes several approximations to estimate the standard error. Which one most closely estimates the true standard deviation of the distribution? Lastly, central and noncentral methods have been proposed to construct a conﬁdence interval around d. Which method leads to more precise coverage, and how to calculate it? Results suggest that the best way to standardize the effect in both designs is by using the pooled standard deviation in conjunction with a correction factor to unbias d. Likewise, the best standard error approximation is given by substituting the gamma function from the true formula by its approximation. Lastly, results from the conﬁdence interval simulations show that, under the normality assumption, the noncentral method is always superior, especially with small sample sizes. However, the central method is equivalent to the noncentral method when n is greater than 20 in each group for a between-group design and when n is greater than 24 pairs of observations for a repeated measures design. A practical guide to apply the ﬁndings of this study can be found after the general discussion.},
	language = {en},
	number = {4},
	urldate = {2024-04-21},
	journal = {The Quantitative Methods for Psychology},
	author = {Goulet-Pelletier, Jean-Christophe and Cousineau, Denis},
	month = dec,
	year = {2018},
	pages = {242--265},
	file = {Goulet-Pelletier and Cousineau - 2018 - A review of effect sizes and their confidence inte.pdf:C\:\\Users\\au645332\\Zotero\\storage\\P6NPDIUC\\Goulet-Pelletier and Cousineau - 2018 - A review of effect sizes and their confidence inte.pdf:application/pdf},
}

@article{lakens_calculating_2013,
	title = {Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and {ANOVAs}},
	volume = {4},
	issn = {1664-1078},
	shorttitle = {Calculating and reporting effect sizes to facilitate cumulative science},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2013.00863/full},
	doi = {10.3389/fpsyg.2013.00863},
	abstract = {{\textless}p{\textgreater}Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for {\textless}italic{\textgreater}t{\textless}/italic{\textgreater}-tests and ANOVA's such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between within- and between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-04-21},
	journal = {Frontiers in Psychology},
	author = {Lakens, Daniel},
	month = nov,
	year = {2013},
	note = {Publisher: Frontiers},
	keywords = {Cohen's d, effect sizes, eta-squared, power analysis, sample size planning},
	file = {Full Text:C\:\\Users\\au645332\\Zotero\\storage\\TTCK4PX4\\Lakens - 2013 - Calculating and reporting effect sizes to facilita.pdf:application/pdf},
}

@book{hedges_statistical_2014,
	title = {Statistical {Methods} for {Meta}-{Analysis}},
	isbn = {978-0-08-057065-5},
	abstract = {The main purpose of this book is to address the statistical issues for integrating independent studies. There exist a number of papers and books that discuss the mechanics of collecting, coding, and preparing data for a meta-analysis , and we do not deal with these. Because this book concerns methodology, the content necessarily is statistical, and at times mathematical. In order to make the material accessible to a wider audience, we have not provided proofs in the text. Where proofs are given, they are placed as commentary at the end of a chapter. These can be omitted at the discretion of the reader.Throughout the book we describe computational procedures whenever required. Many computations can be completed on a hand calculator, whereas some require the use of a standard statistical package such as SAS, SPSS, or BMD. Readers with experience using a statistical package or who conduct analyses such as multiple regression or analysis of variance should be able to carry out the analyses described with the aid of a statistical package.},
	language = {en},
	publisher = {Academic Press},
	author = {Hedges, Larry V. and Olkin, Ingram},
	month = jun,
	year = {2014},
	note = {Google-Books-ID: 7GviBQAAQBAJ},
	keywords = {Education / Testing \& Measurement, Mathematics / Probability \& Statistics / General, Social Science / Research},
}

@article{pedersen_drift_2017,
	title = {The drift diffusion model as the choice rule in reinforcement learning},
	volume = {24},
	issn = {1069-9384},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5487295/},
	doi = {10.3758/s13423-016-1199-y},
	abstract = {Current reinforcement-learning models often assume simplified decision processes that do not fully reflect the dynamic complexities of choice processes. Conversely, sequential-sampling models of decision making account for both choice accuracy and response time, but assume that decisions are based on static decision values. To combine these two computational models of decision making and learning, we implemented reinforcement-learning models in which the drift diffusion model describes the choice process, thereby capturing both within- and across-trial dynamics. To exemplify the utility of this approach, we quantitatively fit data from a common reinforcement-learning paradigm using hierarchical Bayesian parameter estimation, and compared model variants to determine whether they could capture the effects of stimulant medication in adult patients with attention-deficit hyper-activity disorder (ADHD). The model with the best relative fit provided a good description of the learning process, choices, and response times. A parameter recovery experiment showed that the hierarchical Bayesian modeling approach enabled accurate estimation of the model parameters. The model approach described here, using simultaneous estimation of reinforcement-learning and drift diffusion model parameters, shows promise for revealing new insights into the cognitive and neural mechanisms of learning and decision making, as well as the alteration of such processes in clinical groups.},
	number = {4},
	urldate = {2024-04-21},
	journal = {Psychonomic bulletin \& review},
	author = {Pedersen, Mads Lund and Frank, Michael J. and Biele, Guido},
	month = aug,
	year = {2017},
	pmid = {27966103},
	pmcid = {PMC5487295},
	pages = {1234--1251},
	file = {PubMed Central Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\RTXJHN9Y\\Pedersen et al. - 2017 - The drift diffusion model as the choice rule in re.pdf:application/pdf},
}

@misc{hess_bayesian_2024,
	title = {Bayesian {Workflow} for {Generative} {Modeling} in {Computational} {Psychiatry}},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2024.02.19.581001v1},
	doi = {10.1101/2024.02.19.581001},
	abstract = {Computational (generative) modelling of behaviour has considerable potential for clinical applications. In order to unlock the potential of generative models, reliable statistical inference is crucial. For this, Bayesian workflow has been suggested which, however, has rarely been applied in Translational Neuromodeling and Computational Psychiatry (TN/CP) so far. Here, we present a worked example of Bayesian workflow in the context of a typical application scenario for TN/CP.
This application example uses Hierarchical Gaussian Filter (HGF) models, a family of computational models for hierarchical Bayesian belief updating. When equipped with a suitable response model, HGF models can be fit to behavioural data from cognitive tasks; these data frequently consist of binary responses and are typically univariate. This poses challenges for statistical inference due to the limited information contained in such data. We present a novel set of response models that allow for simultaneous inference from multivariate (here: two) behavioural data types. Using both simulations and empirical data from a speed-incentivised associative reward learning (SPIRL) task, we show that harnessing information from two different data streams (binary responses and continuous response times) improves the accuracy of inference (specifically, identifiability of parameters and models). Moreover, we find a linear relationship between log-transformed response times in the SPIRL task and participants’ uncertainty about the outcome.
Our analysis illustrates the benefits of Bayesian workflow for a typical use case in TN/CP. We argue that adopting Bayesian workflow for generative modelling helps increase the transparency and robustness of results, which in turn is of fundamental importance for the long-term success of TN/CP.},
	language = {en},
	urldate = {2024-04-21},
	publisher = {bioRxiv},
	author = {Hess, Alexander J. and Iglesias, Sandra and Köchli, Laura and Marino, Stephanie and Müller-Schrader, Matthias and Rigoux, Lionel and Mathys, Christoph and Harrison, Olivia K. and Heinzle, Jakob and Frässle, Stefan and Stephan, Klaas Enno},
	month = feb,
	year = {2024},
	note = {Pages: 2024.02.19.581001
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\84JZKB84\\Hess et al. - 2024 - Bayesian Workflow for Generative Modeling in Compu.pdf:application/pdf},
}

@article{ioannidis_why_2005,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	url = {https://dx.plos.org/10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	language = {en},
	number = {8},
	urldate = {2024-04-21},
	journal = {PLoS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	pages = {e124},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\TX6G8RKE\\Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:application/pdf},
}

@article{wiggins_replication_2019,
	title = {The replication crisis in psychology: {An} overview for theoretical and philosophical psychology},
	volume = {39},
	issn = {2151-3341},
	shorttitle = {The replication crisis in psychology},
	doi = {10.1037/teo0000137},
	abstract = {[Correction Notice: An Erratum for this article was reported in Vol 41(2) of Journal of Theoretical and Philosophical Psychology (see record 2021-51056-003). In the article, the second author’s last name was misspelled in the byline, author note, and running head and should appear instead as Christopherson. The online version of this article has been corrected.] Psychology is in a replication crisis that has brought about a period of self-reflection and reform. Yet, this reform appears in many ways to focus primarily on methodological and statistical practices, with little consideration for the foundational issues that concern many theoretical and philosophical psychologists and that may provide a richer account of the crisis. In this article, we offer an overview of the history of the replication crisis, the critiques and reforms at the heart of the crisis, and several points of intersection between the reform movement and broader theoretical and philosophical issues. We argue that the problems of the replication crisis and the concerns of the reform movement in fact provide various points of entry for theoretical and philosophical psychologists to collaborate with reformers in providing a more deeply philosophical critique and reform. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
	number = {4},
	journal = {Journal of Theoretical and Philosophical Psychology},
	author = {Wiggins, Bradford J. and Christopherson, Cody D.},
	year = {2019},
	note = {Place: US
Publisher: Educational Publishing Foundation},
	keywords = {Psychology, History, Crises, Experimental Replication, Philosophies, Psychologists},
	pages = {202--217},
	file = {Snapshot:C\:\\Users\\au645332\\Zotero\\storage\\GLZJPKAY\\doiLanding.html:text/html},
}

@article{aarts_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	url = {https://ink.library.smu.edu.sg/lkcsb_research/5257},
	doi = {10.1126/science.aac4716},
	number = {6251},
	journal = {Science},
	author = {AARTS, Alexander A. and al, et and LIN, Stephanie C.},
	month = aug,
	year = {2015},
	pages = {943--950},
	file = {"Estimating the reproducibility of psychological science" by Alexander A. AARTS, et al et al.:C\:\\Users\\au645332\\Zotero\\storage\\FXZZXZY4\\5257.html:text/html;Full Text:C\:\\Users\\au645332\\Zotero\\storage\\WKTUE78I\\AARTS et al. - 2015 - Estimating the reproducibility of psychological sc.pdf:application/pdf},
}

@article{felix_singleton_statistical_2023,
	title = {The statistical power of psychology research: a systematic review and meta-analysis},
	url = {https://osf.io/yr8st/download},
	author = {Felix Singleton, Thorn and Fidler, Fiona},
	year = {2023},
}

@article{benjamin_redefine_2018,
	title = {Redefine statistical significance},
	volume = {2},
	copyright = {2017 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-017-0189-z},
	doi = {10.1038/s41562-017-0189-z},
	abstract = {We propose to change the default P-value threshold for statistical significance from 0.05 to 0.005 for claims of new discoveries.},
	language = {en},
	number = {1},
	urldate = {2024-04-21},
	journal = {Nature Human Behaviour},
	author = {Benjamin, Daniel J. and Berger, James O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, E.-J. and Berk, Richard and Bollen, Kenneth A. and Brembs, Björn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and De Boeck, Paul and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and Hua Ho, Teck and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P. A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and McCarthy, Michael and Moore, Don A. and Morgan, Stephen L. and Munafó, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Schönbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and Van Zandt, Trisha and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
	month = jan,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Statistics},
	pages = {6--10},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\UAJ2EY44\\Benjamin et al. - 2018 - Redefine statistical significance.pdf:application/pdf},
}

@article{maccoun_blind_2015,
	title = {Blind analysis: {Hide} results to seek the truth},
	volume = {526},
	copyright = {2015 Springer Nature Limited},
	issn = {1476-4687},
	shorttitle = {Blind analysis},
	url = {https://www.nature.com/articles/526187a},
	doi = {10.1038/526187a},
	abstract = {More fields should, like particle physics, adopt blind analysis to thwart bias, urge Robert MacCoun and Saul Perlmutter.},
	language = {en},
	number = {7572},
	urldate = {2024-04-21},
	journal = {Nature},
	author = {MacCoun, Robert and Perlmutter, Saul},
	month = oct,
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	keywords = {Research management},
	pages = {187--189},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\4IE3AD26\\MacCoun and Perlmutter - 2015 - Blind analysis Hide results to seek the truth.pdf:application/pdf},
}

@article{klein_blind_2005,
	title = {{BLIND} {ANALYSIS} {IN} {NUCLEAR} {AND} {PARTICLE} {PHYSICS}},
	volume = {55},
	issn = {0163-8998, 1545-4134},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.nucl.55.090704.151521},
	doi = {10.1146/annurev.nucl.55.090704.151521},
	abstract = {Over the past decade or so, blind analysis has become a widely used tool in nuclear and particle physics measurements. A blind analysis avoids the possibility of experimenters biasing their result toward their own preconceptions by preventing them from knowing the answer until the analysis is complete. There is at least circumstantial evidence that such a bias has aﬀected past measurements, and as experiments have become costlier and more diﬃcult and hence harder to reproduce, the possibility of bias has become a more important issue than in the past. We describe here the motivations for performing a blind analysis, and give several modern examples of successful blind analysis strategies.},
	language = {en},
	number = {1},
	urldate = {2024-04-21},
	journal = {Annual Review of Nuclear and Particle Science},
	author = {Klein, Joshua R and Roodman, Aaron},
	month = dec,
	year = {2005},
	pages = {141--163},
	file = {Klein and Roodman - 2005 - BLIND ANALYSIS IN NUCLEAR AND PARTICLE PHYSICS.pdf:C\:\\Users\\au645332\\Zotero\\storage\\UEM969A8\\Klein and Roodman - 2005 - BLIND ANALYSIS IN NUCLEAR AND PARTICLE PHYSICS.pdf:application/pdf},
}

@article{evans_improving_2023,
	title = {Improving evidence-based practice through preregistration of applied research: {Barriers} and recommendations},
	volume = {30},
	issn = {0898-9621},
	shorttitle = {Improving evidence-based practice through preregistration of applied research},
	url = {https://doi.org/10.1080/08989621.2021.1969233},
	doi = {10.1080/08989621.2021.1969233},
	abstract = {Preregistration is the practice of publicly publishing plans on central components of the research process before access to, or collection, of data. Within the context of the replication crisis, open science practices like preregistration have been pivotal in facilitating greater transparency in research. However, such practices have been applied nearly exclusively to basic academic research, with rare consideration of the relevance to applied and consultancy-based research. This is particularly problematic as such research is typically reported with very low levels of transparency and accountability despite being disseminated as influential gray literature to inform practice. Evidence-based practice is best served by an appreciation of multiple sources of quality evidence, thus the current review considers the potential of preregistration to improve both the accessibility and credibility of applied research toward more rigorous evidence-based practice. The current three-part review outlines, first, the opportunities of preregistration for applied research, and second, three barriers – practical challenges, stakeholder roles, and the suitability of preregistration. Last, this review makes four recommendations to overcome these barriers and maximize the opportunities of preregistration for academics, industry, and the structures they are held within – changes to preregistration templates, new types of templates, education and training, and recognition and structural changes.},
	number = {2},
	urldate = {2024-04-21},
	journal = {Accountability in Research},
	author = {Evans, Thomas Rhys and Branney, Peter and Clements, Andrew and Hatton, Ella},
	month = feb,
	year = {2023},
	pmid = {34396837},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/08989621.2021.1969233},
	keywords = {open science, accountability, Applied research, gray literature, preregistration, transparency},
	pages = {88--108},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\6FITMVCN\\Evans et al. - 2023 - Improving evidence-based practice through preregis.pdf:application/pdf},
}

@article{chambers_past_2022,
	title = {The past, present and future of {Registered} {Reports}},
	volume = {6},
	copyright = {2021 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-021-01193-7},
	doi = {10.1038/s41562-021-01193-7},
	abstract = {Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.},
	language = {en},
	number = {1},
	urldate = {2024-04-21},
	journal = {Nature Human Behaviour},
	author = {Chambers, Christopher D. and Tzavella, Loukia},
	month = jan,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Culture, Publishing},
	pages = {29--42},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\P4IKV3SV\\Chambers and Tzavella - 2022 - The past, present and future of Registered Reports.pdf:application/pdf},
}

@misc{ho_cognitive_2021,
	title = {Cognitive science as a source of forward and inverse models of human decisions for robotics and control},
	url = {http://arxiv.org/abs/2109.00127},
	doi = {10.48550/arXiv.2109.00127},
	abstract = {Those designing autonomous systems that interact with humans will invariably face questions about how humans think and make decisions. Fortunately, computational cognitive science offers insight into human decision-making using tools that will be familiar to those with backgrounds in optimization and control (e.g., probability theory, statistical machine learning, and reinforcement learning). Here, we review some of this work, focusing on how cognitive science can provide forward models of human decision-making and inverse models of how humans think about others' decision-making. We highlight relevant recent developments, including approaches that synthesize blackbox and theory-driven modeling, accounts that recast heuristics and biases as forms of bounded optimality, and models that characterize human theory of mind and communication in decision-theoretic terms. In doing so, we aim to provide readers with a glimpse of the range of frameworks, methodologies, and actionable insights that lie at the intersection of cognitive science and control research.},
	urldate = {2024-04-21},
	publisher = {arXiv},
	author = {Ho, Mark K. and Griffiths, Thomas L.},
	month = aug,
	year = {2021},
	note = {arXiv:2109.00127 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	annote = {Comment: Invited submission for Annual Review of Control, Robotics, and Autonomous Systems},
	file = {arXiv Fulltext PDF:C\:\\Users\\au645332\\Zotero\\storage\\JFZFNHU3\\Ho and Griffiths - 2021 - Cognitive science as a source of forward and inver.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\au645332\\Zotero\\storage\\KG7N7YKK\\2109.html:text/html},
}

@article{mcclelland_place_2009,
	title = {The {Place} of {Modeling} in {Cognitive} {Science}},
	volume = {1},
	issn = {1756-8757, 1756-8765},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1756-8765.2008.01003.x},
	doi = {10.1111/j.1756-8765.2008.01003.x},
	abstract = {Abstract
            I consider the role of cognitive modeling in cognitive science. Modeling, and the computers that enable it, are central to the field, but the role of modeling is often misunderstood. Models are not intended to capture fully the processes they attempt to elucidate. Rather, they are explorations of ideas about the nature of cognitive processes. In these explorations, simplification is essential—through simplification, the implications of the central ideas become more transparent. This is not to say that simplification has no downsides; it does, and these are discussed. I then consider several contemporary frameworks for cognitive modeling, stressing the idea that each framework is useful in its own particular ways. Increases in computer power (by a factor of about 4 million) since 1958 have enabled new modeling paradigms to emerge, but these also depend on new ways of thinking. Will new paradigms emerge again with the next 1,000‐fold increase?},
	language = {en},
	number = {1},
	urldate = {2024-04-21},
	journal = {Topics in Cognitive Science},
	author = {McClelland, James L.},
	month = jan,
	year = {2009},
	pages = {11--38},
	file = {McClelland - 2009 - The Place of Modeling in Cognitive Science.pdf:C\:\\Users\\au645332\\Zotero\\storage\\3FU73ZVS\\McClelland - 2009 - The Place of Modeling in Cognitive Science.pdf:application/pdf},
}

@article{zuidema_five_2020,
	title = {Five {Ways} in {Which} {Computational} {Modeling} {Can} {Help} {Advance} {Cognitive} {Science}: {Lessons} {From} {Artificial} {Grammar} {Learning}},
	volume = {12},
	issn = {1756-8757},
	shorttitle = {Five {Ways} in {Which} {Computational} {Modeling} {Can} {Help} {Advance} {Cognitive} {Science}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7496886/},
	doi = {10.1111/tops.12474},
	abstract = {There is a rich tradition of building computational models in cognitive science, but modeling, theoretical, and experimental research are not as tightly integrated as they could be. In this paper, we show that computational techniques—even simple ones that are straightforward to use—can greatly facilitate designing, implementing, and analyzing experiments, and generally help lift research to a new level. We focus on the domain of artificial grammar learning, and we give five concrete examples in this domain for (a) formalizing and clarifying theories, (b) generating stimuli, (c) visualization, (d) model selection, and (e) exploring the hypothesis space., Zuidema et al. illustrate how empirical AGL studies can benefit from computational models and techniques. Computational models can help clarifying theories, and thus in delineating research questions, but also in facilitating experimental design, stimulus generation, and data analysis. The authors show, with a series of examples, how computational modeling can be integrated with empirical AGL approaches, and how model selection techniques can indicate the most likely model to explain experimental outcomes.},
	number = {3},
	urldate = {2024-04-21},
	journal = {Topics in Cognitive Science},
	author = {Zuidema, Willem and French, Robert M. and Alhama, Raquel G. and Ellis, Kevin and O'Donnell, Timothy J. and Sainburg, Tim and Gentner, Timothy Q.},
	month = jul,
	year = {2020},
	pmid = {31663267},
	pmcid = {PMC7496886},
	pages = {925--941},
	file = {PubMed Central Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\HLIYS9K6\\Zuidema et al. - 2020 - Five Ways in Which Computational Modeling Can Help.pdf:application/pdf},
}

@article{stasinopoulos_generalized_2008,
	title = {Generalized {Additive} {Models} for {Location} {Scale} and {Shape} ({GAMLSS}) in {R}},
	volume = {23},
	copyright = {Copyright (c) 2007 D. Mikis Stasinopoulos, Robert A. Rigby},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v023.i07},
	doi = {10.18637/jss.v023.i07},
	abstract = {GAMLSS is a general framework for fitting regression type models where the distribution of the response variable does not have to belong to the exponential family and includes highly skew and kurtotic continuous and discrete distribution. GAMLSS allows all the parameters of the distribution of the response variable to be modelled as linear/non-linear or smooth functions of the explanatory variables. This  paper starts by defining the statistical framework of GAMLSS, then  describes the current implementation of GAMLSS in R and finally gives four different data examples to demonstrate how GAMLSS can be used for statistical modelling.},
	language = {en},
	urldate = {2024-04-21},
	journal = {Journal of Statistical Software},
	author = {Stasinopoulos, D. Mikis and Rigby, Robert A.},
	year = {2008},
	pages = {1--46},
}

@article{kuznetsova_lmertest_2017,
	title = {\textbf{{lmerTest}} {Package}: {Tests} in {Linear} {Mixed} {Effects} {Models}},
	volume = {82},
	issn = {1548-7660},
	shorttitle = {\textbf{{lmerTest}} {Package}},
	url = {http://www.jstatsoft.org/v82/i13/},
	doi = {10.18637/jss.v082.i13},
	abstract = {One of the frequent questions by users of the mixed model function lmer of the lme4 package has been: How can I get p values for the F and t tests for objects returned by lmer? The lmerTest package extends the ‘lmerMod’ class of the lme4 package, by overloading the anova and summary functions by providing p values for tests for ﬁxed eﬀects. We have implemented the Satterthwaite’s method for approximating degrees of freedom for the t and F tests. We have also implemented the construction of Type I–III ANOVA tables. Furthermore, one may also obtain the summary as well as the anova table using the Kenward-Roger approximation for denominator degrees of freedom (based on the KRmodcomp function from the pbkrtest package). Some other convenient mixed model analysis tools such as a step method, that performs backward elimination of nonsigniﬁcant eﬀects – both random and ﬁxed, calculation of population means and multiple comparison tests together with plot facilities are provided by the package as well.},
	language = {en},
	number = {13},
	urldate = {2024-04-21},
	journal = {Journal of Statistical Software},
	author = {Kuznetsova, Alexandra and Brockhoff, Per B. and Christensen, Rune H. B.},
	year = {2017},
	file = {Kuznetsova et al. - 2017 - lmerTest Package Tests in Linear Mixed Eff.pdf:C\:\\Users\\au645332\\Zotero\\storage\\EGY8ZAWV\\Kuznetsova et al. - 2017 - lmerTest Package Tests in Linear Mixed Eff.pdf:application/pdf},
}

@article{apon_high_2010,
	title = {High {Performance} {Computing} {Instrumentation} and {Research} {Productivity} in {US} {Universities}},
	volume = {10},
	abstract = {This paper studies the relationship between investments in High-Performance Computing (HPC) instrumentation and research competitiveness. Measures of institutional HPC investment are computed from data that is readily available from the Top 500 list, a list that has been published twice a year since 1993 that lists the fastest 500 computers in the world at that time. Institutions that are studied include US doctoral-granting institutions that fall into the very high or high research rankings according to the Carnegie Foundation classifications and additional institutions that have had entries in the Top 500 list. Research competitiveness is derived from federal funding data, compilations of scholarly publications, and institutional rankings. Correlation and Two Stage Least Square regression is used to analyze the research-related returns to investment in HPC. Two models are examined and give results that are both economically and statistically significant. Appearance on the Top 500 list is associated with a contemporaneous increase in NSF funding levels as well as a contemporaneous increase in the number of publications. The rate of depreciation in returns to HPC is rapid. The conclusion is that consistent investments in HPC at even modest levels are strongly correlated to research competitiveness.},
	journal = {JITI Journal of Information Technology Impact},
	author = {Apon, Amy and Ahalt, Stanley and Dantuluri, Vijay and Gurdgiev, Constantin and Limayem, Moez and Ngo, Linh and Stealey, Michael},
	month = jan,
	year = {2010},
	pages = {87--98},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\WWCHCRAV\\Apon et al. - 2010 - High Performance Computing Instrumentation and Res.pdf:application/pdf},
}

@incollection{johannes_fundamentals_2009,
	address = {Berlin, Heidelberg},
	title = {Fundamentals of {Gravity}, {Elements} of {Potential} {Theory}},
	isbn = {978-3-540-85329-9},
	url = {https://doi.org/10.1007/978-3-540-85329-9_2},
	abstract = {Gravity is the vector g of gravity acceleration. Usually its norm g is called “gravity”. It is composed of the vectorially added components of the Earth’s gravitation and the rotational or centrifugal acceleration. Its value is roughly 9.81±0.03 m/s2 at the Earth’s surface. A small part is time-varying. Gravity decreases with distance from the surface, both upward (Newton’s law) and from some depth also downward (as only the masses below the observer exert gravitational attraction).},
	language = {en},
	urldate = {2024-04-21},
	booktitle = {Gravity {Interpretation}: {Fundamentals} and {Application} of {Gravity} {Inversion} and {Geological} {Interpretation}},
	publisher = {Springer},
	author = {Johannes, Wolfgang Jacoby and Smilde, Peter L.},
	editor = {Johannes, Wolfgang Jacoby and Smilde, Peter L.},
	year = {2009},
	doi = {10.1007/978-3-540-85329-9_2},
	keywords = {Gravity Anomaly, Gravity Effect, Harmonic Function, Mass Element, Potential Theory},
	pages = {23--111},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\KH72ECYZ\\Johannes and Smilde - 2009 - Fundamentals of Gravity, Elements of Potential The.pdf:application/pdf},
}

@article{efron_estimating_1983,
	title = {Estimating the {Error} {Rate} of a {Prediction} {Rule}: {Improvement} on {Cross}-{Validation}},
	volume = {78},
	issn = {0162-1459},
	shorttitle = {Estimating the {Error} {Rate} of a {Prediction} {Rule}},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10477973},
	doi = {10.1080/01621459.1983.10477973},
	abstract = {We construct a prediction rule on the basis of some data, and then wish to estimate the error rate of this rule in classifying future observations. Cross-validation provides a nearly unbiased estimate, using only the original data. Cross-validation turns out to be related closely to the bootstrap estimate of the error rate. This article has two purposes: to understand better the theoretical basis of the prediction problem, and to investigate some related estimators, which seem to offer considerably improved estimation in small samples.},
	number = {382},
	urldate = {2024-04-22},
	journal = {Journal of the American Statistical Association},
	author = {Efron, Bradley},
	month = jun,
	year = {1983},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1983.10477973},
	keywords = {ANOVA decomposition, Bootstrap, Logistic regression, Prediction problem},
	pages = {316--331},
}

@book{efron_introduction_1994,
	address = {New York},
	title = {An {Introduction} to the {Bootstrap}},
	isbn = {978-0-429-24659-3},
	abstract = {An Introduction to the Bootstrap arms scientists and engineers as well as statisticians with the computational techniques they need to analyze and understand complicated data sets. The bootstrap is a computer-based method of statistical inference that answers statistical questions without formulas and gives a direct appreciation of variance, bias, coverage, and other probabilistic phenomena. This book presents an overview of the bootstrap and related methods for assessing statistical accuracy, concentrating on the ideas rather than their mathematical justification. Not just for beginners, the presentation starts off slowly, but builds in both scope and depth to ideas that are quite sophisticated.},
	publisher = {Chapman and Hall/CRC},
	author = {Efron, Bradley and Tibshirani, R. J.},
	month = may,
	year = {1994},
	doi = {10.1201/9780429246593},
}

@article{wu_jackknife_1986,
	title = {Jackknife, {Bootstrap} and {Other} {Resampling} {Methods} in {Regression} {Analysis}},
	volume = {14},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-14/issue-4/Jackknife-Bootstrap-and-Other-Resampling-Methods-in-Regression-Analysis/10.1214/aos/1176350142.full},
	doi = {10.1214/aos/1176350142},
	abstract = {Motivated by a representation for the least squares estimator, we propose a class of weighted jackknife variance estimators for the least squares estimator by deleting any fixed number of observations at a time. They are unbiased for homoscedastic errors and a special case, the delete-one jackknife, is almost unbiased for heteroscedastic errors. The method is extended to cover nonlinear parameters, regression \$M\$-estimators, nonlinear regression and generalized linear models. Interval estimators can be constructed from the jackknife histogram. Three bootstrap methods are considered. Two are shown to give biased variance estimators and one does not have the bias-robustness property enjoyed by the weighted delete-one jackknife. A general method for resampling residuals is proposed. It gives variance estimators that are bias-robust. Several bias-reducing estimators are proposed. Some simulation results are reported.},
	number = {4},
	urldate = {2024-04-22},
	journal = {The Annals of Statistics},
	author = {Wu, C. F. J.},
	month = dec,
	year = {1986},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {\$M\$-regression, 62G05, 62J02, 62J05, balanced residuals, bias reduction, bias-robustness, bootstrap, Fieller's linterval, generalized linear models, jackknife percentile, Linear regression, Nonlinear regression, representation of the least squares estimator, variable jackknife, Weighted jackknife},
	pages = {1261--1295},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\BHIV2MIL\\Wu - 1986 - Jackknife, Bootstrap and Other Resampling Methods .pdf:application/pdf},
}

@incollection{forbes_chapter_2023,
	title = {Chapter 11 - {Supporting} the replication of your research},
	isbn = {978-0-323-90969-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780323909693000037},
	abstract = {Researchers in many scientific fields have discovered that original research findings may be inaccurate and, when not verified via multiple and independent replication studies, inaccurate studies may be perceived reliable. This chapter provides an overview of replication of single-case experimental research along with ethical rationale to encourage replication research. We discuss the replication crisis affecting various scientific fields and the sources of diminished confidence in research findings from group comparisons research. We also briefly compare and contrast group comparisons and single-case experimental research methods in the current academic climate. To encourage research quality and transparency, we also provide an overview of open science practices that single-case experimental researchers can use to adhere to ethical principles such as truth, justice (i.e., fairness), and the pursuit of excellence in the human services.},
	urldate = {2024-04-22},
	booktitle = {Research {Ethics} in {Behavior} {Analysis}},
	publisher = {Academic Press},
	author = {Forbes, Heather J. and Travers, Jason C. and Johnson, Jenee Vickers},
	editor = {Cox, David J. and Syed, Noor Y. and Brodhead, Matthew T. and Quigley, Shawn P.},
	month = jan,
	year = {2023},
	doi = {10.1016/B978-0-323-90969-3.00003-7},
	keywords = {Open science, Replication research, Research ethics, Research transparency, Single case experimental research},
	pages = {237--262},
	file = {ScienceDirect Snapshot:C\:\\Users\\au645332\\Zotero\\storage\\X8PXDUY8\\B9780323909693000037.html:text/html},
}

@article{head_extent_2015,
	title = {The {Extent} and {Consequences} of {P}-{Hacking} in {Science}},
	volume = {13},
	issn = {1544-9173},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4359000/},
	doi = {10.1371/journal.pbio.1002106},
	abstract = {A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as “p-hacking,” occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses., Publication bias resulting from so-called "p-hacking" is pervasive throughout the life sciences; however, its effects on general conclusions made from the literature appear to be weak.},
	number = {3},
	urldate = {2024-04-22},
	journal = {PLoS Biology},
	author = {Head, Megan L. and Holman, Luke and Lanfear, Rob and Kahn, Andrew T. and Jennions, Michael D.},
	month = mar,
	year = {2015},
	pmid = {25768323},
	pmcid = {PMC4359000},
	pages = {e1002106},
	file = {PubMed Central Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\8BDGHCHT\\Head et al. - 2015 - The Extent and Consequences of P-Hacking in Scienc.pdf:application/pdf},
}

@article{dedrick_multilevel_2009,
	title = {Multilevel {Modeling}: {A} {Review} of {Methodological} {Issues} and                {Applications}},
	volume = {79},
	issn = {0034-6543},
	shorttitle = {Multilevel {Modeling}},
	url = {https://doi.org/10.3102/0034654308325581},
	doi = {10.3102/0034654308325581},
	abstract = {This study analyzed the reporting of multilevel modeling applications of a sample of 99 articles from 13 peer-reviewed journals in education and the social sciences. A checklist, derived from the methodological literature on multilevel modeling and focusing on the issues of model development and specification, data considerations, estimation, and inference, was used to analyze the articles. The most common applications were two-level models where individuals were nested within contexts. Most studies were non-experimental and used nonprobability samples. The amount of data at each level varied widely across studies, as did the number of models examined. Analyses of reporting practices indicated some clear problems, with many articles not reporting enough information for a reader to critique the reported analyses. For example, in many articles, one could not determine how many models were estimated, what covariance structure was assumed, what type of centering if any was used, whether the data were consistent with assumptions, whether outliers were present, or how the models were estimated. Guidelines for researchers reporting multilevel analyses are provided.},
	language = {en},
	number = {1},
	urldate = {2024-04-23},
	journal = {Review of Educational Research},
	author = {Dedrick, Robert F. and Ferron, John M. and Hess, Melinda R. and Hogarty, Kristine Y. and Kromrey, Jeffrey D. and Lang, Thomas R. and Niles, John D. and Lee, Reginald S.},
	month = mar,
	year = {2009},
	note = {Publisher: American Educational Research Association},
	pages = {69--102},
	file = {SAGE PDF Full Text:C\:\\Users\\au645332\\Zotero\\storage\\4HWK73IN\\Dedrick et al. - 2009 - Multilevel Modeling A Review of Methodological Is.pdf:application/pdf},
}

@article{vehtari_rank-normalization_2021,
	title = {Rank-normalization, folding, and localization: {An} improved \${\textbackslash}widehat\{{R}\}\$ for assessing convergence of {MCMC}},
	volume = {16},
	issn = {1936-0975},
	shorttitle = {Rank-normalization, folding, and localization},
	url = {http://arxiv.org/abs/1903.08008},
	doi = {10.1214/20-BA1221},
	abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic \${\textbackslash}widehat\{R\}\$ of Gelman and Rubin (1992) has serious flaws. Traditional \${\textbackslash}widehat\{R\}\$ will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice.},
	number = {2},
	urldate = {2024-04-23},
	journal = {Bayesian Analysis},
	author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and Bürkner, Paul-Christian},
	month = jun,
	year = {2021},
	note = {arXiv:1903.08008 [stat]},
	keywords = {Statistics - Computation, Statistics - Methodology},
	annote = {Comment: Two small fixes. Published in Bayesian analysis https://doi.org/10.1214/20-BA1221},
	file = {arXiv.org Snapshot:C\:\\Users\\au645332\\Zotero\\storage\\SIJPYUY4\\1903.html:text/html;Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\NP7UBXSB\\Vehtari et al. - 2021 - Rank-normalization, folding, and localization An .pdf:application/pdf},
}

@article{ariew_duhem_1984,
	title = {The {Duhem} {Thesis}},
	volume = {35},
	issn = {0007-0882},
	url = {https://www.jstor.org/stable/687336},
	number = {4},
	urldate = {2024-04-23},
	journal = {The British Journal for the Philosophy of Science},
	author = {Ariew, Roger},
	year = {1984},
	note = {Publisher: [Oxford University Press, The British Society for the Philosophy of Science]},
	pages = {313--325},
	file = {JSTOR Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\ZDZECYP3\\Ariew - 1984 - The Duhem Thesis.pdf:application/pdf},
}

@article{laitin_reporting_2021,
	title = {Reporting all results efficiently: {A} {RARE} proposal to open up the file drawer},
	volume = {118},
	issn = {1091-6490},
	shorttitle = {Reporting all results efficiently},
	doi = {10.1073/pnas.2106178118},
	abstract = {While the social sciences have made impressive progress in adopting transparent research practices that facilitate verification, replication, and reuse of materials, the problem of publication bias persists. Bias on the part of peer reviewers and journal editors, as well as the use of outdated research practices by authors, continues to skew literature toward statistically significant effects, many of which may be false positives. To mitigate this bias, we propose a framework to enable authors to report all results efficiently (RARE), with an initial focus on experimental and other prospective empirical social science research that utilizes public study registries. This framework depicts an integrated system that leverages the capacities of existing infrastructure in the form of public registries, institutional review boards, journals, and granting agencies, as well as investigators themselves, to efficiently incentivize full reporting and thereby, improve confidence in social science findings. In addition to increasing access to the results of scientific endeavors, a well-coordinated research ecosystem can prevent scholars from wasting time investigating the same questions in ways that have not worked in the past and reduce wasted funds on the part of granting agencies.},
	language = {eng},
	number = {52},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Laitin, David D. and Miguel, Edward and Alrababa'h, Ala' and Bogdanoski, Aleksandar and Grant, Sean and Hoeberling, Katherine and Hyunjung Mo, Cecilia and Moore, Don A. and Vazire, Simine and Weinstein, Jeremy and Williamson, Scott},
	month = dec,
	year = {2021},
	pmid = {34933997},
	pmcid = {PMC8719896},
	keywords = {file drawer problem, null findings, publication bias, registries, research transparency},
	pages = {e2106178118},
	file = {Full Text:C\:\\Users\\au645332\\Zotero\\storage\\UXMJFR64\\Laitin et al. - 2021 - Reporting all results efficiently A RARE proposal.pdf:application/pdf},
}

@article{miller_how_2024,
	title = {How {Many} {Participants}? {How} {Many} {Trials}? {Maximizing} the {Power} of {Reaction} {Time} {Studies}},
	volume = {56},
	issn = {1554-3528},
	shorttitle = {How {Many} {Participants}?},
	url = {https://doi.org/10.3758/s13428-023-02155-9},
	doi = {10.3758/s13428-023-02155-9},
	abstract = {Due to limitations in the resources available for carrying out reaction time (RT) experiments, researchers often have to choose between testing relatively few participants with relatively many trials each or testing relatively many participants with relatively few trials each. To compare the experimental power that would be obtained under each of these options, I simulated virtual experiments using subsets of participants and trials from eight large real RT datasets examining 19 experimental effects. The simulations compared designs using the first \$\$N\_T\$\$trials from \$\$N\_P\$\$randomly selected participants, holding constant the total number of trials across all participants, \$\$N\_P {\textbackslash}! {\textbackslash}times {\textbackslash}! N\_T\$\$. The \$\$[N\_P,N\_T]\$\$combination maximizing the power to detect each effect depended on how the mean and variability of that effect changed with practice. For most effects, power was greater in designs having many participants with few trials each rather than the reverse, suggesting that researchers should usually try to recruit large numbers of participants for short experimental sessions. In some cases, power for a fixed total number of trials across all participants was maximized by having as few as two trials per participant in each condition. Where researchers can make plausible predictions about how their effects will change over the course of a session, they can use those predictions to increase their experimental power.},
	language = {en},
	number = {3},
	urldate = {2024-04-23},
	journal = {Behavior Research Methods},
	author = {Miller, Jeff},
	month = mar,
	year = {2024},
	keywords = {Number of trials, Practice effects, Reaction times, Sample size, Statistical power, Within-subjects designs},
	pages = {2398--2421},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\45Z54GY6\\Miller - 2024 - How Many Participants How Many Trials Maximizing.pdf:application/pdf},
}

@article{kerr_harking_1998,
	title = {{HARKing}: {Hypothesizing} {After} the {Results} are {Known}},
	volume = {2},
	issn = {1088-8683},
	shorttitle = {{HARKing}},
	url = {https://doi.org/10.1207/s15327957pspr0203_4},
	doi = {10.1207/s15327957pspr0203_4},
	abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing's costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
	language = {en},
	number = {3},
	urldate = {2024-04-23},
	journal = {Personality and Social Psychology Review},
	author = {Kerr, Norbert L.},
	month = aug,
	year = {1998},
	note = {Publisher: SAGE Publications Inc},
	pages = {196--217},
	file = {Submitted Version:C\:\\Users\\au645332\\Zotero\\storage\\J4CR4UCU\\Kerr - 1998 - HARKing Hypothesizing After the Results are Known.pdf:application/pdf},
}

@article{durstewitz_computational_2016,
	series = {Computational modeling},
	title = {Computational models as statistical tools},
	volume = {11},
	issn = {2352-1546},
	url = {https://www.sciencedirect.com/science/article/pii/S2352154616301371},
	doi = {10.1016/j.cobeha.2016.07.004},
	abstract = {Traditionally, models in statistics are relatively simple ‘general purpose’ quantitative inference tools, while models in computational neuroscience aim more at mechanistically explaining specific observations. Research on methods for inferring behavioral and neural models from data, however, has shown that a lot could be gained by merging these approaches, augmenting computational models with distributional assumptions. This enables estimation of parameters of such models in a principled way, comes with confidence regions that quantify uncertainty in estimates, and allows for quantitative assessment of prediction quality of computational models and tests of specific hypotheses about underlying mechanisms. Thus, unlike in conventional statistics, inferences about the latent dynamical mechanisms that generated the observed data can be drawn. Future directions and challenges of this approach are discussed.},
	urldate = {2024-04-23},
	journal = {Current Opinion in Behavioral Sciences},
	author = {Durstewitz, Daniel and Koppe, Georgia and Toutounji, Hazem},
	month = oct,
	year = {2016},
	pages = {93--99},
	file = {ScienceDirect Snapshot:C\:\\Users\\au645332\\Zotero\\storage\\DWGSWTBQ\\S2352154616301371.html:text/html},
}

@article{ma_memorability_2024,
	title = {Memorability shapes perceived time (and vice versa)},
	copyright = {2024 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-024-01863-2},
	doi = {10.1038/s41562-024-01863-2},
	abstract = {Visual stimuli are known to vary in their perceived duration. Some visual stimuli are also known to linger for longer in memory. Yet, whether these two features of visual processing are linked is unknown. Despite early assumptions that time is an extracted or higher-order feature of perception, more recent work over the past two decades has demonstrated that timing may be instantiated within sensory modality circuits. A primary location for many of these studies is the visual system, where duration-sensitive responses have been demonstrated. Furthermore, visual stimulus features have been observed to shift perceived duration. These findings suggest that visual circuits mediate or construct perceived time. Here we present evidence across a series of experiments that perceived time is affected by the image properties of scene size, clutter and memorability. More specifically, we observe that scene size and memorability dilate time, whereas clutter contracts it. Furthermore, the durations of more memorable images are also perceived more precisely. Conversely, the longer the perceived duration of an image, the more memorable it is. To explain these findings, we applied a recurrent convolutional neural network model of the ventral visual system, in which images are progressively processed over time. We find that more memorable images are processed faster, and that this increase in processing speed predicts both the lengthening and the increased precision of perceived durations. These findings provide evidence for a link between image features, time perception and memory that can be further explored with models of visual processing.},
	language = {en},
	urldate = {2024-04-23},
	journal = {Nature Human Behaviour},
	author = {Ma, Alex C. and Cameron, Ayana D. and Wiener, Martin},
	month = apr,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Pattern vision, Sensory processing},
	pages = {1--13},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\7K9SXARH\\Ma et al. - 2024 - Memorability shapes perceived time (and vice versa.pdf:application/pdf},
}

@article{coates_changes_2014,
	title = {Changes across the psychometric function following perceptual learning of an {RSVP} reading task},
	volume = {5},
	issn = {1664-1078},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4274879/},
	doi = {10.3389/fpsyg.2014.01434},
	abstract = {Several recent studies have shown that perceptual learning can result in improvements in reading speed for people with macular disease (e.g., Chung, ; Tarita-Nistor et al., ). The improvements were reported as an increase in reading speed defined by specific criteria; however, little is known about how other properties of the reading performance or the participants' perceptual responses change as a consequence of learning. In this paper, we performed detailed analyses of data following perceptual learning using an RSVP (rapid serial visual presentation) reading task, looking beyond the change in reading speed defined by the threshold at a given accuracy on a psychometric function relating response accuracy with word exposure duration. Specifically, we explored the statistical characteristics of the response data to address two specific questions: was there a change in the slope of the psychometric function and did the improvements in performance occur consistently across different word exposure durations? Our results show that there is a general steepening of the slope of the psychometric function, leading to non-uniform improvements across stimulus levels.},
	urldate = {2024-04-23},
	journal = {Frontiers in Psychology},
	author = {Coates, Daniel R. and Chung, Susana T. L.},
	month = dec,
	year = {2014},
	pmid = {25566119},
	pmcid = {PMC4274879},
	pages = {1434},
	file = {PubMed Central Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\D4FIVPN3\\Coates and Chung - 2014 - Changes across the psychometric function following.pdf:application/pdf},
}

@article{bahrami_what_2012,
	title = {What failure in collective decision-making tells us about metacognition},
	volume = {367},
	issn = {1471-2970},
	doi = {10.1098/rstb.2011.0420},
	abstract = {Condorcet (1785) proposed that a majority vote drawn from individual, independent and fallible (but not totally uninformed) opinions provides near-perfect accuracy if the number of voters is adequately large. Research in social psychology has since then repeatedly demonstrated that collectives can and do fail more often than expected by Condorcet. Since human collective decisions often follow from exchange of opinions, these failures provide an exquisite opportunity to understand human communication of metacognitive confidence. This question can be addressed by recasting collective decision-making as an information-integration problem similar to multisensory (cross-modal) perception. Previous research in systems neuroscience shows that one brain can integrate information from multiple senses nearly optimally. Inverting the question, we ask: under what conditions can two brains integrate information about one sensory modality optimally? We review recent work that has taken this approach and report discoveries about the quantitative limits of collective perceptual decision-making, and the role of the mode of communication and feedback in collective decision-making. We propose that shared metacognitive confidence conveys the strength of an individual's opinion and its reliability inseparably. We further suggest that a functional role of shared metacognition is to provide substitute signals in situations where outcome is necessary for learning but unavailable or impossible to establish.},
	language = {eng},
	number = {1594},
	journal = {Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences},
	author = {Bahrami, Bahador and Olsen, Karsten and Bang, Dan and Roepstorff, Andreas and Rees, Geraint and Frith, Chris},
	month = may,
	year = {2012},
	pmid = {22492752},
	pmcid = {PMC3318766},
	keywords = {Choice Behavior, Cognition, Decision Making, Humans, Models, Psychological},
	pages = {1350--1365},
	file = {Full Text:C\:\\Users\\au645332\\Zotero\\storage\\THKK9T37\\Bahrami et al. - 2012 - What failure in collective decision-making tells u.pdf:application/pdf},
}

@article{kruschke_bayesian_2021,
	title = {Bayesian {Analysis} {Reporting} {Guidelines}},
	volume = {5},
	copyright = {2021 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-021-01177-7},
	doi = {10.1038/s41562-021-01177-7},
	abstract = {Previous surveys of the literature have shown that reports of statistical analyses often lack important information, causing lack of transparency and failure of reproducibility. Editors and authors agree that guidelines for reporting should be encouraged. This Review presents a set of Bayesian analysis reporting guidelines (BARG). The BARG encompass the features of previous guidelines, while including many additional details for contemporary Bayesian analyses, with explanations. An extensive example of applying the BARG is presented. The BARG should be useful to researchers, authors, reviewers, editors, educators and students. Utilization, endorsement and promotion of the BARG may improve the quality, transparency and reproducibility of Bayesian analyses.},
	language = {en},
	number = {10},
	urldate = {2024-04-23},
	journal = {Nature Human Behaviour},
	author = {Kruschke, John K.},
	month = oct,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Medical research, Psychology},
	pages = {1282--1291},
	file = {Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\KZ4CTG72\\Kruschke - 2021 - Bayesian Analysis Reporting Guidelines.pdf:application/pdf},
}

@article{gomes_should_2022,
	title = {Should {I} use fixed effects or random effects when {I} have fewer than five levels of a grouping factor in a mixed-effects model?},
	volume = {10},
	issn = {2167-8359},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8784019/},
	doi = {10.7717/peerj.12794},
	abstract = {As linear mixed-effects models (LMMs) have become a widespread tool in ecology, the need to guide the use of such tools is increasingly important. One common guideline is that one needs at least five levels of the grouping variable associated with a random effect. Having so few levels makes the estimation of the variance of random effects terms (such as ecological sites, individuals, or populations) difficult, but it need not muddy one’s ability to estimate fixed effects terms—which are often of primary interest in ecology. Here, I simulate datasets and fit simple models to show that having few random effects levels does not strongly influence the parameter estimates or uncertainty around those estimates for fixed effects terms—at least in the case presented here. Instead, the coverage probability of fixed effects estimates is sample size dependent. LMMs including low-level random effects terms may come at the expense of increased singular fits, but this did not appear to influence coverage probability or RMSE, except in low sample size (N = 30) scenarios. Thus, it may be acceptable to use fewer than five levels of random effects if one is not interested in making inferences about the random effects terms (i.e. when they are ‘nuisance’ parameters used to group non-independent data), but further work is needed to explore alternative scenarios. Given the widespread accessibility of LMMs in ecology and evolution, future simulation studies and further assessments of these statistical methods are necessary to understand the consequences both of violating and of routinely following simple guidelines.},
	urldate = {2024-04-23},
	journal = {PeerJ},
	author = {Gomes, Dylan G.E.},
	month = jan,
	year = {2022},
	pmid = {35116198},
	pmcid = {PMC8784019},
	pages = {e12794},
	file = {PubMed Central Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\24M8FFGR\\Gomes - 2022 - Should I use fixed effects or random effects when .pdf:application/pdf},
}

@article{van_boekel_pool_2021,
	title = {To pool or not to pool: {That} is the question in microbial kinetics},
	volume = {354},
	issn = {0168-1605},
	shorttitle = {To pool or not to pool},
	url = {https://www.sciencedirect.com/science/article/pii/S0168160521002427},
	doi = {10.1016/j.ijfoodmicro.2021.109283},
	abstract = {Variation observed in heat inactivation of Salmonella strains (data from Combase) was characterized using multilevel modeling with two case studies. One study concerned repetitions at one temperature, the other concerned isothermal experiments at various temperatures. Multilevel models characterize variation at various levels and handle dependencies in the data. The Weibull model was applied using Bayesian regression. The research question was how parameters varied with experimental conditions and how data can best be analyzed: no pooling (each experiment analyzed separately), complete pooling (all data analyzed together) or partial pooling (connecting the experiments while allowing for variation between experiments). In the first case study, level 1 consisted of the measurements, level 2 of the group of repetitions. While variation in the initial number parameter was low (set by the researchers), the Weibull shape factor varied for each repetition from 0.58–1.44, and the rate parameter from 0.006–0.074 h. With partial pooling variation was much less, with complete pooling variation was strongly underestimated. In the second case study, level 1 consisted of the measurements, level 2 of the group of repetitions per temperature experiment, level 3 of the cluster of various temperature experiments. The research question was how temperature affected the Weibull parameters. Variation in initial numbers was low (set by the researchers), the rate parameter was obviously affected by temperature, the estimate of the shape parameter depended on how the data were analyzed. With partial pooling, and one-step global modeling with a Bigelow-type model for the rate parameter, shape parameter variation was minimal. Model comparison based on prediction capacity of the various models was explored. The probability distribution of calculated decimal reduction times was much narrower using multilevel global modeling compared to the usual single level two-step approach. Multilevel modeling of microbial heat inactivation appears to be a suitable and powerful method to characterize and quantify variation at various levels. It handles possible dependencies in the data, and yields unbiased parameter estimates. The answer on the question “to pool or not to pool” depends on the goal of modeling, but if the goal is prediction, then partial pooling using multilevel modeling is the answer, provided that the experimental data allow that.},
	urldate = {2024-04-23},
	journal = {International Journal of Food Microbiology},
	author = {van Boekel, M. A. J. S.},
	month = sep,
	year = {2021},
	keywords = {Bayesian regression, Heat inactivation, Model comparison, Multilevel modeling, Pooling, Prediction, Weibull model},
	pages = {109283},
	file = {ScienceDirect Snapshot:C\:\\Users\\au645332\\Zotero\\storage\\6LXXQELD\\S0168160521002427.html:text/html},
}

@article{ranger_modeling_2020,
	title = {Modeling {Responses} and {Response} {Times} in {Tests} {With} the {Hierarchical} {Model} and the {Three}-{Parameter} {Lognormal} {Distribution}},
	volume = {80},
	issn = {0013-1644},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7565119/},
	doi = {10.1177/0013164420908916},
	abstract = {The hierarchical model of van der Linden is the most popular model for responses
and response times in tests. It is composed of two separate submodels—one for
the responses and one for the response times—that are joined at a higher level.
The submodel for the response times is based on the lognormal distribution. The
lognormal distribution is a skew distribution with a support from zero to
infinity. Such a support is unrealistic as the solution process demands a
minimal processing time that sets a response time threshold. Ignoring this
response time threshold misspecifies the model and threatens the validity of
model-based inferences. In this article, the response time model of van der
Linden is replaced by a model that is based on the three-parameter lognormal
distribution. The three-parameter lognormal distribution extends the lognormal
distribution by an additional location parameter that bounds the support away
from zero. Two different approaches to model fitting are proposed and evaluated
with regard to parameter recovery in a simulation study. The extended model is
applied to two data sets. In both data sets, the extension improves the fit of
the hierarchical model.},
	number = {6},
	urldate = {2024-04-24},
	journal = {Educational and Psychological Measurement},
	author = {Ranger, Jochen and Kuhn, Jörg Tobias and Ortner, Tuulia M.},
	month = dec,
	year = {2020},
	pmid = {33116327},
	pmcid = {PMC7565119},
	pages = {1059--1089},
	file = {PubMed Central Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\PFSN3PJ3\\Ranger et al. - 2020 - Modeling Responses and Response Times in Tests Wit.pdf:application/pdf},
}

@article{jain_comparative_2015,
	title = {A comparative study of visual and auditory reaction times on the basis of gender and physical activity levels of medical first year students},
	volume = {5},
	issn = {2229-516X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4456887/},
	doi = {10.4103/2229-516X.157168},
	abstract = {Background:
Reaction time (RT) is a measure of the response to a stimulus. RT plays a very important role in our lives as its practical implications may be of great consequences. Factors that can affect the average human RT include age, sex, left or right hand, central versus peripheral vision, practice, fatigue, fasting, breathing cycle, personality types, exercise, and intelligence of the subject.

Aim:
The aim was to compare visual RTs (VRTs) and auditory RTs (ARTs) on the basis of gender and physical activity levels of medical 1st year students.

Materials and Methods:
The present cross-sectional study was conducted on 120 healthy medical students in age group of 18–20 years. RT for target stimulus that is, for the beep tone for measuring ART, and red circle for measuring VRT was determined using Inquisit 4.0 (Computer Software) in the laptop. The task was to press the spacebar as soon as the stimulus is presented. Five readings of each stimulus were taken, and their respective fastest RT's for each stimuli were recorded. Statistical analysis was done.

Results:
In both the sexes’ RT to the auditory stimulus was significantly less (P {\textless} 0.001) as compared to the visual stimulus. Significant difference was found between RT of male and female medical students (P {\textless} 0.001) as well as between sedentary and regularly exercising healthy medical 1st year students.

Conclusion:
The ART is faster than the VRT in medical students. Furthermore, male medical students have faster RTs as compared to female medical students for both auditory as well as visual stimuli. Regularly exercising medical students have faster RTs when compared with medical students with sedentary lifestyles.},
	number = {2},
	urldate = {2024-04-24},
	journal = {International Journal of Applied and Basic Medical Research},
	author = {Jain, Aditya and Bansal, Ramta and Kumar, Avnish and Singh, KD},
	year = {2015},
	pmid = {26097821},
	pmcid = {PMC4456887},
	pages = {124--127},
	file = {PubMed Central Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\XL97ZHCZ\\Jain et al. - 2015 - A comparative study of visual and auditory reactio.pdf:application/pdf},
}



@misc{vehtari_pareto_2024,
	title = {Pareto {Smoothed} {Importance} {Sampling}},
	url = {http://arxiv.org/abs/1507.02646},
	abstract = {Importance weighting is a general way to adjust Monte Carlo integration to account for draws from the wrong distribution, but the resulting estimate can be highly variable when the importance ratios have a heavy right tail. This routinely occurs when there are aspects of the target distribution that are not well captured by the approximating distribution, in which case more stable estimates can be obtained by modifying extreme importance ratios. We present a new method for stabilizing importance weights using a generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios. The method, which empirically performs better than existing methods for stabilizing importance sampling estimates, includes stabilized effective sample size estimates, Monte Carlo error estimates, and convergence diagnostics. The presented Pareto \${\textbackslash}hat\{k\}\$ finite sample convergence rate diagnostic is useful for any Monte Carlo estimator.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Vehtari, Aki and Simpson, Daniel and Gelman, Andrew and Yao, Yuling and Gabry, Jonah},
	month = mar,
	year = {2024},
	note = {arXiv:1507.02646 [stat]},
	keywords = {Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
	annote = {Comment: The final version with minor corrections. To be published in JMLR. 58 pages},
	file = {arXiv.org Snapshot:C\:\\Users\\au645332\\Zotero\\storage\\F7YFCQZ4\\1507.html:text/html;Full Text PDF:C\:\\Users\\au645332\\Zotero\\storage\\AD75SXTJ\\Vehtari et al. - 2024 - Pareto Smoothed Importance Sampling.pdf:application/pdf},
}
