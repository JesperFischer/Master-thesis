# Experimental data

Having investigated how the PF, both in terms of the interaction between parameter values, but also in terms of estimation uncertainty of the parameters are influenced by various factors. The thesis now turns to a re-analysis already published data. The goal with this re-analysis is 2-fold. First, it reiterates the fact that making assumptions about the structure of the data, can make big differences in the parameter estimates and their uncertainties. Second, it will serve as a starting point for understanding the utility of the internal model validity, as a metrics to gauge how trials and subjects interact on the statistical power of a model to reject a hypothesis, which will be investigated in the last section of the thesis. This final aspect of testing hypotheses will tie together how the validity steps above, can help determine the ability of a particular model to conduct hypothesis testing. 

## *Heart rate discrimination task*

The Heart rate discrimination task (HRD) as introduced in @legrand_heart_2022, is an interoceptive task, entailing that participants were instructed to attend to their internal bodily states. The study recruited 223 participants, who completed the task twice, within 6 weeks between visits. HRD task has participants internalize their own heart rate for 5 seconds, meanwhile the participant's heart rate is monitored and calculated in real time. Subsequently, participants are exposed to five auditory tones with a given frequency (not the internal frequency of the tone, but the frequency of how fast the tones is presented), that is either faster or slower than their own objective heart rate. The amount this auditory tone's frequency is faster or slower, is determined by the PSI ADO algorithm introduced in the "Adaptive design optimizing" section. This means that the stimulus value for the PF for this experiment, is the difference between the external tone's frequency and the observed heart rate of the participant. The binary responses are therefore either faster or slower, with faster referring to the belief that the individual heart rate was faster than the tone's frequency. For instance, one might have a heart rate of 50 beats per minute (BPM) and then hear tones in a frequency of 40 BPM, they would then be asked to respond whether this 40 BPM tone is slower or faster than their own heart rate. The authors of the experiment, ran a single participant level model of each subject, in each session, and then correlated the slope and threshold of the PF. They found a medium correlation between the threshold r = 0.5 p \< .001 between sessions, while the correlation for the slope was negligible r = 0.1, p = .15 [@legrand_heart_2022]. This particularly low reliability estimates of the slope, of the PF, entails that this parameter is a state and not a trait of a particular individual, at least over the 6 week time span investigated. The next section investigates how these reliability estimates might change given different assumptions, on the structure of the data, but also by employing different models by incorporating additional information in terms of reaction times and confidence ratings.

## *The models*

This section describes the models fit to the test-retest data-set described above. These models sort to examine the influence of different assumptions, on the correlation between session one and two, of the PF parameters.
The baseline model, is the single fit model and is going to be the same as fitted by the authors. This entails estimating each individual for each of the sessions separately without a lapse rate (i.e. a two parameter PF). Subsequently, a CC between the estimates from session one and two is calculated. Adding and propagating the uncertainty of these estimates will serve as the next model. Next, the same model as above with a lapse rate with be tested, in order to understand the influence of this parameter, in this particular data-set. 
Two types of hierarchical models are going to be fit. The first is a single layer hierarchical model, amounting to modeling the two sessions from the same multivariate normal distribution with priors for each session. This model directly captures the correlation between sessions, as its included in the variance - covariance matrix of the multivariate normal distribution. This model amounts to the model displayed in figure 3, with the participant level distributions removed. The last type of model is the nested hierarchical model (figure 3). This model assumes that all subjects have a mean level parameter which are drawn from the same population level multivariate normal distribution. Each parameter for each session is then drawn from a subject level distribution. For this last model the ICC is the statistical metric estimated by the model itself (i.e. what has been described as $ICC_1$), and the correlation will be calculated afterwards. 
In addition to examining the influence of the assumed data structure, in the fitted models, reaction times for each trial is also going to be included. This will be done in the same vein as described in the section about increasing information in cognitive models. Finally, a full model is going to be fit, which will incorporate continuous confidence rating available in the data-set. This full model will not only incorporate the reaction times on a trial-by-trial basis, but also these confidence ratings for each trial. Confidence ratings were included in the task of the original experiment to examine the participants' interoceptive metacognitive abilities. Here these confidence ratings are used to inform the parameters of the underlying psychometric function, similarly to the reaction times. The confidence ratings are going to be modeled in close resemblance to the reaction times, just inverted. This inversion is because at the threshold of the psychometric function the uncertainty about the stimulus representation is the highest, and therefore reaction times should be their highest, but confidence should be at the lowest. Another difference between the reaction times and the confidence ratings is their range of possible values, and therefore the probability density function used to describe them. The confidence ratings in the task were bounded between 0 and 100 ranging from complete uncertainty to certainty. The beta distribution is a natural choice of probability density function for such kind of double bounded variable, as its bounded between 0 and 1 [@geissinger_case_2022]. The problem with using beta distribution, in this case, is the edge cases of 0 and 1's which for the confidence ratings are 0 and 100, when dividing each confidence rating with 100. One approach to circumvent this issue is to model these edge cases separately, by using a zero-one-inflated beta distribution. However, this model treats these edge values as separate processes, which does not align with the experiment, because confidence ratings are meant to represent a continuous. For simplicity, the thesis therefore subtracts a small number i.e. 0.001 from the 1 ratings and adds 0.001 to the 0 ratings, making it possible to use the beta distribution for the full range of confidence ratings. Admittedly, this approach of modeling the bounded ratings between 0 and 100 is tenuous, and new methods are slowly being developed see @kubinec_ordered_2023. Reaction times of the responses, were at maximum 8 seconds, and were modeled by the shifted log normal distribution introduced previously. To fully understand the parameters and implications thereof readers are refered the [Github](https://github.com/JesperFischer/Master-thesis/tree/main/Shiny%20app), where a shiny app has been made to demonstrate the full model [@R-shiny].

## *Results*

Table 3 displays the CC between the first and second session for the threshold and slope for each model, when uncertainty was propagated using bootstrapping. For a full table of all parameters, of all models, as well as with and without uncertainty propagation, see [Supplementary table 1](https://github.com/JesperFischer/Master-thesis/blob/main/Supplementary%20tables/Supplementary%20table1.xlsx). This table is linked to the github of the thesis, due to the size.

\newpage

```{r table3, fig.width = 7, fig.height = 7, warning = F, message = F, echo = F}
table3 = read.csv(here::here("tables","table3.csv")) %>% mutate(X = NULL)

table3 = flextable::flextable(table3) %>% width(c(1,2), width = 1.7)%>% width(5, width = 1.1)%>% width(4, width = 1)


table3 = set_caption(table3,
  caption ="Table 3. Results from reanalysis of legrand (2022). Table showing the correlation between sessions of the threshold (alpha) and slope (beta) parameter of the psychometric function using different model fomulations as well as hierarchical model structures.")



table3
```

\linebreak

Table 3 highlights the differences in the session-by-session correlation of the slope and threshold for the PF, when additional assumptions of the hierarchical structure is assumed (structure column). Additionally, models also included the reactions times as well as the confidence ratings (model column). Generally, table 3 shows an increase in the correlation with higher assumed structure, but also with increased complexity of the type of responses modeled. Interestingly, the models with confidence ratings included, performed worse than the models with only the added reaction time, perhaps indicating improper modeling of these. The main difference in session-by-session correlation between the two hierarchical models can be found in the threshold, as the nested hierarchical model outperforms the non-nested hierarchical model. 

A concern of this approach of just looking the CCs, is that a model with a high session by session correlation, might not fit the data. Therefore an examination of the model fit is crucial, inorder to ensure that the nested hierarchical model also fits the data. One approach to access this would be to examine model fit using common metrics such as; leave one out cross validation, information criterion etc. The difficulty with this, is that most of the models are incompatible. This incompatibility stems from the models being fit to differing amounts of subjects, in the case of hierarchical vs single fit models, and to differing amounts of dependent variables in the case of within model architectures. Another consideration for not conducting model comparison, in the models that are comparable, is that the difference between these model is in the assumption of the data, and is therefore something that should have been decided, before modeling the data. Here, all types were used in order to investigate the differences in results. Therefore, given that was known that each subject was accessed twice (and not a new participant was tested), and that the nested Hierarchical model captures this assumption, one should be inclined to choose this model, regardless of the session by session correlation.

Instead of directly comparing the comparable models, one might look at posterior predictive checks. These checks investigates whether the model predictions align with the data. Posterior predictive checks were performed for the most complicated models, to ensure that the models are capturing the underlying patterns, in the collected data. Posterior predictive checks, on both group level and single subject level, can be seen in supplementary Figure 8-11 together with Supplementary Note 5.
