<!-- This section still needs work and determined how much more or less is needed-->

## *Modeling definitions*

This thesis will revolve around building, refining, testing, and designing models of cognition. To do this cognitive modelling will be deployed. Here cognitive modelling is meant as an intermediate level in a hierarchy of computational models on top, and statistical models in the bottom. The distinction between these concepts can be found in their flexibility, assumptions, and scope of investigation. It should be noted that all these types of models have many things in common such as being mathematical representations of a data generating process and that these are working definitions with fuzzy boundaries [@durstewitz_computational_2016].

*Statistical models* are the models primarily used in medical, social, and educational sciences, these models mostly consist of linear and generalized linear (mixed) models. What these models have in common is that they are linear combinations of independent variables which are sometimes transformed (making them generalized) to a particular domain such that this linear combination maps to a dependent variable. The mathematical representation of such models are as follows:

$$
F(y)=\beta¬∑X+\epsilon
$$ 
Where y is a vector of dependent variables of N elements, F is a link function that maps the conditional mean unto a particular space, common link function are the logit and log transformations which maps unto domains of [0 ; 1] and [0 ; ‚àû] respectively, which makes predictions on probabilities and strictly positive values like reaction times possible. $\beta$ is a vector of regression coefficients of P predictors which gets estimated, X Is a matrix of independent variables of size [N, P]. Lastly $\epsilon$ is a vector of N elements containing the errors of the model predictions on the dependent variables. The benefit of these regression models is that maximum likelihood estimators are available meaning that parameters estimates can be calculated using a frequentists statistical framework, making the estimation process fast and efficient. However, the downfall of these models is that they put quite big constraints on the types of models that can be fit, i.e. there must be a linear mapping between all independent variable and the dependent variable in a domain that can be mapped with a link function. This constraint will in many instances make theories hard or impossible to test as human behavior and cognition is highly nonlinear in many ways [@ivanova_beyond_2022]. It should be noted that the correlation coefficient examined in the previous section, can be thought of as a special case of this linear model where $\beta$ is a single value and y and x are z-transformed vectors, see supplementary figure 2.

*Cognitive models* are models that are meant to resemble the generative processes of human behavior more closely. These models are generally more theoretically driven as the constraint of linear combinations is avoided, by employing different optimization schemes that sometimes use sampling algorithms to obtain the results. In many cases cognitive models are estimated in a Bayesian framework due to the flexibility with which models can be specified. The main advantage of these models, in this context, is the added freedom in model specification, but see discussion for other advantages.

*Computational models* are the upper most level of the hierarchy which here will be used to refer to the generalization of cognitive models to other scientific domains, such as physics, biology chemistry etc. These models are outside the scope of this thesis.

These three categories are arbitrary, and many methods and models will fall between them, with this vague definition. However many times these arbitrary definitions do add value in communicating what general framework we are working in and thereby what methods are used. The next section will describe a particular cognitive model which will be the the focal point for the rest of the thesis.

\newpage

## *Model descriptions*

In this thesis the psychometric function (PF), will be investigated as this has been a stable corner stone in the cognitive science literature across many different sub fields [@courtin_spatial_2023; @bahrami_what_2012; @coates_changes_2014; @ma_memorability_2024]. The psychometric function is a continuous function that maps real or positive inputs into probabilities, i.e. the domain is $[-\infty ; \infty]$ whereas the range is $[0 ; 1]$. In most cases the PF used is like a logistic regression in statistical modeling and is commonly used in perceptual research where the inputs are stimulus intensities, and the probabilities are then converted into binary forced choices through a Bernoulli or binomial distribution. The mapping of inputs to probabilities is usually done through a cumulative density function such as the cumulative logistic or normal distribution, which amounts to conducting a logistic or probit regression in the statistical framework. The main difference between the statistical and cognitive framework of the PF is the number of parameters. The least number of parameters used to describe the PF is 2 the threshold and the slope ($\alpha$,$\beta$). These two parameters describe the center of the curve, with $\alpha$ being the intensity of the stimulus at probability 0.5 and $\beta$ being the steepness of the function around this value. In the cognitive modeling framework one or two more parameters are typically introduced the lapse and guess rates ($\lambda$, $\gamma$). These two parameters together handle the tails (i.e. the far ends) of the psychometric functions and essentially makes the probability in the two ends of the psychometric no deterministic i.e. the upper and lower bounds become ùõæ and ùúÜ instead of 0 and 1, see figure 4. These parameters help with fitting the PF to data where sometimes attentional slips or wrong button presses happen and it can be shown that including these parameters will greatly improve the estimation of the slope of the PF if lapses and or guesses are present in the data [@wichmann_psychometric_2001]. This also makes intuitive sense as the function cannot predict deterministic (i.e. probabilities of 0 or 1) if there are responses at a high stimulus level which was caused by a lapse. Figure 4 depicts how all these parameters change the shape of the PF. For the sake of this thesis, I'll be using the cumulative normal distribution to map stimulus values to probabilities with a single lapse rate. This single lapse rate will govern the distance between the upper and lower bound, essentially making it equally likely to have an erroneous response for high and low stimulus values. This mathematical formulation of the function is as follows:

$$
p(x | \alpha, \beta, \lambda) = \lambda + (1-2 * \lambda) * (0.5+0.5 * erf{(\frac{x-\alpha}{\beta * \sqrt{2}})})    
$$

<!-- this plot should be of above function-->

```{r figure4, fig.width = 7, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 4 Psychometric parameters.** Displays how the parameters alpha ($\\alpha$), beta ($\\beta$) and lambda ($\\lambda$) of the psychometric fucntion changes its shape. Columns display how the beta parameters changes the slope of the function. Rows show how alpha changes the location of the center of the function changes. Lastly, colors in the plot depict how lambda changes the asympotes in extreme stimulus (x) values."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_4_psychometric_parameters.png")), scale = 1)

```

## *Model validation.*

In the same vein of validating the bootstrapping approach with the analytical solution in the previous section about measurement uncertainty, cognitive models themselves are many times validated to ensure that at least in principle the parameters of the model can be estimated with increasing accuracy with increasing number of trials. This section will highlight some of the emerging ways in which computational models in the cognitive science literature are being tested and validated and takes offset in the seminal paper from Wilson and Collins [@wilson_ten_2019] describing 10 simple rules of computational modeling, which is commonly cited when validation of computational models are described [@hess_bayesian_2024].

There are at least three main challenges when building and validation cognitive models which are particularly relevant when writing novel models. How do we know that our models do what we think they do (identifiability). How do we know that they accurately estimate the parameters of interest (Internal validity)? And lastly how do we know that we can distinguish between competing models (external validity). The last challenge is beyond the scope of the current thesis and is well covered elsewhere [@wilson_ten_2019]. 

The answer to the first two challenges must be found in simulations when our models become more and more complex and analytical solutions are sparse. This simulation practice revolves around selecting an appropriate range of parameter and using these to simulate data from our models and then refitting the data to then see how well the model approximates the simulated parameter values. It should therefore come as no surprise that ensuring that in these simulations, we would like our models to perform well, such that we can have faith in them when the real underlying process is unknown, i.e. analyzing real world data. An appropriate range of parameter values for a particular model can be difficult to select as is exactly the problem of identifiability. However, several lines of information can help gauge this. Firstly, looking at mathematical constraints of the model formulations can reduce the possible ranges of parameter values. For the case of the psychometric function this amounts to ensuring that the slope is strictly positive as this ensures that increasing levels of stimuli (when x increases) will produce greater probabilities of responding 1, but also ensure that the standard deviation of the underlying probability density function is strictly positive. The lapse rate of the psychometric will be constrained between 0 and 0.5 again to ensure that the shape of the psychometric. lapse rates below 0 and above 1 will produce probability values outside the [0; 1] range and values above 0.5 will flip the shape of the psychometric, as negative slope values will. Not containing the PF in this way could lead to two distinct solutions to a given problem as negative slope values and lapse rates above 0.5 would be able to produce the same mathematical transformation of stimulus values to probabilities making the solution non unique.

From a more theoretical level an appropriate range of parameter values can be narrowed down by looking at the function of interest and investigating whether the observed behavior (given the parameter values) is physically or biologically plausible and which values we would expect are more frequent. For the PF we might expect a few of our participants to not be particularly interested in the task and therefore just respond at random, which would amount to having a lapse rate of 0.5 or really shallow slopes, however this behavior is quite unlikely and expecting only few lapses in the experiment, given that it is conducted in a quiet environment is likely. Lastly using empirical knowledge from the literature at large helps narrow the parameter space further. For the sake of argument, one might investigate the detection threshold for cold stimulation on the skin. Just given this information alone we can narrow down the threshold for the cold detection to being below the skin temperature of around 30-34 degrees$^\circ$ [@courtin_spatial_2023] and -273 degrees. however knowledge from the scientific literature would suggest that thresholds between 28 and 33$^\circ$ would capture most of the population [@lithfous_accurate_2020]. These same arguments would apply for the slope. This practice of investigating the assumptions of the used parameter values is closely related to those of prior predictive checks when doing Bayesian inference. Prior predictive checks serve as a check of the model, without having seen any data. This check also revolves around simulating data from just the priors of the model and then investigating whether these conform with both what is physically and theoretically plausible, but also serves as a tool to investigate that the model can capture the behavior that is expected from the given experiment [@kruschke_bayesian_2021].

The next challenge is about internal validity i.e. can our model estimate the parameter values that was used to simulate the data that the model estimates the parameter values on. To test and validate our models, we simulate data from pre-specified parameter values which have been deemed to be appropriate using the first step. We then feed our models with this simulated data and investigate how well the model can estimate the latent simulated parameters. This exercise of simulating behavior and then re-estimating the parameter values from the simulated behavior is commonly known as parameter recovery. Generally if this procedure succeeds, then the parameters are said to be recovered. The satisfactory criterion often refers to some correlation coefficient, between the estimated and simulated parameter values [@wilson_ten_2019]. 


Parameter recovery can thus be thought of as an internal validation of a model, which if done properly should increase the faith in the parameter estimates when the model is fit to real world data. This is because if we had known the parameters values beforehand (i.e. simulated them) then we know that they are somewhat close to the estimated parameter values we got from fitting our model to the data. The assumption is thus; if our model recovers the parameter values well in a simulated setting then it must also do so when fitted to real world data where the underlying parameters are unknown. This assumption is of cause not necessarily true and rests on auxiliary assumptions such as that the underlying generative cognitive model is the same or at least close to the same as the one used to model the data. Because the process of doing parameter recovery assumes that we know the underlying generative model, which is not the case when fitting real world data. To further elucidate this point we imagine using the 3 parameter PF described above, we find that it recovers its parameters well using simulated parameters from the same model. However, if we instead of simulating data from the same underlying model, instead simulated data where the underlying cumulative distribution was the logistic or another cumulative probability density function, we might find that our model cannot well recover the parameters. This is of course nonsensical from the beginning, as how might our model recover parameters of another model, but many times the differences in our model space (i.e. the models that we think underlie the generative process) are similar and the parameters have similar meanings as they come from the same or similar underlying theory, meaning they can be compared. This last point of ensuring that we are selecting the right generative model is the challenge of external validity. The challenge is that infinitely many generative models exist that are also compatible with the observed behavior. This challenge cannot easily be solved as ensuring that we are using the right generative model would entail testing all generative models and being able to compare them, while ensuring that all these models are distinguishable. What is therefore commonly done in the cognitive science litterature, is to use the theoretical framework(s) to build competing models which contain different assumptions of the underlying generative process and then compare this subset of the entire model space, as these are the models that our theories deem relevant. This highlights two important aspects, firstly our models reflect our theories and are therefore at best as good as our theories and secondly, we are surely missing the real generative model in most cases. In practice what is commonly done is that models are fit to real world data and then compared on how well they can describe the data using statistical metrics such as information criteria. The problem with this approach is whether we can accurately distinguish the the particular models that we are testing. This challenge has been addressed using model recovery, which is the act of simulating data from all tested models and then refitting all models to the data simulated by all individual models. Going back to the example of the PF we might have two competing theories of how stimulus values are translated into binary choices, one involving the lapse rate and one without, further we want to ensure that we can distinguish between the normal and logistic cumulative distributions which transform stimulus values into probabilities in different fashions. In this practical example the model space consists of 4 models i.e. two or three parameters for each of the two types of PFs. One would therefore simulate data from these 4 distinct models and fit them all individually to each of the 4 simulated datasets and lastly determine which of the 4 models describe the data the best in each case. The result of such model recovery is a N times N matrix with N being the number of models, the rows being which model was used for the simulation and columns being which model was used to fit the data. The entries of the matrix are commonly depicted as the probability of choosing a particular model given the data simulating model. An identity matrix therefore represents that the models are completely distinguishable and anything else would indicate that in some of the simulations the best fitting model was not the model that simulated the data [@wilson_ten_2019].

## *Limitations of current model validation steps*

The model validation steps above should ideally serve to increase our faith in our models, their parameters, and the comparison between them. However, the metrics used to access these different types of validations are flawed. Firstly, the metrics used can be easily manipulated (faithfully or not) to show good model validation when masking the actual poor or terrible validation. This problem can thus introduce false faith in the model and overconfidence in the inference made based on it. Next and perhaps more importantly the metrics used are not sensitive or specific enough to give the person building the model information about how and were in the model space the models perform well, thereby leaving valuable insights hidden. In this section I will highlight the metrics commonly used in the literature for model validation which are described in @wilson_ten_2019, focusing one of the challenges described above; internal recoverability or parameter recovery.

As mentioned above internal recoverability of computational models are accessed by simulating data from a model given a set of parameters. This behavioral data is then fitted to the model which then optimizes for the parameters, given the data. What is commonly done is then estimating the correlation coefficient between the estimated and simulated parameters. In their seminal paper @wilson_ten_2019 describes that in a perfect world the estimated and simulated parameters should be tightly correlated without any bias, and that a weak correlation could mean bugs in the code or an underpowered study i.e. few trials. They also reiterate that plotting simulated vs estimated parameters should be done to access if ranges of parameter values are problematic and whether there might be biases. I will here argue that the correlation coefficient is an inappropriate metric and that a version of an intra class correlation (ICC) is better suited for the task. Acknowledging two important things; neither metric is perfect, and visually inspecting the simulated vs estimated parameter scatterplot is crucial. The importance in using the right metric is therefore as a precautionary step given that some literatures are starting to just report correlation coefficients without this crucial scatter plot, which arguably in some cases would make the correlation coefficient meaningless, see problems with the correlation coefficient below. These precautionary steps are crucial to enforce, in the development stages of new statistical, cognitive or computational models as they will serve as the basis of model validation and if not sensitive or specific enough many resources might be used in using a model that in reality cannot be properly identified. This would therefore serve as a roadblock for scientific progress as years might pass before someone realizes that the model used in the field is not behaving properly.

## *Current problems with internal recoverability of models (parameter recovery)*

The first and perhaps biggest problem of internal recoverability of computational models Is that it is not universally done, which from a readers perspective makes it hard or even impossible to know if the generative model in question can be trusted. The second, almost ubiquitous problem in the literature using parameter recovery is that interactions between parameters are either neglected or disregarded. This is less of a concern for individuals using an established cognitive models wanting to ensure that given their experimental design and ranges of parameters are sensible, but a big concern in highly cited method papers describing and formalizing the models themselves. A prime example of this is the Hierarchical Gaussian filter paper [@mathys_bayesian_2011 ; @mathys_uncertainty_2014]. Where after having laid out the equations of the model, two of the most crucial parameters of the model are held constant when performing parameter recovery. Even in much more simple models such as with the PF described above, I will show that there are tradeoffs and interchangeability between parameters. The last problem with parameter recovery is the metric used to access it. As has been suggested elsewhere, the correlational approach to parameter recovery is at best insufficient and at worst misleading [@schurr_dynamic_2024]. The three most obvious problems with using correlation coefficients to examine internal recoverability is as follows;

Correlation coefficients are invariant to linear transformations, making two sets of variables i.e. [1,2,3] and [1,2,3] have the same correlation after transforming on one of the sets with linear transformation. y=2\*x+3 (or report as a matrix idk) Resulting in the sets [1,2,3] and [5,7,9]. This invariance to linear transformations does not make sense for parameter recovery as we want a metric that penalizes this behavior.

The domain of correlations is between -1 and 1. This directionality also does not make sense given that a correlation coefficient of -1 would mean perfect parameter recovery, with a negative sign of the simulated or estimated parameter meaning that you do recover the value (or the linear transformed value) just not the sign. Ideally, we would want a metric that goes from no recovery to perfect recovery.

Lastly, the interpretation of the correlation coefficient in terms of parameter recovery is difficult. What is a sufficiently large correlation coefficient for the parameter to be said to be recovered and what types of uncertainty is causing the correlation to be less than ideal. Authors have tried to make such distinction without much traction [@white_testing_2018]. All these issues are similar to what researchers face when wanting to estimate the stability and or test -retest reliability of different metrics over time, where the solution has been to use the ICC as the metric instead of simple correlation coefficients [@schurr_dynamic_2024].

## *ICC Parameter recovery*

Given that the idea of using the ICC as a metric for parameter recovery is relatively new and to the authors knowledge has only been suggested and not been used anywhere in the literature [@schurr_dynamic_2024] I will here outline what the ICC is and how it can overcome some of the shortcomings of the correlation coefficient. The ICC is in its simplest form a ratio of irreducible variances (uncertainties) to the total variance in the data. In practical terms revovling around a cognitive science experiment the irreducible uncertainty is the uncertainty between subjects, whereas the total uncertainty can have several parts. In order to calculate the ICC, a model is needed that can properly account for these different types of variance and the typical approach are hierarchical models, where known structure of the data is added to the model.

Taking an example, we imagine a researcher doing a test-retest reliability study on a parameter of a cognitive model. His subjects are coming in for x sessions and doing the same cognitive task each time. We will now assume that all subjects come from the same underlying distribution of say humans (i.e. the population), this is the highest level in the hierarchy and is governed by a population mean and a population variance, i.e. the between subject variance. The next level in the hierarchy is the subject level, here each subject has their own means and variances (within subject variances), their means are drawn from the population distribution. Now for each session that the subject is in, a parameter value is drawn from this subject level distribution which then governs the participants' behavioral responses. This nested hierarchical structure is demonstrated in figure 5, where each of the levels are governed by the levels above and each of the levels has a variance associated with it, where the between subject variance is the variance of the population level distribution and the within subject variance is the variance of each of the participant level distributions. The ICC as mentioned above is the ratio between within and between subject variances.

$$
ICC = \frac{\sigma^2_{between}}{\sigma^2_{between} + \sigma^2_{within}}
$$ 

Where $\sigma^2_{between}$ is the variance between the subjects' parameter estimates and $\sigma^2_{within}$ is the within subject variance. Given that we are interested in the performance of the model we can simulate agents that have no within subject variance i.e. the same true parameter values for each session and then see how the number of subjects and or trials of the cognitive task will influence the model's ability to pick up on this association. This approach has one clear problem it does not necessarily tell us something about how well the model estimates the true parameter values for each participant at each session, as it just looks at how close each parameter is to itself between sessions. To capture this, one might use the mean squared errors (MSE) between the simulated and estimated parameter values, which serves as a residual error of the model. Including this into the ICC formulation is easy, as this is just another source of variance which can be added into the denominator, highlighting the fact that the ICC is a partitioning of variance in the model. This partitioning of variance is exactly what we are interested in when building and validating models, as this tells us where the model fails and where it might excel. In figure 5 this amounts to the difference between the estimated parameter value of a particular subject at a particular session and the simulated value. Formally we add the MSE into the equation and get.

$$
ICC = \frac{\sigma^2_{between}}{\sigma^2_{between} + \sigma^2_{within} + \sigma_\epsilon^2}
$$

Where $\sigma_\epsilon^2$ is the MSE. This conceptualization allows us to put parameter recovery for a model into a single value for each parameter that ranges from 0 to 1 which is going to be trial and subject level dependent, but also dependent on the simulated ranges of parameter values.

<!-- this plot should be of the nested hierarchial model -->

```{r figure5, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "**Figure 5. Visualization of a nested hierarchical model with sessions nested in subjects in a population.**"}

readRDS(file = here::here("Figures","figure_5_nested_hierarchical.RDS"))
```

## *Standard parameter recovery.*

The model and task used to demonstrate and investigate parameter recovery in this thesis is the 3 parameter PF described above which is widely used in the cognitive science literature from perception to decision making [@courtin_spatial_2023; @gold_how_2013].

After having specified the model, we can simulate data from different ranges of parameters to select appropriate ranges of parameter values. Firstly, parameter ranges are selected and simulated in accordance to table 1 and figure 6. Using the probabilistic programming language Stan and its interface with R, Rstan [@R-cmdstanr], it is possible to invert the model from the data to obtain estimates of the latent parameters which were used to simulate the data in the first place. Note that for all models displayed and estimated their convergence was accessed by ensuring rhat values were below 1.03 and no divergent transitions were present. Ideally all chains would have been inspected, but given the vast simulation approach presented throughout the thesis, visual inspection of each model was infeasiable and summary diagnotistic were used. Furthermore all priors for all models were weakly informed, meaning that most of the prior distributions were set as normal distributions with means of 0 and standard deviations of 3-5 in the unconstrained space. Readers are referred to to the supplementary material or the github for a list of all the priors used.


<!-- table of parameters and ranges -->

```{r table1, warning = F, message = F, echo = F, fig.cap = "**Table 1: parameter distributions** Parameter distributions for the simulated agents and the transformations for each of the parameters."}

table1 = read.csv(here::here("tables","table1.csv"))

table1$X = NULL


table1 = flextable::flextable(table1)

table1 = flextable::autofit(table1)

table1
```

```{r figure6, fig.width = 7, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 6. Displaying 100 samples of the parameters of the psychometric function from table1.** Visualization of the implications of the simulated parameters of table1. Black lines depicting individual subjects, while the red line depicts the group mean."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_6_psychometric_simulations.png")), scale = 1)
```


For the sake of argument, the pairwise scatter plot of estimated vs simulated parameter values are depicted in figure 7 with the added estimation uncertainty. This particular simulation is done for 100 subjects over 100 trials each where the stimulus values were selected as a sequence from -50 to 50 in increments of 1. Figure 7 also displays how adding the estimation uncertainty (of the parameters) to the correlation coefficient changes the resulting size and uncertainty estimate of the correlation coefficient (i.e. its estimation uncertainty). This addition of the estimation uncertainty again highlights how deceptive these estimates can be if uncertainty is not properly propagated as they are all inflated. This inflation is however not necessarily always the case, as if a couple of points fall way off the identity line with high uncertainties, they will have less weight when accessed with uncertainty compared to without, meaning that adding estimation uncertainty could in principle also increase the correlation coefficient, highlighting the non trivial and linear link when uncertainties are non linear aswell.

```{r figure7, fig.width = 7.2, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 7. Parameter recovery for the three parameters of the psychometric function.** Scatter plot of Simulated vs recovered parameter values, with error bars displaying the 95 highest density interval for that parameter on that simulation. Text on each facet shows the estimated correlation coefficient with its standard error with and without accounting for estimation uncertainty in the individual estimates."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","parameter_recovery_v1.PNG")), scale = 1)
```


Next the purposed ICC metric is tested on the same data set as above, crucially the data set above was simulated using only 50 simulations that were duplicated, making it eligible to compare the above standard parameter recovery with the ICC. This simulation therefore implies that there is no within subject variation, as the first 50 datasets were duplicated. One particular difference between the above single fit models presented above and the proposed model depicted in figure 5 is the hierarchical structure embedded in the model. The hierarchical structure of the model serves to shrink parameter estimates in relation to their distance and uncertainty from the mean of the higher level which they are drawn from, which in the end has been shown to improve predictive capacity. Hierarchical models are becoming corner stones in most cognitive science experiments [@bates_fitting_2015; @van_boekel_pool_2021; @gomes_should_2022].

Therefore to fairly test and compare the two internal validity metrics, correlation coefficient and the ICC, each of the metrics were calculated from this hierarchical model. the Two ICC values were calculated as above now referred to as $ICC_1$ and $ICC_2$.

```{r figure 8, fig.width = 7.2, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 8. Parameter recovery for the three parameters of the psychometric function using the hierarchical model.** Scatter plot of Simulated vs recovered parameter values, with error bars displaying the 95 highest density interval for that parameter on that simulation."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","parameter_recovery_v2_nested.PNG")), scale = 1)
```

As can be seen in figure 8, this hierarchical fit does improve the parameter recovery, both from a visual inspection (points falling closer to the identity line with less estimation uncertainty) and by comparing the correlation estimates between figure 7 and 8. It should here be noted that a single simulation like this would not be enough to ensure that the parameters are nicely recovered as a good example of this is the lambda parameter. The pairwise scatter plot of the nested hierarchical model seems to suggest that this parameter is quite well recovered. However, if we back calculate a lambda value of -5 (on the unconstrained scale as depicted in figure 7 and 8) that would corresponds to a lapse rate of around 1.3%, which obviously should be deficult to estimate when there are 100 trials for each subject, as most subjects therefore would have few if any lapses. See supplementary note 2 for a deeper explanation on this.

Turning the attention to the ICC values, it is firstly observed that $ICC_1$ on each of the 3 parameters has an upper bound at the maximum value of one, which is confirmed looked at the scatter plot. This can be seen as all the session one estimates are hidden behind the session two estimates with only a few estimates deviating slightly. The $ICC_2$ estimate is crucially the lowest for all three parameters. Visual inspection of the pairwise scatter plot makes this clear as this metric is penalizes for both the degree to which the points fall away from the identity line but also by the estimation uncertainty associated with these points. This therefore also explains why the alpha parameter is close to being asymptotic at 1, but with a little to be desired for simulated values between 0 and 10. An interesting observation in Figure 8 is that the difference between lambda and beta is minute in all the metrics used. In the next section it will be shown how it is possible to do better by reducing the estimation uncertainty. To lastly highlight the difference between the hierarchical and single fit model Figure 9 depicts the distribution of estimation uncertainty for the three parameters in the single and hierarchical fit. This Figure helps to explain why hierarchical models in general are prefered to single fit models as the partial pooling improves estimation of the parameters [@bates_fitting_2015; @van_boekel_pool_2021; @gomes_should_2022].

<!-- Estimation uncertainty  -->

```{r figure 9, fig.width = 7.2, fig.height = 4, warning = F, message = F, echo = F, fig.cap = "**Figure 9 Estimation uncertainty for each parameter for both single and hierarchical fit models** Each panel represents one of the three parameters of the psychometric function with the estimated uncertainty depicted as histograms. The color of the histogram shows whether the model was fit using the single fit or hierarchical model."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","Estimation_uncertainty_v1.PNG")), scale = 1)
```

<!-- Title idk  -->
