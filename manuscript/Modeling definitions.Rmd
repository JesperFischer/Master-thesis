<!-- This section still needs work and determined how much more or less is needed-->

# Modeling definitions and validation.

The rest of the thesis will revolve around refining, testing, and designing models of cognition. To do this cognitive modelling will be deployed. Here cognitive modelling is meant as an intermediate level in a hierarchy of computational models on top, and statistical models in the bottom. The distinction between these concepts can be found in their flexibility, assumptions, and scope of investigation. It should be noted that all these types of models have many things in common such as being mathematical representations of a data generating process and that these are working definitions with fuzzy boundaries [@durstewitz_computational_2016].

*Statistical models* are the models primarily used in medical, social, and educational sciences, these models mostly consist of linear and generalized linear (mixed) models. What these models have in common is that they are linear combinations of independent variables which are sometimes transformed (making them generalized) to a particular domain such that this linear combination maps to a dependent variable. The mathematical representation of such models are as follows:

$$
F(y)=\beta·X+\epsilon
$$ 
Where y is a vector of dependent variables of N elements, F is a link function that maps the conditional mean unto a particular space, common link function are the logit and log transformations which maps unto domains of [0 ; 1] and [0 ; ∞] respectively, which makes predictions on probabilities and strictly positive values like reaction times possible. $\beta$ is a vector of regression coefficients of P predictors which gets estimated, X Is a matrix of independent variables of size [N, P]. Lastly $\epsilon$ is a vector of N elements containing the errors of the model predictions on the dependent variables. The benefit of these statistical or regression models is that maximum likelihood estimators are available meaning that parameters estimates can be calculated using a frequentists statistical framework, making the estimation process fast and efficient. The downfall however is that they put quite big constraints on the types of models that can be fit, i.e. there must be a linear mapping between all independent variable and the dependent variable in a domain that can be mapped with a link function. This constraint will in many instances make theories hard or impossible to test as human behavior and cognition is nonlinear in different ways [@ivanova_beyond_2022]. It should be noted that the correlation coefficient examined in the previous section, can be thought of as a special case of this linear model where $\beta$ is a single value and y and x are z-transformed vectors, see supplementary figure 2.

*Cognitive models* are models that are meant to resemble the generative processes of human behavior more closely. These models are generally more theoretically driven as the constraint of linear combinations is avoided, by employing different optimization schemes that sometimes use sampling algorithms to obtain the results. In many cases cognitive models are estimated in a Bayesian framework due to the flexibility with which models can be specified. The main advantage of these models, in this context, is the added freedom in model specification, but see discussion for other advantages.

*Computational models* are the upper most level of the hierarchy which here will be used to refer to the generalization of cognitive models to other scientific domains, such as physics, biology chemistry etc. These models are outside the scope of this thesis.

These three categories are arbitrary, and many methods and models will fall between them, with this vague definition. However many times these arbitrary definitions do add value in communicating what general framework we are working in and thereby what methods are used. The next section will describe a particular cognitive model which will be the focal point for the rest of the thesis.

\newpage

## *The Psychometric function.*

The thesis will explore the psychometric function (PF), as this type of function has been a stable corner stone in the cognitive science literature across many different sub fields [@courtin_spatial_2023; @bahrami_what_2012; @coates_changes_2014; @ma_memorability_2024]. The psychometric function is a continuous function that maps real inputs (here called intensity values) onto probabilities, i.e. the domain $[-\infty ; \infty]$ with the range of $[0 ; 1]$. In most cases the PF used is like a logistic regression in statistical modeling and is commonly used in perceptual research where the inputs are stimulus intensities, and the probabilities are then converted into binary forced choices through a Bernoulli or binomial distribution. The mapping of inputs to probabilities is usually done through a cumulative density function such as the cumulative logistic or normal distribution, which amounts to conducting a logistic or probit regression in the statistical framework. The main difference between the statistical and cognitive framework of the PF is the number of parameters. The least number of parameters used to describe the PF is 2, the threshold ($\alpha$) and the slope ($\beta$).

These two parameters describe the center of the curve, with $\alpha$ being the intensity of the stimulus at probability 0.5 (i.e. the x-value at y = 0.5) and $\beta$ being the steepness of the function around the threshold. In the cognitive modeling framework one or two more parameters are typically introduced the lapse ($\lambda$) and guess rates ($\gamma$). These two parameters together handle the tails (i.e. the far ends) of the psychometric function and essentially makes the probability in the two ends of the psychometric no deterministic i.e. the upper and lower bounds become ($\lambda$) and ($\gamma$) instead of 0 and 1, see figure 4. These parameters help with fitting the PF to data where attentional slips or wrong button presses happen and it can be shown that including these parameters will greatly improve the estimation of the slope of the PF if lapses and or guesses are present in the data [@wichmann_psychometric_2001]. Figure 4 depicts how all these parameters change the shape of the PF. For the sake of this thesis, the cumulative normal distribution is used to map stimulus values to probabilities only with a single lapse rate. This single lapse rate will govern the distance between the upper and lower bound, essentially making it equally likely to have an erroneous response for high and low stimulus values. This mathematical formulation of the function is as follows:

$$
p(x | \alpha, \beta, \lambda) = \lambda + (1-2 * \lambda) * (0.5+0.5 * erf{(\frac{x-\alpha}{\beta * \sqrt{2}})})    
$$

<!-- this plot should be of above function-->

```{r figure4, fig.width = 7, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 4 Psychometric parameters.** Displays how the parameters alpha ($\\alpha$), beta ($\\beta$) and lambda ($\\lambda$) of the psychometric fucntion changes its shape. Columns display how the beta parameters changes the slope of the function. Rows show how alpha changes the location of the center of the function changes. Lastly, colors in the plot depict how lambda changes the asympotes in extreme stimulus (x) values."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_4_psychometric_parameters.png")), scale = 1)

```

## *Model validation.*

In the same vein of validating the bootstrapping approach with the analytical solution in the previous section about measurement uncertainty, cognitive models themselves are many times validated to ensure that at least in principle the parameters of the model can be estimated with increasing accuracy with increasing number of trials. This section will highlight some of the emerging ways in which computational models in the cognitive science literature are being tested and validated and takes offset in the seminal paper from Wilson and Collins [@wilson_ten_2019] describing 10 simple rules of computational modeling, which is commonly cited when validation of computational models are described [@hess_bayesian_2024].

There are at least three main challenges when building and validation cognitive models, which are particularly relevant when writing novel models. How do we know that our models do what we think they do (identifiability). How do we know that they accurately estimate the parameters of interest (Internal validity)? And lastly how do we know that we can distinguish between competing models (external validity). The last challenge is beyond the scope of the current thesis and is well covered elsewhere [@wilson_ten_2019]. The answer to the first two challenges must be found in simulations, especially when our models become more and more complex and analytical solutions are sparse or non existent. 



This simulation practice revolves around selecting an appropriate range of parameter values and using these to simulate data from our models and then refitting the data to then see how well the model approximates the simulated parameter values. It should therefore come as no surprise that ensuring that in these simulations, we would like our models to perform well, such that we can have faith in them when the real underlying process is unknown, i.e. analyzing real experimental data. An appropriate range of parameter values for a particular model can be difficult to select as this is exactly the problem of identifiability. However, several lines of information can help gauge this. Firstly, looking at mathematical constraints of the model formulations can reduce the possible ranges of parameter values. For the case of the PF this amounts to ensuring that the slope is strictly positive as this ensures that increasing levels of stimuli will produce greater probabilities of responding 1, but also ensure that the standard deviation of the underlying probability density function is strictly positive. The lapse rate of the PF will be constrained between 0 and 0.5 again to ensure a particular the shape of the PF. lapse rates below 0 and above 1 will produce probability values outside the [0; 1] range and values above 0.5 will flip the shape of the PF, as negative slope values will. Not constraining the PF in this way could lead to two distinct solutions to a given problem as negative slope values and lapse rates above 0.5 would be able to produce the same mathematical transformation of stimulus values to probabilities making the solution non unique. From a more theoretical level an appropriate range of parameter values can be narrowed down by looking at the function of interest and investigating whether the observed behavior (given the simulated parameter values) is physically or biologically plausible. For the PF we might expect a few of our participants to not be particularly interested in the task and therefore just respond at random, which would amount to having a high lapse rate i.e. above 0.2 (amounting to 20% lapse responses) or really shallow slopes, however this behavior is quite unlikely and expecting only few lapses in the experiment, given that it is conducted in a quiet environment is likely. Lastly using empirical knowledge from the literature at large helps narrow the parameter space further. For the sake of argument, one might investigate the detection threshold for cold stimulation on the skin. Just given this information alone we can narrow down the threshold for cold detection to being below the skin temperature of around 30-34 degrees$^\circ$ [@courtin_spatial_2023] and above the absolute zero of -273 degrees. However knowledge from the scientific literature would suggest that thresholds between 28 and 33$^\circ$ would capture most of the population [@lithfous_accurate_2020]. These same arguments would apply for the slope. This practice of investigating the assumptions of the simulated parameter values is closely related to those of prior predictive checks when doing Bayesian inference. Prior predictive checks serve as a check of the model, without having seen any data. This check also revolves around simulating data from just the priors of the model and then investigating whether these conform with both what is physically and theoretically plausible, but also serves as a tool to investigate that the model can capture the behavior that is expected from the given experiment [@kruschke_bayesian_2021]. 

The next challenge is about internal validity i.e. can our model estimate the parameter values that was used to simulate the data that the model estimates the parameter values on. To test and validate our models in this regard, we simulate data from pre-specified parameter values which have been deemed to be appropriate using the first step described above. We then feed our models with this simulated data and investigate how well the model can estimate the latent simulated parameters (i.e. those that produced the data). This exercise of simulating behavior and then re-estimating the parameter values from the simulated behavior is commonly known as parameter recovery in the cognitive modeling framework. Generally if this procedure succeeds, then the parameters are said to be recovered and the satisfactory criterion often refers to some correlation coefficient between the estimated and simulated parameter values [@wilson_ten_2019]. Parameter recovery can thus be thought of as an internal validation of a model, which if done properly should increase the faith in the parameter estimates when the model is fit to real experimental data. This is because if we had known the parameters values beforehand (i.e. simulated them) then we know that they are somewhat close to the estimated parameter values we got from fitting our model to the data. The assumption is thus; if our model recovers the parameter values well in a simulated setting then it must also do so when fitted to real experimental data where the underlying parameters are unknown. This assumption is of cause not necessarily true and rests on auxiliary assumptions such as; that the underlying generative cognitive model is the same or at least close to the same as the one used to model the data. Because the process of doing parameter recovery assumes that we know the underlying generative model, which is not the case when fitting real experimental data. To further elucidate this point, imagine using the 3 parameter PF described above; we find that it recovers its parameters well using simulated parameters from the same model. However, if we instead of simulating data from the same underlying PF, instead simulated data where the underlying cumulative distribution was the logistic, one might find that the model cannot well recover the parameters. This is of course nonsensical from the beginning, as how might our model recover parameters of another model, but many times the differences in our model space (i.e. the models that we think underlie the generative process) are similar and the parameters have similar meanings as they come from the same or similar underlying theories, meaning they can be compared as would be the case in this example. This last point of ensuring that we are selecting the right generative model is the challenge of external validity. The challenge is that infinitely many generative models exist that are also compatible with the observed behavior. This challenge cannot easily be solved as ensuring that we are using the right generative model would entail testing all generative models and being able to compare them, while ensuring that all these models are distinguishable. What is therefore commonly done in the cognitive science litterature, is to use the theoretical framework(s) to build competing models which contain different assumptions of the underlying generative process and then compare this subset of the entire model space, as these are the models that our theories deem relevant. This highlights two important aspects, firstly our models reflect our theories and are therefore at best as good as our theories and secondly, we are surely missing the real generative model in most cases. In practice what is commonly done is that models are fit to real experimental data and then compared on how well they can describe the data using statistical metrics such as information criteria or leave one out cross validation. The problem with this approach is whether we can accurately distinguish the the particular models that we are testing in principle. 

This challenge has been addressed using model recovery, which is the act of simulating data from all tested models and then refitting all models to the data simulated by all individual models. Going back to the example of the PF we might have two competing theories of how stimulus values are translated into binary choices, one involving the lapse rate and one without, further we want to ensure that we can distinguish between the normal and logistic cumulative distributions which transform stimulus values into probabilities in different fashions. In this practical example the model space consists of 4 models i.e. two or three parameters for each of the two types of PFs. One would therefore simulate data from these 4 distinct models and fit them all individually to each of the 4 simulated data-sets and lastly determine which of the 4 models describe the data the best in each case. The result of such model recovery is a N times N matrix with N being the number of models, the rows being which model was used for the simulation and columns being which model was used to fit the data. The entries of the matrix are commonly depicted as the probability of choosing a particular model given the data simulating model. An identity matrix therefore represents that the models are completely distinguishable and anything else would indicate that in some of the simulations the best fitting model was not the model that simulated the data [@wilson_ten_2019].

## *Limitations of current internal model validation steps*

The model validation steps above should ideally serve to increase our faith in our models, their parameters, and the comparison between them. However, some of the metrics used to access these different types of validations are flawed. Firstly, the metrics used can be easily manipulated (faithfully or not) to show good model validation when masking the actual poor or terrible validation. This problem can thus introduce false faith in the model and overconfidence in the inference made based on it. Next and perhaps more importantly the metrics used are not sensitive or specific enough to give the person building the model, information about how and were in the model space the models perform well, thereby leaving valuable insights hidden. In this section I will highlight the metrics commonly used in the literature for model validation which are described in @wilson_ten_2019, focusing one of the challenges described above; internal validity or parameter recovery. As mentioned above internal recoverability or validity of computational models are accessed by simulating data from a model given a set of parameters. This behavioral data is then fitted to the model which then optimizes for the parameters, given the data. What is commonly done is then estimating the correlation coefficient between the estimated and simulated parameters. In their seminal paper @wilson_ten_2019 describes that in a perfect world the estimated and simulated parameters should be tightly correlated without any bias, and that a weak correlation could mean bugs in the code or an under powered study i.e. few trials. They also reiterate that plotting simulated vs estimated parameters in a scatter plot should be done to access if ranges of parameter values are problematic and whether there might be biases. I will here argue that the correlation coefficient is an inappropriate metric and that a version of an intra class correlation (ICC) is better suited for the task. Acknowledging two important things; neither metric is perfect, and visually inspecting the simulated vs estimated parameter scatterplot is crucial. The importance in using the right metric is therefore as a precautionary step given that some literatures are starting to just report correlation coefficients without this crucial scatter plot, which arguably in some cases would make the correlation coefficient meaningless, see problems with the correlation coefficient below. These precautionary steps are crucial to enforce, in the development stages of new statistical, cognitive or computational models as they will serve as the basis of model validation and if not sensitive or specific enough many resources might be used in using a model that in reality cannot be properly identified. This would therefore serve as a roadblock for scientific progress as years might pass before it is realized that the model used in the field is cannot behave properly even in simulated settings.

## *Current problems with internal recoverability of models (parameter recovery)*

The first and perhaps biggest problem of internal validation of computational models, is that it is not universally done, which makes it hard or even impossible to know if the generative model in question can be trusted. The second, almost ubiquitous problem in the literature using parameter recovery is that interactions between parameters are either neglected or disregarded. This is less of a concern for individuals using an established cognitive models wanting to ensure that given their experimental design and ranges of parameters are sensible, but a big concern in highly cited method papers describing and formalizing the models themselves. A prime example of this is the Hierarchical Gaussian filter paper [@mathys_bayesian_2011 ; @mathys_uncertainty_2014]. Where after having laid out the equations of the model, two of the most crucial parameters of the model are held constant when performing parameter recovery. Even in much more simple models such as with the PF described above, I will show that there are tradeoffs and interchangeability between parameters. The last problem with parameter recovery is the metric used to access it. As has been suggested elsewhere, the correlational approach  is at best insufficient and at worst misleading [@schurr_dynamic_2024]. The three most obvious problems with using correlation coefficients to examine internal validity is as follows;

Correlation coefficients are invariant to linear transformations, making two sets of variables i.e. [1,2,3] and [1,2,3] have the same correlation after transforming one of the sets with linear transformation. Say the we use the transformation, $f(x)=2\cdot x+3$, resulting in the sets [1,2,3] and [5,7,9] this set will have the same correlation coefficient, but in terms of model validation would be very different. This in variance to linear transformations does not make sense for parameter recovery as we want a metric that penalizes this behavior.

The domain of correlations is  [-1; 1]. This directionality also does not make sense given that a correlation coefficient of -1 would mean perfect parameter recovery, with a negative sign, meaning that you do recover the value (or the linear transformed value) just not the sign. Ideally, we would want a metric that goes from no recovery to perfect recovery, and not from perfect recovery without the sign to perfect recovery with the sign.

Lastly, the interpretation of the correlation coefficient in terms of parameter recovery is difficult. What is a sufficiently large correlation coefficient for the parameter such that it can be said to be recovered and what types of uncertainty is causing the correlation to be less than ideal? Authors have tried to make such distinction without much traction [@white_testing_2018]. All these issues are similar to what researchers have faced when wanting to estimate the stability and or test -retest reliability of different metrics over time, where the solution has been to use the ICC as the metric instead of simple correlation coefficients [@schurr_dynamic_2024].

## *ICC Parameter recovery*

Given that the idea of using the ICC as a metric for parameter recovery is relatively new and has only been suggested and not implemented anywhere in the literature [@schurr_dynamic_2024] I will here outline what the ICC is and how it can overcome some of the shortcomings of the correlation coefficient in model validity. The ICC is in its simplest form a ratio of irreducible variances (uncertainties) to the total variance in the data. In practical terms revolving around a cognitive science experiment the irreducible uncertainty is the uncertainty between subjects, whereas the total uncertainty can have several parts. In order to calculate the ICC, these variances needs to be estimated such that their ratio can be computed. The estimation of the variances can be done using a model that can properly account for these different types of variance and the typical approach are hierarchical models, where known structure of the data is added to the model. Taking the introductory example, of a researcher doing a test-retest reliability study on the parameters of now a generalized linear model investigating anxiety and binary responses. His subjects are coming in for x sessions and doing the same task each time. We will now assume that all subjects come from the same underlying distribution of say humans (i.e. the population), this is the highest level in the hierarchy and is governed by a population mean and a population variance, i.e. the between subject variance. The next level in the hierarchy is the subject level, here each subject has their own means and variances (within subject variances), their means are drawn from the population level distribution. Now for each session that the subject is in, a parameter value is drawn from this subject level distribution which then governs the participants' behavioral responses. This nested hierarchical structure is demonstrated in figure 5, where each of the levels are governed by the levels above and each of the levels has a variance associated with it. Where the between subject variance is the variance of the population level distribution and the within subject variance is the variance of each of the participant level distributions. The ICC as mentioned above is the ratio between this within and between subject variances, see equation below:


$$
ICC = \frac{\sigma^2_{between}}{\sigma^2_{between} + \sigma^2_{within}}
$$ 

Where $\sigma^2_{between}$ is the variance between the subjects' parameter estimates and $\sigma^2_{within}$ is the within subject variance. Given that we are interested in the performance of the model we can simulate agents that have no within subject variance i.e. the same true parameter values for each session and then see how the number of subjects and or trials of the cognitive task will influence the model's ability to pick up on this association. This approach has one clear problem it does not necessarily tell us something about how well the model estimates the true parameter values for each participant at each session, as it just looks at how close each parameter is to itself between sessions. To capture this, one might use the mean squared errors (MSE) between the simulated and estimated parameter values, which serves as a residual error of the model. Including this into the ICC formulation is easy, as this is just another source of variance which can be added into the denominator, highlighting the fact that the ICC is a partitioning of variance in the model. This partitioning of variance is exactly what we are interested in when building and validating models, as this tells us where the model fails and where it might excel. In figure 5 this amounts to taking the difference between the estimated parameter value (distribution) of a particular subject at a particular session and the simulated value. Formally we add the MSE into the equation and get:

$$
ICC = \frac{\sigma^2_{between}}{\sigma^2_{between} + \sigma^2_{within} + \sigma_\epsilon^2}
$$

Where $\sigma_\epsilon^2$ is the MSE. This conceptualization allows us to put parameter recovery for a model into a single value for each parameter that ranges from 0 to 1 which is going to be trial and subject level dependent, but also dependent on the simulated ranges of parameter values like the correlation coefficient.

<!-- this plot should be of the nested hierarchial model -->

```{r figure5, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "**Figure 5. Visualization of a nested hierarchical model with sessions nested in subjects in a population.** Displaying from the top down how the nested hierarchical model assumes structure of the underlying data. From the model's perspective data (points in the bottom plot) is entered at the lowest level where the cognitive or statistical model is fitted. The parameters of this model is drawn from a session distribution which is nested within a subject distribution which again is nested in a population distribution. This nesting of the parameters of the model allows for seamless estimation of the partitioning of variance within the model. For the sake of parameter recovery within subject variance is simulated to be 0, and the mean squared difference between the session level distribution and the simulated parameter value is calculated and put into the denominator of the ICC metric."}

readRDS(file = here::here("Figures","figure_5_nested_hierarchical.RDS"))
```

Figure 5 displays the conceptualization of the ICC with the added MSE. Parameters flow from the population level distribution to the subject level distribtuion and into the session level distributions which then forms the cognitive model on the data at the trial level. Figure 5 displays this as a psychometric function for two sessions. The idea of parameter recovery using this framework, is therefore to access the degree to which the whole model can distinguish between the types of variation (uncertainty). What will be shown below is that within subject variance can be simulated to be 0 and the MSE can be calculated as the difference between the session level parameter estimates and the simulated parameter values. This therefore means that in the simulated setting the ICC displayed in the above equation is 1, i.e. the only source of variation is between subjects, but that the model itself might ascribe some of this variation to either within subjects or session variation in the parameter estimates.


## *Standard parameter recovery.*

The model and task used to demonstrate and investigate parameter recovery in this thesis is the 3 parameter PF described in the section "The Psychometric function", which is widely used in the cognitive science literature from perception to decision making [@courtin_spatial_2023; @gold_how_2013]. After having specified the model, we can simulate data from different ranges of parameters to select an appropriate range of parameter values. Parameter ranges are selected and simulated in accordance to table 1 and figure 6. Using the probabilistic programming language Stan and its interface with R, Rstan [@R-cmdstanr], it is possible to invert the model from the data to obtain estimates of the latent parameters which were used to simulate the data in the first place. Note that for all models displayed and estimated their convergence was accessed by ensuring rhat values were below 1.03 and that no divergent transitions were present. Ideally all chains would have been inspected, but given the vast simulation approach presented throughout the thesis, visual inspection of each model was infeasiable and summary diagnotistic were used. Furthermore all priors for all models presented were weakly informed, meaning that most of the prior distributions were set as normal distributions with means of 0 and standard deviations of 3-5 in the unconstrained space. Readers are referred to to the supplementary material or the github for a list of all the priors used.


<!-- table of parameters and ranges -->

```{r table1, warning = F, message = F, echo = F, fig.cap = "**Table 1: parameter distributions** Parameter distributions for the simulated agents and the transformations for each of the parameters."}

table1 = read.csv(here::here("tables","table1.csv"))

table1$X = NULL


table1 = flextable::flextable(table1)

table1 = flextable::autofit(table1)

table1
```

```{r figure6, fig.width = 7, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 6. Displaying 100 samples of the parameters of the psychometric function from table 1.** Visualization of the implications of the simulated parameters of table 1. Black lines depicting individual subjects, while the red line depicts the group mean."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_6_psychometric_simulations.png")), scale = 1)
```


For the sake of argument, the pairwise scatter plot of estimated vs simulated parameter values are depicted in figure 7 with the added estimation uncertainty. This particular simulation is done for 100 subjects over 100 trials where each of the stimulus values were selected as a sequence from -50 to 50 in increments of 1. Figure 7 also displays how adding the estimation uncertainty (of the parameters) to the correlation coefficient changes the resulting size and uncertainty estimate of the correlation coefficient (i.e. its estimation uncertainty). This addition of the estimation uncertainty again highlights how deceptive these estimates can be if uncertainty is not properly propagated as they are all inflated. This inflation is however not necessarily always the case, as if a couple of points fall way off the identity line with high uncertainties, they will have less weight when accessed with uncertainty compared to without, meaning that adding estimation uncertainty could in principle also increase the correlation coefficient and decrease its estimation uncertainty, highlighting the non trivial and non linear link when uncertainties from fitted models are added.


```{r figure7, fig.width = 7.2, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 7. Parameter recovery for the three parameters of the psychometric function in the unconstrained space.** Scatter plot of Simulated vs recovered parameter values, with error bars displaying the 95 % highest density interval for that parameter on that simulation. Text on each facet shows the estimated correlation coefficient with its standard error (estimation uncertainty) with and without accounting for estimation uncertainty in the individual estimates."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","parameter_recovery_v1.PNG")), scale = 1)
```


Next the purposed ICC metric is tested on the same data set as above, crucially the data set above was simulated using only 50 simulations that were duplicated, making it eligible to compare the above standard parameter recovery with the ICC. This simulation therefore implies that there is no within subject variation, as the first 50 data-sets were duplicated. One particular difference between the above single fit models presented and the proposed model depicted in figure 5 is the hierarchical structure embedded in the model. The hierarchical structure of the model serves to shrink parameter estimates in relation to their distance and uncertainty from the mean of the higher level dsitribution which they are drawn from, which in the end has been shown to improve predictive capacity. Hierarchical models are becoming corner stones in most cognitive science experiments [@bates_fitting_2015; @van_boekel_pool_2021; @gomes_should_2022].

Therefore to fairly test and compare the two internal validity metrics, the correlation coefficient and the ICC, each of the metrics were calculated from this hierarchical model. the Two ICC values were calculated as above now referred to as $ICC_1$ and $ICC_2$, referring to not including MSE and including it.

```{r figure 8, fig.width = 7.2, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 8. Parameter recovery for the three parameters of the psychometric function in the unconstrained space, using the hierarchical model.** Scatter plot of Simulated vs recovered parameter values, with error bars displaying the 95 % highest density interval for that parameter on that simulation. Text on each facet shows four metrics of internal model validity, correlation coefficient without and with accounting for estimation uncertainty, and the purposed ICC metric without and with including the mean sqaured error."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","parameter_recovery_v2_nested.PNG")), scale = 1)
```

As can be seen in figure 8, the hierarchical fit does improve the parameter recovery, both from a visual inspection (points falling closer to the identity line with less estimation uncertainty) and by comparing the correlation estimates between figure 7 and 8. This finding helps to explain why hierarchical models in general are preferred to single fit models as the partial pooling improves estimation of the parameters [@bates_fitting_2015; @van_boekel_pool_2021; @gomes_should_2022]. This is highlighted in figure 9 where estimation uncertainty, here the 95% credibility interval, of each parameter at each session is plotted as histograms for each parameter (facet) and colored by the type of fit, generally showing higher uncertainties in the single fit model compared to the hierarchical model. It should here be noted that a single simulation like this would not be enough to ensure that the parameters are nicely recovered as a good example of this is the lambda parameter. The pairwise scatter plot of the nested hierarchical model seems to suggest that this parameter is quite well recovered. However, if we back calculate a lambda value of -5 (on the unconstrained scale as depicted in figure 7 and 8) that would corresponds to a lapse rate of around 1.3%, which should be difficult if not impossible to accurately estimate when there are 100 trials for each subject, as most subjects would have very few, if any lapses. See supplementary note 2 for a deeper explanation on this. Turning the attention to the ICC values, it is observed that $ICC_1$ on each of the 3 parameters has an upper bound at the maximum value of one, which is confirmed looked at the scatter plot. This can be seen as all the session one estimates are hidden behind the session two estimates with only a few estimates deviating slightly. The $ICC_2$ estimate is crucially the lowest for all three parameters. Visual inspection of the pairwise scatter plot makes this clear as this metric is penalized for both the degree to which the points fall away from the identity line but also by the estimation uncertainty associated with these points. This therefore also explains why the alpha parameter is close to being asymptotic at 1, but with a little to be desired for simulated values between 0 and 10. 

In the next section it will be shown how these metrics, especially the $ICC_2$, is influenced by different factors and that by reducing estimation uncertainty it is possible to increase these. Four different different strategies will be introduced, adaptive optimization design, increasing the number of trials, assuming different mean group level slope values and lastly jointly modeling the binary responses with response times.

```{r figure 9, fig.width = 7.2, fig.height = 4, warning = F, message = F, echo = F, fig.cap = "**Figure 9 Estimation uncertainty for each parameter for both single and hierarchical fit models** Each panel represents one of the three parameters of the psychometric function with the estimated uncertainty (95% credibility interval) depicted as histograms. The color of the histogram shows whether the model was fit using the single fit or hierarchical model."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","Estimation_uncertainty_v1.PNG")), scale = 1)
```

