## *Importance of uncertainty minimization*

Throughout the thesis uncertainties, from measurements to estimations to the uncertainty of these estimations over time, has been investigated, through statistical and cognitive modeling. The section on measurement uncertainty was a brief overview highlighting ways in which computational resources can be used to account for these uncertainties. Next, the section about estimation uncertainty showed how different approaches can be utilized, from smart design of the experiment, to including additional information present, to decrease this kind of uncertainty. In the last section reliability of estimates were examined using experimental data and how the approach of adding addition information to the analysis, can increase the reliability of the test re-test reliability.


To fully appreciate and explore, how these uncertainties interact and their implications for hypothesis testing, the thesis will below conduct a power analysis, for the experiment analysed in the previous section. In this power analysis, measurement uncertainty is assumed to be negligible. This amounts to assuming that participant's heart rate is estimated with infinite precision, consistent with the previous analysis, as the authors of the experiment did not disclose the uncertainty in these estimates.


Furthermore, only the simplest form of the PF with 3 parameters is going to be analyzed, focusing only on a difference in thresholds. The power analysis is limited in scope, in order to fully capture the potential of this particular way of conducting a power analysis. The simplicity of the model, is for the power analysis to fully explore the effects of combinations of subjects and trials on statistical power, with the least amount of computational overhead, but see discussion and limitations for further elaboration of this. The following sections introduce power analyses, and how they are and can be contextualized.


# Power analysis

When researchers are interested in the parameter values of their models, they often seek interest in how they differ by some manipulation. This could for instance be a pharmacological intervention, or a difference between healthy controls and patient populations. In such a scenario a key question is, how many participants and/or trials do I need to reliably detect a particular size of effect, between the two conditions? These estimates of trials and participants can, in principle, be calculated a priori to conducting the experiment, given some assumptions. This type of a priori analyses thus tries to answer the question of, what is the probability that my results are going to be "significant", given some "real" underlying effect. Here "significant" refers to the standard frequentist approach of rejecting or failing to reject a null hypothesis, based on a significance level. 

Usually, this concept of hypothesis testing is illustrated in a 2 by 2 matrix, with the real latent effect being in one dimension, and the model results in the other dimension (see table 4). The probabilities of landing in either of the four categories are usually described as functions of our statistical significance threshold (alpha / p-value), and the statistical power of our model and test (1-$\beta$). This framing of power analyses, is thus to imply that results are significant, if the p-value is less than a particular value (5%), and that the probability to detect this effect, given that it is present, is another arbitrary value, typically set at 80% [@chen_roles_2023; @dumas-mallet_low_2017].

\newpage

```{r table 4, fig.width = 7, fig.height = 7, warning = F, message = F, echo = F}

table4 = read.csv(here::here("tables","table4.csv")) %>% mutate(X = NULL)

names(table4) = c(" ", "Reality (effect)","Reality (no effect)")
# Create a flextable
table4 <- flextable(table4) %>% width(j = 1, width = 2) %>% width(j = 2:3, width = 1.3)


table4 = set_caption(table4,
  caption ="Table 4. 2 by 2 confusion matrix of whether the is an underlying effect (Reality) and whether a model is able to correctly identify this effect or not whether its present or not.")

table4
```
## *Power analysis in practice*

Our models in Cognitive Science will reject and fail to reject, different effect sizes at different rates, based on their magnitude, as well as the amount of data i.e. the number of subjects and/or number of trials. Increasing the number of subjects and/or trials serve to reduce the uncertainty in the estimated effects and thereby increasing the probability of detecting said effect.


With this understanding, the commonly depicted table above (table 4) is somewhat misleading, as the dimension of "reality" is a continuous variable of the size of the effect. Our models then have a specific probability of rejecting a hypothesis, based on the particular effect size observed at a particular set of subjects, trials and significance level. For example, consider a researcher wanting to detect an effect of gender on height, in the human population. Assuming an underlying effect, the researchers observes X females and Y males and conducts a statistical analysis to determine whether he can reject the null hypothesis (there are no differences in height, in the two genders). Compare the above hypothesis to the hypothesis that there is an effect of age (late adolescents vs. adults) on height. The gender difference is generally larger than the age difference, and therefore with all else being equal (trials, subjects, statistical model etc.) this difference will be easier to detect compared to the difference in height, based on age. Therefore, in conducting power analyses observed effect sizes are simulated, (effect sizes in the data that is observed) with differing amounts of trials and subjects. The ability of the statistical model to reject these simulated experiments, is then accessed. Usually, this involves counting the number of times the model achieves "significant" results, compared to non-significant results, which then represents the power of the model, at that number of trials, subjects and observed effect size. This approach accurately captures how we expect the model to behave when we fit the data to the model after obtaining it. It tells us if we observe a particular effect size, we will with a specific probability be able to reject the null hypothesis. The utility of such analysis therefore lies in being able to examine how many subjects and/or trials are needed, to obtain a statistical power of usually 80%, given that a particular effect size in the population is present. The assumed effect size in the population might be informed by previous studies and/or meta-analyses in the field. Additional assumptions are then needed, in order to approximate the distribution of effect sizes, as these statistical metrics also have uncertainty associated with them.


The power simulations conducted in this thesis will focus on a repeated measures design investigating a threshold difference, due to some intervention. Subjects, trials, and effect sizes in a variety of combinations are therefore simulated (figure 14). The choice of effect size metric was the cohens' $d_rm$, as seen in the formula below. This particular effect size is suitable for repeated measures design because it accounts for the correlation between the two sessions of each participant, i.e. the test retest reliability of the metric investigated.

The simulation process followed these steps. First, a set of agents were simulated from a multivariate normal distribution with two sessions, the parameters were informed by the group level parameters of the binary nested hierarchical model, presented in section about experimental data, (see supplementary table 2, for the exact values for each parameter). Next, the thresholds of the second session for the agents, were increased by a random variable, drawn from the difference distribution. This difference distribution was calculated based on the two equations presented below, where the second session variance was defined as 1.5 times the variance of the first session. To ensure a particular observed effect size, this process was repeated until an observed effect size of the desired value, was obtained within $\pm$ 0.01. This step of re-sampling for a particular effect size was mainly for visualization purposes (see Figure 14 and the accompanying text). After simulating the particular parameter values of each agent at each session, the agents was put through the pathfinder algorithm to obtain their trial-by-trial stimulus values. The complete trial-by-trial data-set was then fitted using a single layered hierarchical model, where the threshold was parameterized as a linear combination of an intercept, and a dummy coding of session with a difference parameter (supplementary note 6, for the full model description).

$$
\mu_{\delta} = d_{rm} * \frac{\sqrt{Var_1 + Var_2 - 2 * \sigma_1* \sigma_2 * \rho}}{\sqrt{2 * (1-\rho)}}
$$

$$
\sigma_{dif} = \sqrt{Var_1 + Var_2 - 2 * \sigma_1* \sigma_2 * \rho}
$$

Mean and standard deviation of the difference distribution between the two sessions. Where $Var_1$ is the variance of session 1 and $Var_2$ is the variance of session 2. $\rho$ is the correlation coefficient between the two sessions and $\mu_{\delta}$ is the mean of the difference distribution with $d_{rm}$ being the standardized effect size between the two sessions.




## *Power analysis results*

Understanding of the goal of conducting a power analysis, it can be difficult to chose the number of combinations of trials, subjects and observed effect sizes to explore. The space of these combinations is theoretically infinite, and practically also quite large. Therefore, conducting a complete power analysis with all combinations of trials, subject and observed effects sizes for a single model is unfeasible. However, this thesis will demonstrate that the variation in how well the PF rejects the null hypothesis, given subject and trial combinations is stable over observed effect sizes. This stability allows for possible ways to give good predictions of power, without needing to simulate the exact number of participants or trials. This procedure is therefore about simulating a set of trials, subjects and observed effect sizes and then extrapolate from these simulations. 

For the current power analysis, a result is considered significant if less than 5% of the posterior difference distribution of the threshold crosses 0, analogous to setting an alpha value of 5% in a frequentist power analysis. Furthermore, 100 simulations are going to be run for each subject, trials observed effect size combination to ensure reasonable estimates on the probability of rejecting the null hypothesis. To effectively display the raw results of the power analysis and facilitate visual comparison of the effects of trials and subjects, the beta distribution is going to be used. A beta distribution is going to be used to aggregate and propagate the uncertainty the 100 simulations for each effect size. this is done by utilizing that the beta distribution can be parameterized with one parameter counting the number of times an event has happened, while the other parameter counting how many times this event did not happen. 


Starting with what is analogous to a uniform prior, on the probability of rejecting the null hypothesis (Beta(1,1)), it is possible to  update this probability density function with the amount of significant or non-significant results in the 100 simulations. This updating process will result in a probability density function, that contains the information in the 100 binary points (i.e. significant or not simulations). Figure 14, shows each trial/subject combination with points representing this prior uniform beta distribution being updated by the 100 data-points.


Several important observations are worth noting when viewing Figure 14. The shape of the points, for each trial/subject combination, very closely resembles a psychometric function, where both subjects and trials influence the steepness and the location of the function. This suggests that increasing the number of subjects, and trials to a lesser extent, has two important features. Firstly, it shifts the points towards higher power, with lower effect sizes. Additionally, it seems to increase sensitivity to the observed effect size, as evident by the slope of the curve getting steeper, with higher number of subjects. Investigating the number of trial's influence on power there there seems to be large diminishing returns. This means that the effectiveness of increasing the number of trials, to achieve higher power, is highly dependent on the number of trials itself. In practice and as shown in figure 14, increasing trials from 10 makes a big difference in the shape of the function, but the difference between  high and very high i.e. 100 to 150 trials, has a lesser effect. The tendency of the function to be less affected by ever increasing trials is also present for the number of subjects, to a lesser extent.

This observation aligns with the expectation when considering the function at its extremes, in terms of trials and subjects. When subject and trial numbers approach infinity, one would expect, assuming the model has been shown to become increasingly better with increasing trials (like with the ICC metric presented previously), that the model would be able to detect even the smallest difference in groups. This would essentially entail that the function would consistently be at y = 1, with x approaching 0 from the positive direction, and then jump to (0,0) in the (observed effect size, power) curve, as no difference would entail no power. Conversely, when no subjects or trials are present the curve should approach a flat line at y = 0, entailing no power for any amount of effect size. Essentially, the function would asympote to a step function in the limit when x goes to 0, and subjects and trials goes to infinity. This can mathematically be written as:

$$
\lim_{{(s,t)\to\infty}} \left(\Psi(x, \alpha, \beta, s, t) = \begin{cases} 0 & \text{if } x \leq 0 \\ 1 & \text{if } x > 0 \end{cases}\right)
$$

where s and t are the number of subjects and trials respectively, and x, $\alpha$ and $\beta$ are the observed effectsize, the threshold of the psychometric function and the slope of the psychometric function, respectively. 

These observations will be used in the next section in order to extrapolate the results from figure 14. This will enable the possibility of constructing a model that maps trials, subjects and effect sizes to statistical power.


```{r Figure 14, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "**Figure 14, depicts power as a function of observed effectsizes in different combinations of trials and subjects.** The x-axis represents the observed (& simulated) effect size with the y-axis depecting the statistical power of the model, i.e., the proportion of rejected null hypothesis to failed rejections."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","poweranalysis_scatter.PNG")), scale = 1)
```

## *Modeling of power analysis*

Using the information from above, one needs to investigate the latent psychometric function describing the relationship between subjects, trials and effect sizes. Ideally, this psychometric function enforces the curve going through the origin, as an observed effect size of 0 should always entail no power. Next the parameters of this psychometric functions, i.e. the threshold and slope, need to be parameterized by the number of trials and subjects. This parmeterization of trials and subjects should ensure that the function moves towards a step function right as x becomes greater than 0, when subjects and trials approach infinity. 

Before fitting the general case, used for extrapolation, but also ensuring that a psychometric function is well fitting function to the problem, each set of trials and subject combinations are fit independently to the parameters of the psychometric function. This involves estimating a threshold and slope of the psychometric function for each trial and subject combination. This steps makes it possible to ensure that fitted functions pass through the points, depicted in figure 14, which will increase confidence in the following type of modeling.

Several types of psychometric functions might be used for this type of analysis, were the ultimate goal is out of sample predictability and/or extrapolation. This would entail that the best model could be selected based on leave one out cross validation. The ideal model is the model that can best describe new data, as we want to use the function for prediction on not already simulated data. This is because the overall goal with this power analysis is to use the quite sparsely simulated space of trials, subjects and effect sizes, depicted in figure 14, to inform a model that can predict outside the realms, which it has been tested on. Therefore, these models were compared using the Pareto smoothed importance sampling leave one out cross validation [@yao_using_2018; @vehtari_practical_2017; @vehtari_pareto_2024]. 


Three types of psychometric functions were fit, the cumulative normal, the cumulative logistic and the cumulative Weibull function. The main differences between the normal and logistic function is that the logistic function has heavier tails than the normal allowing for more disperse observations. The difference between the Weibull and the two other functions is that the Weibull function is forced through the origin, resulting in a distinct shape compared to the other two functions. 


The choice of the cumulative normal or logistic function does not necessarily violate the assumptions laid out above. This is due to the way that the parameters are going to be dependent on the trials and subjects. This can be understood if one considers an asymptote at 0, for the slope and threshold (i.e. a step function also for the cummulative normal and logistic function), when trials and subjects move to infinity. This exactly aligns with the observation from above, that the psychometric function approaches a step-function (as the slope gets closer to 0) and that the location of this step function approaches x = 0, but never reaches it with increasing trials and subjects. 

The results of the preliminary independent analysis on trials and subjects can be seen in figure 15, where the independently fit logistic psychometric functions are overlaid on the observed data-points from figure 14. The figure highlights a good fit, for most of the trials and subject combinations (i.e. functions passing through the points).

```{r Figure 15, fig.width = 6, fig.height = 6, warning = F, message = F, echo = F, fig.cap = "**Figure 15; depicts power as a function of observed effectsizes in different combinations of trials and subjects.** Lines and shaded area represents the marginial effects and 95 % credibility interval of the independently fit logistic psychometric function to each trial by subject combination. "}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","power_individualfits.PNG")), scale = 1)

```

## *Continuous mapping of the power analysis*

Moving to the continuous mapping of subjects and trials, to the psychometric function's parameters. This mapping needs to be defined as a function that relates subjects and trials, to the slope and threshold of the psychometric function. As argued above, the the steepness increases, and the threshold moves to the left with increasing trials and subjects following a pattern of diminishing returns. A first choice of this mapping function could therefore be to model the two parameters as exponentially decreasing by trials, subjects, together with their interaction. An exponentially decreasing function in the complete general case would mean the following relationship.

$$
\Theta = \beta_0 * exp(-\beta * X) + \alpha_{asym}
$$

Here $\Theta$ represents the parameters of the psychometric function, where $\alpha_{asym}$ denotes the parameter value when the number of trials and subjects approach infinity. $\beta$ is vector of parameters determining the steepness of the exponential decrease from the co-variates in the matrix X, here trials subjects and their interaction. The parameter $\beta_0$ serve, together with $\alpha_{asym}$, as the value of the parameter when trials and subjects are 0. 

Another formulation of the dependency might be a power law equation, as shown below.

$$
\Theta = \beta_0 * X^{\beta}
$$

Both approaches can produce the observed behavior, and difference in the two fomulations depends on the underlying relationship between the parameters and the the matrix X (i.e., trials and subjects and perhaps their interaction). The exponential equation assumes that as trials and subjects increase by a fixed amount, the parameters will decrease by a percentage. The power law on the other hand assumes that as trials and subjects increase by a percentage, the parameters will decrease by a percentage.

Several ways of investigating which of these two approaches results in the better fit. Firstly plotting the parameters of the independent fits (figure 15) vs trials and/or subjects, in two different coordinate systems, either (log(y),x) or (log(y),log(x)). Which of these coordinate systems produces the best-looking linear line, would be the best candidate. Supplementary Figure 12 displays the three function's parameters fitted independently on each of the two coordinate scales. Using this approach no obvious differences were found.



Another approach invovles fitting both types of models, and then compare them on leave one out cross validation as described above. However, this approach revealed problems with 15, 25 and 3 % of observations for the normal, Weibull and logistic functions, respectively. These procentages were accessed using the pareto k diagnostic value, which was above 1 for these percentages of data-points. Essentially, this renders the comparison meaningless [@vehtari_pareto_2024]. 

Investigating these function, the logistic cumulative function produced the least amount of problems, with pareto k values. This function was therefore used, when fitting trials and subjects as continuously informing the parameters of the latent psychometric function. The first model, was the exponentially decreasing function. Four other models were fit, with different parameterizing of the power law equation. These four models had different approaches to modeling trials and subjects and their interaction, as there is no straightforward way of combining X and β. The first
power law was an additive model, with the following parameterization

$$
\beta_0 \cdot X^\beta = \beta_{01} + s^{\beta_1} + t^{\beta_2} + (t \cdot s)^{\beta_3}
$$

The second power law, with a combination of additive and multiplicative operations: 

$$
\beta_0 \cdot X^\beta = \beta_{01} * (s^{\beta_1} + t^{\beta_2} + (t \cdot s)^{\beta_3})
$$ 
The third power law, was a multiplicative model without an interaction.

$$
\beta_0 \cdot X^\beta = \beta_{01} \cdot s^{\beta_1} \cdot t^{\beta_2}
$$

The last power law, was the multiplicative model with an interaction, but defined as the sum of subjects and trials as the normal interaction of multiplying trials and subjects would lead to a similar model, of the model without an interaction.

$$
\beta_0 \cdot X^\beta = \beta_{01} \cdot s^{\beta_1} \cdot t^{\beta_2}\cdot (t+s)^{\beta_3}
$$

Comparing these five models, with leave one out cross validation showed that the best model was the last power law model, but closely followed by the second power law model, which can be seen in table 5. Importantly for these reported models, the diagnostic values were all below 0.7.

```{r table 5, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "Table 5"}

table5 = read.csv(here::here("tables","table5.csv")) %>% mutate(X = NULL)

table5[,2:3] = round(table5[,2:3],2)

table5$elpd_ratio = round(-table5$elpd_diff/table5$se_diff,2) 

table5 <- flextable(table5) %>% width(j = 1:4, width = 1.5)



table5 = set_caption(table5,
  caption ="Table 5. Model comparison of the power analyis models, using Pareto smoothed importance sampling leave one out cross validation. Expected log predictive density (elpd) difference and standard error between models is depicted in the second and third column and the absolute ratio between these in the fourth column. The Higher the elpd-ratio the bigger the scaled difference (scaled by the uncertainty) between the models and the more confident one might be that one model outperforms the other.")

table5
```

Table 5 indicates that as the trials and subjects increase by a percentage, the parameters of the psychometric decrease by a percentage, as the top two models, which are both variations of the power law. To verify that the tested models capture the underlying simulation, Figure 16 displays the winning model superimposed on the data, with 95 credibility intervals of the mean. As seen, this closely resembles the individual independent fits, with the most drastic deviation in the 5 subjects and 10 trials condition.

```{r Figure 16, fig.width = 6, fig.height = 6, warning = F, message = F, echo = F, fig.cap = "Figure 16; depicts power as a function of observed effectsizes in different combinations of trials and subjects. With lines being the dependently fit logistic psychometric functions to each trial by subject combination."}


cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","poweranalysis_powerfit.PNG")), scale = 1)

```

The marginal posterior distributions of the parameters of the winning model are displayed below in figure 17.

```{r Figure 17, fig.width = 7.2, fig.height = 6, warning = F, message = F, echo = F, fig.cap = "Figure 17. Marginal posterior distributions for the winning model's parameters"}

# cowplot::ggdraw() +
#     cowplot::draw_image(magick::image_read(here::here("Figures","marginals_histogram.PNG")), scale = 1)
readRDS(here::here("Figures","marginals_histogram.RDS"))


```

Meaning that the best the underlying function transforming trials, subjects and observed effect sizes into probabilities of rejecting the null hypothesis of no difference in threshold, follows:

$$
\Psi(d_{\text{obs}}, \alpha, \beta \mid t, s) = \frac{1}{1 + \exp\left(-\frac{1}{\beta(t, s)} \cdot (d_{\text{obs}} - \alpha(t, s))\right)}
$$ 

Where

$$
\beta(t, s) = \beta_I \cdot s^{\beta_{1}} \cdot t^{\beta_{2}}\cdot (t+s)^{\beta_{3}}
$$
and

$$
\alpha(t, s) = \alpha_I \cdot s^{\alpha_{1}} \cdot t^{\alpha_{2}}\cdot (t+s)^{\alpha_{3}}
$$

Where each of these parameters are given by the distributions depicted above, in Figure 17.

## *Utility of the power analysis*

As alluded to in the inital section of the power analysis, the work presented here should be able to help an independent researchers. It should do this by helping to determine the probability of rejecting a particular observed effect size, given trials and subjects, even outside the realm of simulations presented here. However, for a researcher to utilize this function to calculate the probability of rejecting a null hypothesis, given a particular effect size in the population, further assumptions needs to be made. This is because as the effect size when conducting an experiment is not a fixed quantity.

In practice, this means that when conducting an experiment, we observe an effect size that is assumed to be drawn from a latent effect size distribution in the population. Mathematically, this means that the observed effect size that in an experiment is a random variable. The mean and standard deviation of this random variable is given analytically by Cohen, but could also be derived from bootstrapping [@goulet-pelletier_review_2018; @lakens_calculating_2013; @hedges_statistical_2014]. Below are the equations for the mean and standard deviation of the effect size measure used in the power analysis.

$$
\mu_{d_{rm}} = \mu_{\delta}  * \frac{\sqrt{2 * (1-\rho)}}{\sqrt{Var_1 + Var_2 - 2 * \sigma_1* \sigma_2 * \rho}}
$$

$$
\sigma_{d_{\text{rm}}} = \sqrt{\frac{1}{n} + \frac{\mu_{d_{rm}}^2}{2 \cdot n}}
$$

The equation for the mean effect size $\mu_{d_{rm}}$ is mathematically identical to the definition shown in the "Power analysis" section. The standard deviation of the random variable $\sigma_{d_{\text{rm}}}$, is defined as a function of the number of subjects n and the size of the effect itself $\mu_{d_{rm}}$. 
Assuming that the effect size is normally distributed:

$$
d_{obs} \sim N(μ_{d_{rm}} ,\sigma_{d_{rm}})
$$

The probability of rejecting (R) this sampled effect size ($d_{obs}$) is given by the function that was obtained above.

$$
P(R \mid d_{obs}) = \Psi(d_{\text{obs}}, \alpha, \beta, t, s)
$$ 

Ideally the probability of observing a particular effect size AND reject the null hypothesis given this observed effect size is seeked. Probability theory, particularly conditional probabilities, gives us the relationship between these quantities.

$$
P(R \mid d_{obs}) = \frac{P(R \cap d_{\text{obs}})}{P(d_{\text{obs}})}
$$ 

Here $P(R \cap d_{\text{obs}})$ represents the probability that we are interested in, i.e., rejecting AND observing a particular effect size.

$$
P(R \cap d_{\text{obs}}) = P(R \mid d_{obs}) \cdot {P(d_{\text{obs}})}
$$

Integrating over all possible values of the effect size is necessary to integrating out the effect size, i.e., marginalizing.

$$
P(R) = \int_{-\infty}^{\infty} P(R \mid d_{\text{obs}}) \cdot P(d_{\text{obs}}) \, d({d_{\text{obs}}})
$$

Which becomes: 

$$
P(R) = \int_{-\infty}^{\infty} \Psi(d_{\text{obs}}, \alpha, \beta, t, s) \cdot N(μ_{d_{rm}} ,\sigma_{d_{rm}}) \, d({d_{\text{obs}}})
$$

Instead of trying to analytically solve this integral, one can leverage computational resources to approximate it. This can be done by taking draws of the normal distribution of the observed effect size, and then applying them through $\Psi(d_{\text{obs}}, \alpha, \beta, t, s)$. The result of such calculation will give draws from a probability distribution of rejecting the null hypothesis, i.e., P(R). The last step is to calculate the proportion of rejected null hypotheses (p < 0.05), to the total number of draws. Which would entail the power of the study, assuming the mean difference and variance in the two sessions.

## *Sampling varability of the effect size.*

The above high-level explanation of calculating power for an experiment might be quite difficult to understand, and therefore implement for independent researchers. To make this more accessible, I will demonstrate below how this can be done using the concepts described above. The next sections will therefore, provide a practical understanding of the parts that should go into a power analysis and how different factors will influence power.

Firstly, I'll examine and show the influence, and need, for the sampling distribution of the observed effect sizes. In order to demonstrate this, it is assumed that the group mean difference of the threshold in the psychometric function is -5, and the variance in the second session is 1.5 times the variance of the first session. These assumptions entail that the intervention increases the variation in the threshold, but that there is a clear effect of the intervention of the threshold. The assumptions for the choice of mean difference and difference in variance can be visualized by repeated sampling from a multivariate normal distribution with the following parameterization:

$$
\begin{equation} 
  \begin{pmatrix}
  x_i \\
  y_i 
  \end{pmatrix} \sim \mathcal{N}\left(
  \begin{pmatrix}
  \mu_1 \\
  \mu_2 
  \end{pmatrix}, \begin{bmatrix}
  \sigma_1^2 & \sigma_1 \cdot \sigma_2 \cdot \rho_{12} \\
  \sigma_1 \cdot \sigma_2 \cdot \rho_{21} & \sigma_2^2 
  \end{bmatrix} 
  \right)
\end{equation} 
$$

Here $\mu_1$ and $\sigma_1$ are given by the re-analysis of the experimental data and were -8, 8 respectively. Given the assumptions above, $\mu_2$ and $\sigma_2$ become -3, 10. The number of subjects is then varied by only drawing a particular number of random variables from this multivariate normal ($s \in (10, 40, 70, 100)$). To also investigate the effect of the correlation coefficient $\rho$ on the distribution of effect sizes i.e. $p(d_{obs})$, this parameter is also varied ($\rho \in (0,0.3,0.6,0.9)$). 

The results of this simulation can be seen in figure 19. Here it shown that both the sample size i.e. subjects, but also the correlation coefficient between the sessions are important for the variances of the observed effect size.

```{r Figure 18, fig.width = 7, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "Figure 18. Sampling distributions of effectsizes across subjects (facets) and session by session correlations (colors)"}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","histogram_observedeffectsize.PNG")), scale = 1)
```

Now, its possible to visualize how these observed effect size distributions fit into the probability of rejecting the null hypothesis, i.e., $\Psi(d_{\text{obs}}, \alpha, \beta, t, s)$. Note, the observed effect size distributions above are not dependent on the number of trials, in the experiment. The function derived above from the continuous power analysis incorporates this information, together with other factors, that might change the power of the experiment, given an observed effectsize distribution. As shown above, the implications of the function $\Psi(d_{\text{obs}}, \alpha, \beta, t, s)$ can be visualized as psychometric functions in a ($d_{\text{obs}}$ , $\Psi(d_{\text{obs}}, \alpha, \beta, t, s)$) coordinate system with trials and subjects being fixed. Another more informative way to investigate varying number trials and subjects, is to visualize these implications in a 3-dimensional grid of (Subjects , $d_{\text{obs}}$ , $\Psi(d_{\text{obs}}, \alpha, \beta, t, s)$) with facets being a particular set of trials. This visualization can also serve the purpose of projecting the above distributions (figure 18), unto the space of $\Psi(d_{\text{obs}}, \alpha, \beta, t, s)$. 

Figure 19 displays the projection of the histograms from Figure 18 as ellipse, where the vertical width of the ellipse (the major axis) is given by the 95% Highest density interval of the histograms and the horizontal width (the minor axis) is for visualization purposes. In Figure 19, it is shown how the correlation coefficient, the number of subjects as well as the number of trials, affect the power (the background color), given an observed effect size.

```{r Figure19, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "Figure 19. Visualization of how the power of a particular study, where the mean difference between threshold is assumed to be 5 and the variance of the difference being 12. It can be seen that the power of the experiment (background), is informed by the session by session correlation of the parameters, displayed as the color of the sampling distribution of the observed effect sizes (ellipses). Furthermore, the Figure shows how the number of trials and subjects include the power of the study. Subjects increase the power of the study while also decreasing the sampling varability (width of ellipses), whereas trials only increase the power of the study. Trials effect on power can be seen by investigating the upper left cornor of the plots in the four facets, here one can observe that the background color turns more yellow as trials increase."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","power_area_with_ellipses.PNG")), scale = 1)

```

## *Practical implementation of the power analysis*

Now, considering a pratical example of a researcher wanting to conduct a power analysis, utilizing the the simulation and modeling from above. Two assumptions have been made: either a mean effect size or a mean difference of the intervention is assumed, and the variance introduced by the intervention. Below, I investigate a mean difference of the intervention of 4 in threshold, and that the intervention does not increase variability, i.e., the variance in both groups are assumed equal. To fully appreciate the power of this approach, one could even imagine sampling these values as random variables, and not as point estimates (Not done here). Using the effect size equations above, it is possible to derive the mean difference and therefore simulate observed effect sizes which are then put into $\Psi(d_{\text{obs}}, \alpha, \beta, t, s)$ and the probability of rejecting that draw is calculated. This process is repeated over the 4000 draws of the posterior distribution, of the parameters of $\Psi(d_{\text{obs}}, \alpha, \beta, t, s)$. Lastly, calculating the ratio of rejected to failed rejected null hypotheses gives the estimate of statistical power. In the case of not including the sampling varability (prior probability) of the observed effect size, the effect size estimate is just repeatedly entered as 0.5 (calculated from the assumptions of a mean difference of 4 and no varability change in the intervention). This idea of not including the sampling varability, is equivalent to investigating a minimum effect size of 0.5 being deeemed the minimum effect size of interest.


Figure 20 dipicts a grid of subjects X trial that span the space of power to reject the null hypothesis, here the the observed effect size has been "integrated" out. This integration was done with either a constant of 0.5 (left column) or with a normal distribution with a mean of 0.5 and a variance of $\frac{1}{n} + \frac{0.5^2}{2 \cdot n}$, with n being the number of subjects. As a reference frame the red dashed line in figure 20, at subjects = 25, depicts the results from plugging the same assumptions, here $\mu_1 = -8$ $\sigma_1 = 8$ $\mu_2 = -4$ and $\sigma_2=8$ and $\rho = 0.54$, into the statistical software tool G\*power [@faul_gpower_2007].

```{r Figure20, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "Figure 20. Displays the grid of subjects X trials to obtain a particular level of statistical power (background) given a group level difference of 4 and equal variance. Left column displays an analysis omitting sampling varability (a minimum effect interesting effect size calculation), with the right column including sampling varability."}


cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","power_sampling_varability_and_gpower.PNG")), scale = 1)

```

\newpage
