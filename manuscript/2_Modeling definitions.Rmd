# Modeling definitions and validation.

## **Modeling definitions**

The rest of the thesis will revolve around refining, testing, and designing models of cognition. Cognitive modelling will be deployed for this purpose, serving as an intermediate level in a hierarchy of computational models on top, and statistical models in the bottom. These types of models can be thought to differ in their flexibility, assumptions, and scope of investigation. It should be noted that all types of models have many things in common, such as being mathematical representations of a data generating process. This makes these definitions operational and should be thought of as having fuzzy boundaries [@durstewitz_computational_2016].

*Statistical models* are the models primarily used in medical and social sciences. 


These models mostly consist of linear and generalized linear (mixed) models. 

https://www.researchgate.net/publication/374834913_Statistical_Modeling_in_Healthcare_Shaping_the_Future_of_Medical_Research_and_Healthcare_Delivery
https://www.emerald.com/insight/content/doi/10.1108/JHASS-08-2019-0038/full/html


What these models have in common is that they are linear combinations of independent variables which are sometimes transformed (making them generalized) to a particular domain. The mathematical representation of such models are as follows:

$$
F(y)=\beta·X+\epsilon
$$ 
Where y is a vector of dependent variables of N elements, F is a link function that maps the conditional mean unto a particular space, common link function are the logit and log transformations which maps unto domains of [0 ; 1] and [0 ; ∞] respectively, which makes predictions on probabilities and strictly positive values like reaction times possible. $\beta$ is a vector of regression coefficients of P predictors which gets estimated, X Is a matrix of independent variables of size [N, P]. Lastly $\epsilon$ is a vector of N elements containing the errors of the model predictions on the dependent variables. The benefit of these statistical or regression models is that maximum likelihood estimators are available meaning that parameters estimates can be calculated using a frequentists statistical framework, making the estimation process fast and efficient. The downfall however is that they put quite big constraints on the types of models that can be fit, i.e. there must be a linear mapping between all independent variable and the dependent variable in a domain that can be mapped with a link function. This constraint will in many instances make theories hard or impossible to test as human behavior and cognition is nonlinear in different ways [@ivanova_beyond_2022]. It should be noted that the correlation coefficient examined in the previous section, can be thought of as a special case of this linear model where $\beta$ is a single value (the correlation coefficient) and y and x are z-transformed vectors, see supplementary figure 2.

*Cognitive models* are models that are meant to resemble the generative processes of human behavior more closely. These models are generally more theoretically driven as the constraint of linear combinations is avoided, by employing different optimization schemes that sometimes use sampling algorithms to obtain the results. In many cases cognitive models are estimated in a Bayesian framework due to the flexibility with which models can be specified. The main advantage of these models, in this context, is the added freedom in model specification.

*Computational models* are the upper most level of the hierarchy which here will be used to refer to the generalization of cognitive models to other scientific domains, such as physics, biology chemistry etc. These models are outside the scope of this thesis.

These three categories are arbitrary, and many methods and models will fall between them, with this vague definition. However many times these arbitrary definitions do add value in communicating what general framework we are working in and thereby what methods are used. The next section will describe a particular cognitive model which will be the focal point for the rest of the thesis.

\newpage

## *The Psychometric function*

The thesis will explore the psychometric function (PF), as this type of function has been a stable corner stone in the cognitive science literature across many different sub fields [@courtin_spatial_2023; @bahrami_what_2012; @coates_changes_2014; @ma_memorability_2024; @gold_how_2013]. The psychometric function is a continuous function that maps real inputs (here called intensity values) onto probabilities, i.e. the domain $[-\infty ; \infty]$ with the range of $[0 ; 1]$. In most cases the PF used is like a logistic regression in statistical modeling and is commonly used in perceptual research where the inputs are stimulus intensities, and the probabilities are then converted into binary forced choices through a Bernoulli or binomial distribution. The mapping of inputs to probabilities is usually done through a cumulative density function such as the cumulative logistic or normal distribution, which amounts to conducting a logistic or probit regression in the statistical framework. The main difference between the statistical and cognitive framework of the PF is the number of parameters. The least number of parameters used to describe the PF is 2, the threshold ($\alpha$) and the slope ($\beta$).

These two parameters describe the center of the curve, with $\alpha$ being the intensity of the stimulus at probability 0.5 (i.e. the x-value at y = 0.5) and $\beta$ being the steepness of the function around the threshold. In the cognitive modeling framework one or two more parameters are typically introduced the lapse ($\lambda$) and guess rates ($\gamma$). These two parameters together handle the tails (i.e. the far ends) of the psychometric function and essentially makes the probability in the two ends of the psychometric no deterministic i.e. the upper and lower bounds become ($\lambda$) and ($\gamma$) instead of 0 and 1, see figure 2. These parameters help with fitting the PF to data where attentional slips or wrong button presses happen and it can be shown that including these parameters will greatly improve the estimation of the slope of the PF if lapses and or guesses are present in the data [@wichmann_psychometric_2001]. Figure 2 depicts how all these parameters change the shape of the PF. For the sake of this thesis, the cumulative normal distribution is used to map stimulus values to probabilities only with a single lapse rate. This single lapse rate will govern the distance between the upper and lower bound, essentially making it equally likely to have an erroneous response for high and low stimulus values. This mathematical formulation of the function is as follows:

$$
p(x | \alpha, \beta, \lambda) = \lambda + (1-2 * \lambda) * (0.5+0.5 * erf{(\frac{x-\alpha}{\beta * \sqrt{2}})})    
$$

<!-- this plot should be of above function-->

```{r figure2, fig.width = 7, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 2 Psychometric parameters.** Displays how the parameters alpha ($\\alpha$), beta ($\\beta$) and lambda ($\\lambda$) of the psychometric fucntion changes its shape. Columns display how the beta parameters changes the slope of the function. Rows show how alpha changes the location of the center of the function changes. Lastly, colors in the plot depict how lambda changes the asympotes in extreme stimulus (x) values."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_4_psychometric_parameters.png")), scale = 1)

```

## *Model validation.*

In the same vein of validating the bootstrapping approach with the analytical solution in the previous section about measurement uncertainty, cognitive models themselves are many times validated to ensure that at least in principle the parameters of the model can be estimated with increasing accuracy with increasing number of trials. This section will highlight some of the emerging ways in which computational models in the cognitive science literature are being tested and validated and takes offset in the seminal paper from Wilson and Collins [@wilson_ten_2019] describing 10 simple rules of computational modeling, which is commonly cited when validation of computational models are described [@hess_bayesian_2024]. There are at least three main challenges when building and validation cognitive models, which are particularly relevant when writing novel models. How do we know that our models do what we think they do (identifiability). How do we know that they accurately estimate the parameters of interest (Internal validity)? And lastly how do we know that we can distinguish between competing models (external validity). The last challenge is beyond the scope of the current thesis and is well covered elsewhere [@wilson_ten_2019]. The answer to the first two challenges must be found in simulations, especially when our models become more and more complex and analytical solutions are sparse or non existent. 



This simulation practice revolves around selecting an appropriate range of parameter values and using these to simulate data from our models and then refitting the data to then see how well the model approximates the simulated parameter values. It should therefore come as no surprise that ensuring that in these simulations, we would like our models to perform well, such that we can have faith in them when the real underlying process is unknown, i.e. analyzing real experimental data. An appropriate range of parameter values for a particular model can be difficult to select as this is exactly the problem of identifiability. However, several lines of information can help gauge this. Firstly, looking at mathematical constraints of the model formulations can reduce the possible ranges of parameter values. For the case of the PF this amounts to ensuring that the slope is strictly positive as this ensures that increasing levels of stimuli will produce greater probabilities of responding 1, but also ensure that the standard deviation of the underlying probability density function is strictly positive. The lapse rate of the PF will be constrained between 0 and 0.5 again to ensure a particular the shape of the PF. lapse rates below 0 and above 1 will produce probability values outside the [0; 1] range and values above 0.5 will flip the shape of the PF, as negative slope values will. Not constraining the PF in this way could lead to two distinct solutions to a given problem as negative slope values and lapse rates above 0.5 would be able to produce the same mathematical transformation of stimulus values to probabilities, making the solution non unique. From a more theoretical level an appropriate range of parameter values can be narrowed down by looking at the function of interest and investigating whether the observed behavior (given the simulated parameter values) is physically or biologically plausible. For the PF we might expect a few of our participants to not be particularly interested in the task and therefore just respond at random, which would amount to having a high lapse rate i.e. above 0.2 (amounting to 20% lapse responses) or really shallow slopes, however this behavior is quite unlikely and expecting only few lapses in the experiment, given that it is conducted in a quiet environment is likely. Lastly using empirical knowledge from the literature at large helps narrow the parameter space further. For the sake of argument, one might investigate the detection threshold for cold stimulation on the skin. Just given this information alone we can narrow down the threshold for cold detection to being below the skin temperature of around 30-34 degrees$^\circ$ [@courtin_spatial_2023] and above the absolute zero of -273 degrees. However knowledge from the scientific literature would suggest that thresholds between 28 and 33$^\circ$ would capture most of the population [@lithfous_accurate_2020]. These same arguments would also apply for the slope. This practice of investigating the assumptions of the simulated parameter values is closely related to prior predictive checks in Bayesian workflows. Prior predictive checks serve as a check of the model without having seen any data. This check revolves around simulating data from just the priors of the model and then investigating whether these conform with what is physically and theoretically plausible. Additionally, it also serves as a tool to ensure that the model can capture the behavior that is expected from the experiment [@kruschke_bayesian_2021]. 



The next challenge is about internal validity, i.e. can our model estimate the parameter values used to simulate the data on which the model estimates the parameter values. To test and validate our models in this regard, we simulate data from pre-specified parameter values, which have been deemed to be appropriate, using the first step described above. We then fit our models with this simulated data and investigate how well the model can estimate the latent simulated parameters (i.e. those that produced the data). This exercise of simulating behavior and then re-estimating the parameter values from the simulated behavior, is commonly known as parameter recovery in the cognitive modeling framework. Generally, if this procedure succeeds, then the parameters are said to be recovered. The satisfactory criterion and metric used to access this procedure often refers to some correlation coefficient between the estimated and simulated parameter values [@wilson_ten_2019]. Parameter recovery can thus be thought of as an internal validation of a model, which if done properly should increase the faith in the parameter estimates when the model is fit to experimental data. The argumentation is that, if we had known the parameters values beforehand (i.e. simulated them), then we know that they are somewhat close to the estimated parameter values obtained from our fitted model. The assumption is thus; if our model recovers the parameter values well in a simulated setting, then it must also do so when fitted to experimental data where the underlying parameters are unknown. This assumption is, of course not necessarily true, but rests on auxiliary assumptions. These auxiliary assumptions include, that the underlying generative cognitive model is the same or at least close to the same, as the one used to model the data. The process of parameter recovery thus assumes that we know the underlying generative model, which is not the case when fitting experimental data. To further elucidate this point, imagine using the three parameter PF described above. We find that it recovers its parameters well using simulated parameters, from the same model. However, if we instead simulated data another PF say the logistic cumulative distribution, one might find that the model cannot well recover the parameters anymore. This is of course nonsensical from the beginning, as how might our model recover parameters of another model. However, in pratice as would also be the case for the example above, models often share similarities, and parameters may have similar meanings due to underlying theories. This last point, of ensuring that we are selecting the right generative model is the challenge of external validity. The challenge is that infinitely many generative models exist, that are compatible with the observed behavior. This challenge cannot easily be solved, as ensuring that we are using the right generative model would entail testing all generative models. That would mean being able to compare them, while ensuring that all these models are distinguishable in principle. In the Cognitive Science literature the common practice is to use theoretical framework(s) to build competing models with different assumptions of the underlying generative process and then compare this subset of models. This comparison is usually done on how well the models can describe the data using statistical metrics such as information criteria or leave one out cross validation, with some penalization for complexity [@vehtari_practical_2017]. This highlights two important aspects; first, our models reflect our theories and are therefore at best as good as our theories, and second, we are likely missing the true generative model in most cases. This last point of missing the true generative process, can be partly mitigated by ensuring that the tested models are distinguishable, at least in principle. This challenge has been addressed using model recovery, which is the act of simulating data from all tested models and then refitting all models to the data simulated by each individual model. Returning to the example of the PF, we might have two competing theories of how stimulus values are translated into binary choices, one involving the lapse rate and one without. Further, we may want to ensure that we can distinguish between the normal and logistic cumulative distributions, which transform stimulus values into probabilities differently. In this practical example, the model space (the entire set of models) consists of 4 models, i.e. two or three parameters for each of the two types of PFs. To conduct model recovery, data would be simulated from these four distinct models, each model would then be fit to all four simulated datasets, and the best-fitting model would be determined for each case. The result of such model recovery, is an $N X N$ matrix, with N being the number of models. The rows in this matrix would indicate which model was used for the simulation, and columns indicate which model was used to fit the data. The entries of the matrix could then either be the frequency of the winning model or one could calculate the probability of choosing a particular model, given the data simulating model. An identity matrix would represents that the models are completely distinguishable. Any deviation from an identity matrix would entail that for some of the simulations, the best fitting model was not the model that simulated the data [@wilson_ten_2019].

## *Limitations of current internal model validation steps*

The model validation steps above should serve to increase faith in our models, their parameters, and the comparison between them. However, some of the metrics used to access these validations have notable flaws. First, the metrics used can be manipulated (faithfully or not), to show good model validation, by masking the actual poor validation. This problem can thus lead to false confidence in the model and overconfidence in the inference made based on it. Additionally, the metrics often lack sensitivity or specificity to provide the person building the model, information about how and were, in parameter space, the model performs well, thereby leaving valuable insights hidden. In the following section, the thesis will highlight some of the metrics commonly used in the literature for model validity, which are described in @wilson_ten_2019. Here a particular focus will be on the challenges of ensuring internal validity by means of conducting parameter recovery. As mentioned above internal validity is often accessed by simulating data from a model given a set of parameters. This simulated data is then fitted to the model, which then optimizes for the parameters. Subsequently, the correlation coefficient between the estimated and simulated parameters is often computed as an estimate of internal validity [@hess_bayesian_2024; @white_testing_2018; @schurr_dynamic_2024]. In their seminal paper @wilson_ten_2019 describes that ideally, the estimated and simulated parameters should be tightly correlated, without any bias. They also highlight that a weak correlation could mean bugs in the code, or an under powered study i.e. too few trials. They also emphasize the importance of plotting a scatter plot of simulated vs estimated parameter values, to access if ranges of parameter values are problematic, but also to accesses whether there might be biases.

I will here argue that the correlation coefficient is an inappropriate metric and that a version of an intra class correlation (ICC) is better suited for the task. Acknowledging two important things; neither metric is perfect, and visually inspecting the simulated vs estimated parameter scatter plot is crucial. The importance in using the right metric is as a precautionary step, especially considering that some literature are starting to just report correlation coefficients without this crucial scatter plot. Without this scatter plot the correlation coefficient can become meaningless, see the problems with the correlation coefficient below. Failing to sure sensitive and specific metrics for internal validity of the models, may result in significant resources being invested in a model that ultimately fails to perform adequately, hindering scientific progress. It may take years before researchers realize that the model is flawed, even in simulated settings, posing a significant roadblock to scientific advancement.

## *Current problems with internal model validity (parameter recovery)*

The first and perhaps biggest problem of internal validation of computational models, is that it is not universally done. This makes it hard or even impossible to know if the generative model in question, can be trusted. The second, almost ubiquitous problem in the literature using parameter recovery is the oversight of interactions between parameters. This is less of a concern for individuals using an established cognitive models, but should be a big concern in the methods papers describing and formalizing them. 23A prime example of this is the Hierarchical Gaussian filter [@mathys_bayesian_2011 ; @mathys_uncertainty_2014]. Where after having laid out the equations of the model, two of the most crucial parameters of the model, that sets this model apart from the Kalman filter, are held constant when performing parameter recovery. Even in much simpler models, as in the PF described above, there are trade offs and interchangeability between parameters. The last problem with parameter recovery is the reliance of correlation coefficients to access it. As has been suggested elsewhere, that the correlational coefficient is at best insufficient and at worst misleading [@schurr_dynamic_2024]. Three primary problems exists with using correlation coefficient to examine internal validity, namely invariance to linear transformations, the domain and the interpretation of the value.

Correlation coefficients are invariant to linear transformations. The means that two sets of variables i.e. [1,2,3] and [1,2,3] have the same correlation after transforming one of the sets with linear transformation. Consider the transformation $f(x)=2\cdot x+3$, resulting in the sets [1,2,3] and [5,7,9]. The correlation coefficient between these two sets will have the same correlation coefficient as before the transformation. In terms of model validation these two instance would be very different. In the first we would have perfect internal validity, whereas in the second it would be severely lacking. This lack of sensitivity to linear transformations does not make sense for parameter recovery, as we want a metric that penalizes this behavior.

The domain of correlations is  [-1; 1]. However, this directionality is nonsensical for internal validity. For instance, a correlation coefficient of -1 would mean perfect recovery of the parameters of the model, with a negative sign, meaning that you do recover the value (or the linear transformed value) just not the sign. Ideally, we seek a metric that ranges from no recovery to perfect recovery, rather than perfect recovery without the sign to perfect recovery with the sign.

Lastly, the interpretation of the correlation coefficient in terms of parameter recovery poses challenges. Determining what is a sufficiently large correlation coefficient for parameters, such that it can be said to be recovered, and identifying what types of uncertainty is causing the correlation to be less than ideal, is not obvious. Attempts to make such distinctions have been made without much traction [@white_testing_2018]. All these issues resemble what researchers have encountered when trying to estimate the stability and/or test -retest reliability. Here the widely used solution was to use the ICC as the metric instead of the correlation coefficient [@schurr_dynamic_2024].

## *ICC Parameter recovery*

Given that the idea of using the ICC as a metric for parameter recovery is relatively new and has only been suggested, and not implemented anywhere in the literature [@schurr_dynamic_2024], I will here outline what the ICC is and how it can overcome some of the shortcomings of the correlation coefficient in model validity. 

The ICC, in its simplest form, is a ratio of irreducible variances (uncertainties) to the total variance in the data. In practical terms, the irreducible uncertainty is the uncertainty between subjects, whereas the total uncertainty can have several components. To calculate the ICC, these variances needs to be estimated such that their ratio can be computed. The estimation of the variances can be achieved using a model that properly accounts for these different types of variance. These models are typically hierarchical models, where known structure of the data is embedded in the model.

Taking the example from above, of a researcher trying to determine the test-retest reliability on the detection threshold of cold stimulus. The researcher will have his subjects come in for x sessions, and do the same task each visit. We will now assume that all subjects come from the same underlying distribution (i.e., the population), which is governed by a population mean and a population variance, i.e. the between subject variance. From this population level an individual subject level distribution is drawn, here each subject has their own mean and variance (within subject variances). Now for each session that each subject participates in a parameter value is drawn. This parameter value is drawn from subject level distribution, which then governs the participants' behavior on that session. This nested hierarchical structure is demonstrated in figure 3, where each of the levels are governed by the levels above and each level has an associated variance. Where the between subject variance is the variance of the population level distribution, and the within subject variance is the variance of each of the participant level distributions. The ICC as mentioned above is the ratio between this within and between subject variances. This can mathematically be expressed as.


$$
ICC = \frac{\sigma^2_{between}}{\sigma^2_{between} + \sigma^2_{within}}
$$ 

Where $\sigma^2_{between}$ is the variance between the subjects' parameter estimates and $\sigma^2_{within}$ is the within subject variance. 

Given the interest the model's performance, we can simulate agents that have no within subject variance i.e. the same true parameter values for each session. Then its possible to examine how the number of trials and or subjects of the cognitive task will influence the model's ability to capture that there is no within subject variation. Note, the number of sessions could also be examined.


This approach has one clear problem, it does not necessarily tell us something about how well the model estimates the true parameter values for each participant at each session. The ICC described above only estimates how close each parameter is to itself between sessions. To capture the difference between the true simulated value and the estimated parameter value of the model, one might use the mean squared errors (MSE) between the simulated and estimated parameter values. This MSE would serve as a residual error of the model on the parameters. Including this into the ICC formulation above is straightforward, as this is just another source of variance which can be added into the denominator. This also highlights one of the advantages of the ICC, i.e. it is a partitioning of variance (uncertainty) in the model. This partitioning of variance is valuable when building and validating models, as this gives clues to where the model fails and where it might excel. In figure 3 the MSE would amount to taking the difference between the estimated parameter distribution of a particular subject at a particular session and the simulated value. Formally we add the MSE into the ICC equation.   

$$
ICC = \frac{\sigma^2_{between}}{\sigma^2_{between} + \sigma^2_{within} + \sigma_\epsilon^2}
$$

Where $\sigma_\epsilon^2$ is the MSE. This conceptualization allows for putting parameter recovery for a model, into a single value for each parameter, that ranges from 0 to 1. This metric is also going to be trial and subject dependent, as well as on the simulated ranges of parameter values like the correlation coefficient. Here it should be noted that this formulation of the ICC would need a nested hierarchical structure as described above and depicted in Figure 3. This is not necessarily the case for the correlation coefficient. Here one could simulate numerous subjects and then calculate the correlation coefficient on each of these subjects fit individually. Alternatively, the ICC used for parameter recovery could also be calculated in a non-nested hierarchical manner, where only a single session for each subject is simulated, essentially setting $\sigma_{within}^2 = 0$.


```{r figure3, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "**Figure 3. Visualization of a nested hierarchical model with sessions nested within subjects within a population.** Displaying from the top down how the nested hierarchical model assumes structure of the underlying data. Data (points in the bottom plot) is entered at the lowest level where the cognitive or statistical model is fitted (here a psychometric function). The parameters of this model is drawn from a session distribution which is nested within a subject distribution which again is nested within a population distribution. This nesting of the parameters of the model allows for seamless estimation of the partitioning of variance within the model. For the sake of parameter recovery within subject variance is simulated to be 0, and the mean squared difference between the session level distribution and the simulated parameter value is calculated and put into the denominator of the ICC metric."}

readRDS(file = here::here("Figures","figure_5_nested_hierarchical.RDS"))
```

Figure 3 displays the conceptualization of the ICC with the additional MSE. Parameters propagate from the population level distribution to the subject level distribution and into the session level distributions which then forms the cognitive model at the trial level. Figure 3 displays this as a psychometric function for two sessions. 

The concept of parameter recovery using this framework aims to access the degree to which the whole model can distinguish between the types of variation (uncertainty). In this framework the within subject variance can be simulated to be 0 and the MSE can be calculated as the difference between the session level parameter estimates and the simulated parameter values. This entails that in the simulated setting the ICC-value from the above equation is 1, i.e. the only source of variation is between subjects. However, running simulations we can investigate how the model itself ascribes this variation, as uncertainty exists  within subjects or session, but also in the parameter estimates themselves.


## *Standard parameter recovery.*

Turning the attention back to the three parameter PF described in the section "The Psychometric function". This cognitive model, as mentioned, will be the used to demonstrate this novel way of conducting parameter recovery. After having specified the model, one can simulate data from different ranges of parameter values, to select an appropriate range of parameter values. Parameter ranges are selected and simulated in accordance to table 1 and figure 4. Using the probabilistic programming language Stan and its interface with R, Rstan [@R-cmdstanr], one can invert the model from the data to derive the estimates of the latent parameters, which were used to simulate the data in the first place. Note, that for all models displayed and estimated their convergence was accessed by ensuring Rhat values were below 1.03, and that no divergent transitions were present. Ideally all chains would have been inspected, but given the vast amount of simulation presented throughout the thesis, visual inspection of each model was infeasible and summary diagnostic were used instead. Furthermore, all priors for all models presented were weakly informed. This would typically entail that most of the prior distributions were set as normal distributions with means of 0 and standard deviations of 3 between 5, in the unconstrained space. For a comprehensive list of all priors used, readers are referred to the supplementary material or the [GitHub repository.](https://github.com/JesperFischer/Master-thesis)

```{r table1, warning = F, message = F, echo = F}

table1 = read.csv(here::here("tables","table1.csv"))

table1$X = NULL


table1 = flextable::flextable(table1)

table1 = flextable::autofit(table1)

table1 = set_caption(table1,
  caption ="Table 1: parameter distributions. Parameter distributions for the simulated agents and the transformations for each of the parameters.")


table1

```

```{r figure4, fig.width = 7, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 4. Displays 100 samples of the parameters of the psychometric function from table 1.** Visualization of the implications of the simulated parameters of table 1. Black lines depicting individual subjects, while the red line depicts the group mean."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_6_psychometric_simulations.png")), scale = 1)
```

Now 100 subject parameters are simulated based on the parameter values of table 1. The data obtained from these 100 subjects are then refitted using the same model. The pairwise scatter plot of estimated vs simulated parameter values are depicted in figure 5. Here the estimation uncertainty of each parameter is added as vertical lines. This particular simulation was done for 100 subjects over 100 trials, where each of the stimulus values were selected as a sequence from -50 to 50 in increments of 1. Figure 5 also displays how adding estimation uncertainty, to the correlation coefficient, changes the resulting size and uncertainty estimate of the correlation coefficient (i.e. its own estimation uncertainty). This influence of uncertainty on the correlation coefficient resembles what was also shown in section on measurement uncertainty. However it should be noted that the addition of this uncertainty does not necessarily decrease the size and or uncertainty of the correlation coefficient. One could imagine a couple of points falling way off the identity line, with high uncertainties. These points would have less weight, when accessed with uncertainty,, meaning that adding estimation uncertainty could in principle also increase the correlation coefficient and decrease its own estimation uncertainty. This highlights the non trivial and non linear link when uncertainties from fitted models are added.


```{r figure5, fig.width = 7.2, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 5. Parameter recovery for the three parameters of the psychometric function in the unconstrained space.** Scatter plot of Simulated vs recovered parameter values, with error bars displaying the 95 % highest density interval for that parameter on that simulation. Text on each facet shows the estimated correlation coefficient with its standard error (estimation uncertainty) (with) and (without) accounting for estimation uncertainty in the individual parameter estimates (data points)."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","parameter_recovery_v1.PNG")), scale = 1)
```

To evaluate the proposed ICC metric alongside the standard parameter recovery approach presented above, the same data-set as above was utilized. Crucially the data set above was simulated using only 50 simulations that were duplicated, making it eligible to compare the above standard parameter recovery with the ICC proposed. This simulation therefore implies that there is no within subject variation, as the first 50 data-sets were duplicated. One particular difference between the above single fit models and the proposed model depicted in figure 5, is the hierarchical structure embedded in the model. The hierarchical structure serves to shrink parameter estimates in relation to their distance and uncertainty from the mean of the higher level distribution, which they are drawn from. This shrinkage, sometimes called pooling, has been shown to improve predictive capacity and these models are commonly used in the Cognitive Science literature [@bates_fitting_2015; @van_boekel_pool_2021; @gomes_should_2022]. To ensure a fair comparison the two internal validity metrics, the correlation coefficient and the ICC, were calculated from this hierarchical model. the Two ICC values were calculated as above now referred to as $ICC_1$ and $ICC_2$, referring to excluding and including the MSE respectively.

```{r figure 6, fig.width = 7.2, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 6. Parameter recovery for the three parameters of the psychometric function in the unconstrained space, using the nested hierarchical model.** Scatter plot of Simulated vs recovered parameter values, with error bars displaying the 95 % highest density interval for that parameter on that simulation. Text on each facet shows four metrics of internal model validity, correlation coefficient (without) and (with) accounting for estimation uncertainty, and the purposed ICC metric without and with including the mean sqaured error, ICC_1 and ICC_2 respectively."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","parameter_recovery_v2_nested.PNG")), scale = 1)
```

Figure 6 illustrates that the hierarchical model fit improves the parameter recovery. This is evident from both from a visual inspection of the points falling closer to the identity line, with less estimation uncertainty, and quantitatively by comparing the correlation estimates between figure 5 and 6. This finding helps to underscore why hierarchical models in general are preferred to single fit models, as the partial pooling improves estimation of the parameters [@bates_fitting_2015; @van_boekel_pool_2021; @gomes_should_2022]. This finding is further demonstrated in figure 7, where estimation uncertainty, here the 95% credibility interval, of each parameter at each session is plotted as histograms. It is quite important to note that a single simulation like this, would be insufficient to ensure that the parameters are recovered. A good example of this, is the lambda parameter. Investigating the pairwise scatter plot, of the nested hierarchical model, one might suggest that this parameter is quite well recovered. However, back transforming a lambda value of -5, on the unconstrained scale, would entail to a lapse rate of around 1.3%. This should be difficult, if not impossible, for the model to accurately estimate, espeically given the 100 trials for each subject. Supplementary Note 2 describes this in more depth. Turning the attention to the ICC values of figure 5. It is observed that $ICC_1$ on each of the three parameters has an upper bound at the maximum value of one. This can be visually inspected by looking a the variation between pairs of data-points. Here the session one estimates are hidden behind the session two estimates, with only a few estimates deviating slightly. The $ICC_2$ estimate is crucially the lowest for all three parameters. Visual inspection of the pairwise scatter plot makes this clear as well. This metric is penalized for both the degree to which the points fall away from the identity line, but also by the estimation uncertainty associated with these points. This therefore also explains why the alpha parameter is close to being asymptotic at 1, but with a little to be desired for simulated values between 0 and 10. 


In the next section it will be shown how these metrics, especially the $ICC_2$, is influenced by different factors. It will be shown that by reducing estimation uncertainty, it is possible to increase this metric. Four different different strategies will be introduced to minimize estimation uncertainty, these include adaptive optimization design, increasing the number of trials, assuming different mean group level slope values and lastly, jointly modeling of the binary responses with response times.

```{r figure 7, fig.width = 7.2, fig.height = 4, warning = F, message = F, echo = F, fig.cap = "**Figure 7 Estimation uncertainty for each parameter for both single and hierarchical fit models** Each panel represents one of the three parameters of the psychometric function with the estimated uncertainty (95% credibility interval) depicted as histograms. The color of the histogram shows whether the model was fit using the single fit or hierarchical model."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","Estimation_uncertainty_v1.PNG")), scale = 1)
```

