# Modeling definitions and validation.

## **Modeling definitions**

The rest of the thesis will revolve around refining, testing, and designing models of cognition, Cognitive modelling will be deployed for this purpose. Cognitive modeling serves as an intermediate level in a hierarchy of computational models on top, and statistical models in the bottom. These types of models can be thought to differ in their flexibility, assumptions, and scope of investigation. It should be noted that these models have many commonalities, such as being mathematical representations of a data generating process. This makes these definitions operational and should be thought of as having fuzzy boundaries [@durstewitz_computational_2016].

*Statistical models* are the models primarily used in medical and social sciences. These models mostly consist of linear and generalized linear (mixed) models [@bahadori_statistical_2023; @maravelakis_use_2019]. These models are linear combinations of independent variables which are sometimes transformed (making them generalized). The mathematical representation of such models follows:

$$
F(y)=\beta·X+\epsilon
$$ 
Where y is a vector of dependent variables of N elements, F is a link function that maps the conditional mean unto a particular space, common link function are the logit and log transformations which maps unto domains of [0 ; 1] and [0 ; ∞] respectively. These domains could be probabilities and strictly positive values like reaction times. $\beta$ is a vector of regression coefficients with P predictors which gets estimated, X Is a matrix of independent variables of size [N, P]. Lastly $\epsilon$ is a vector of N elements containing the errors of the model. The benefit of these statistical or regression models is that maximum likelihood estimators are available, making parameters estimation fast and efficient. However, one limitation is that they put quite big constraints on the types of models that can be fit, i.e. there must be a linear mapping between all independent variable and the dependent variable, in a domain that can be mapped with a link function. This constraint will in many instances make theories hard or impossible to test as human behavior and cognition can be nonlinear [@ivanova_beyond_2022]. It should be noted that the CC examined in the previous section, can be thought of as a special case of this linear model, where $\beta$ is a single value (the CC) and y and x are z-transformed vectors, see supplementary figure 2.

*Cognitive models* are models that are meant to resemble the generative processes of human behavior more closely. These models are generally more theoretically driven as the constraint of linear combinations is avoided, by employing different optimization schemes. In many cases cognitive models are estimated in a Bayesian framework due to the flexibility with which models can be specified. The main advantage of these models, in this context, is the added freedom in model specification.

*Computational models* are the upper most level of the hierarchy, which here will be used to refer to the generalization of cognitive models to other scientific domains, such as physics, biology chemistry etc. These models are outside the scope of this thesis.

These three categories are arbitrary, and many methods and models will fall between them. However, these arbitrary definitions do add value in communicating, the general framework being worked in and thereby what methods are used. The next section will describe a particular cognitive model, which will be the focal point for the rest of the thesis.

\newpage

## *The Psychometric function*

Here the psychometric function (PF) will be explored, as this type of function has been a stable corner stone in the cognitive science literature across many different sub fields [@courtin_spatial_2023; @bahrami_what_2012; @coates_changes_2014; @ma_memorability_2024; @gold_how_2013]. The PF is a continuous function that maps real inputs (here called intensity values) onto probabilities, i.e. the domain $[-\infty ; \infty]$ with the range of $[0 ; 1]$. In most cases the PF is identical to a logistic regression in statistical modeling and is commonly used in perceptual research where probabilities are then converted into binary forced choices through a Bernoulli or binomial distribution. The PF is usually a cumulative density function such as the cumulative logistic or normal distribution, amounting to a logistic or probit regression in the statistical framework. The main difference between the statistical and cognitive frameworks usage of the PF is the number of parameters. 

The least number of parameters used to describe the PF is two, the threshold ($\alpha$) and the slope ($\beta$). These two parameters describe the center of the curve, with $\alpha$ being the intensity of the stimulus at probability 0.5, with $\beta$ being the steepness of the function around the threshold. In the cognitive modeling framework one or two more parameters are introduced, the lapse ($\lambda$) and guess rates ($\gamma$). These two parameters together handle the tails (i.e. the far ends) of the psychometric function and essentially making the probability in these tails non-deterministic i.e. the upper and lower bounds become ($\lambda$) and ($\gamma$) instead of 0 and 1 (figure 2). These additional parameters help fit the PF to data where attentional slips or wrong button presses happen. It can even be shown that including these parameters will greatly improve the estimation of the slope of the PF, if lapses and or guesses are present in the data [@wichmann_psychometric_2001]. Figure 2 depicts how all these parameters change the shape of the PF. For the sake of this thesis, the cumulative normal distribution is used to map stimulus values to probabilities with a single lapse rate. This lapse rate will govern the distance between the upper and lower bound, essentially making it equally likely to have an erroneous response for high and low stimulus values. The mathematical formulation of the function is as follows:

$$
p(x | \alpha, \beta, \lambda) = \lambda + (1-2 * \lambda) * (0.5+0.5 * erf{(\frac{x-\alpha}{\beta * \sqrt{2}})})    
$$

<!-- this plot should be of above function-->

```{r figure2, fig.width = 7, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 2 Psychometric parameters.** Displays how the parameters of the psychometric function i.e., the threshold (alpha), the slope (beta) and the Lapse rate (lambda) of the psychometric fucntion changes its shape. Columns display how the beta parameters changes the slope of the function. Rows show how alpha changes the location of the center of the function changes. Lastly, colors in the plot depict how lambda changes the asympotes in extreme stimulus (x) values."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_4_psychometric_parameters.png")), scale = 1)

```

## *Model validation.*

In the same vein of validating the bootstrapping approach with the analytical solution, in the previous section about measurement uncertainty, cognitive models themselves are many times validated. This is to ensure that at least in principle the parameters of the model can be estimated with increasing accuracy with increasing number of trials. This section will highlight some of the emerging ways in which computational models in the cognitive science literature are being tested and validated and takes inspiration in the seminal paper from Wilson and Collins [@wilson_ten_2019] describing 10 simple rules of computational modeling, which is commonly cited when validation of computational models are described [@hess_bayesian_2024]. There are at least three main challenges when building and validation cognitive models, which are particularly relevant when writing novel models. How do we know that our models do what we think they do (identifiability)? How do we know that they accurately estimate the parameters of interest (Internal validity)? And lastly, how do we know that we can distinguish between competing models (external validity)? The last challenge is beyond the scope of the current thesis and is well covered elsewhere [@wilson_ten_2019]. The answer to the first two challenges must be found in simulations, especially when our models become more and more complex and analytical solutions are sparse or even non-existent. 


The simulation practice revolves around selecting an appropriate range of parameter values and using these to simulate data from our models. These simulated data are then used to refit the model to examine how well the model approximates the simulated parameter values. Ensuring that in these approximations are close simulations is desirable for the model to perform well, such that one can have faith in them when the real underlying process is unknown, i.e. analyzing real experimental data. An appropriate range of parameter values for a particular model can be difficult to select, as this is exactly the problem of identifiability. Several lines of information can help gauge this. Firstly, looking at mathematical constraints of the model formulations can reduce the possible ranges of parameter values. For the case of the PF this amounts to ensuring that the slope is strictly positive, as this ensures that increasing levels of stimuli will produce greater probabilities of responding 1. This also ensures that the standard deviation of the underlying probability density function is strictly positive. The lapse rate of the PF should be constrained between 0 and 0.5, again to ensure a particular the shape of the PF. Lapse rates below 0 and above 1 will produce probability values outside the [0; 1] range and values above 0.5 will flip the shape of the PF, as negative slope values would Not constraining the PF in this way could lead to two distinct solutions, as negative slope values and lapse rates above 0.5 would be able to produce the same mathematical transformation, making the solution non unique. From a more theoretical level an appropriate range of parameter values can be narrowed down by investigating whether the observed behavior (given the simulated parameter values) is physically or biologically plausible. For the PF we might expect a few of our participants to not be particularly interested in the task, this would amount to having a high lapse rate i.e. above 0.2 (amounting to 20% lapse responses) or really shallow slopes. this behavior however, is quite unlikely and expecting only few lapses in the experiment, given that it is conducted in a quiet environment is likely. Lastly using empirical knowledge from the literature can help narrow the parameter space further. For the sake of argument, one might investigate the detection threshold for cold stimulation on the skin. Just given this information alone it is possible to narrow down the threshold for cold detection to being below the skin temperature of around 30-34 degrees$^\circ$ and above the absolute zero of -273 degrees [@courtin_spatial_2023]. However, knowledge from the scientific literature would suggest that thresholds between 28 and 33$^\circ$ would capture most of the population [@lithfous_accurate_2020]. These same arguments would also apply for the slope. This practice of investigating the assumptions of the simulated parameter values is closely related to prior predictive checks in Bayesian workflows [@kruschke_bayesian_2021]. 



The next challenge is about internal validity, i.e. can our model estimate the parameter values used to simulate the data on which the model estimates the parameter values. To test and validate our models in this regard, we simulate data from pre-specified parameter values, which have been deemed to be appropriate, using the first step described above. We then fit our models on this simulated data and investigate how well the model can estimate the latent simulated parameters (i.e. those that produced the data). This exercise of simulating behavior and then re-estimating the parameter values from the simulated behavior, is commonly known as parameter recovery. Generally, if this procedure succeeds, then the parameters are said to be recovered. The satisfactory criterion and metric used to access this procedure, often refers to some CC between the estimated and simulated parameter values [@wilson_ten_2019]. Parameter recovery can thus be thought of as an internal validation of a model, which if done properly should increase the faith in the parameter estimates, when the model is fit to experimental data. The argumentation is thus, had the parameter estimates been known beforehand (i.e. simulated them), then we know that they are close to the estimated parameter values obtained. The assumption of this argument is that if our model recovers the parameter values well in a simulated setting, then it must also do so when fitted to experimental data, where the underlying parameters are unknown. This assumption rests on auxiliary assumptions. These auxiliary assumptions include, that the model that generated the data is the same or close to, as the one used to model the data. The process of parameter recovery thus assumes that we know the underlying generative model, which is not the case when fitting experimental data. 


This point, of ensuring that we are selecting the right generative model is the challenge of external validity. The challenge is that infinitely many generative models exist, that are compatible with the observed behavior. This challenge cannot easily be solved, as ensuring that we are using the right generative model would entail testing all generative models. That would mean being able to compare them, while ensuring that all these models are distinguishable In the Cognitive Science literature the common practice is to use theoretical framework(s) to build competing models with different assumptions of the underlying generative process and then compare this subset of models (REF). This comparison of models is usually done on how well the models can describe the data, using statistical metrics such as information criteria or leave one out cross validation, with some penalization for complexity [@vehtari_practical_2017]. This highlights two important aspects; first, our models reflect our theories and are therefore at best as good as our theories, and second, we are likely missing the true generative model in. This point of missing the true generative process, can be partly mitigated by ensuring that the tested models are distinguishable, at least in principle. This challenge has been addressed by simulating data from all tested models and then refitting all models to the data simulated by each individual model. Returning to the example of the PF, we might have two competing theories of how stimulus values are translated into binary choices, one involving the lapse rate and one without. To conduct model recovery, data would be simulated from these two distinct models, each model would then be fit to all simulated datasets, and the best-fitting model would be determined for each case. The result of such model recovery, is an N by N matrix, with N being the number of models. The rows in this matrix would indicate which model was used for the data simulation, and columns indicate which model was used to fit the data. The entries of the matrix would then be the frequency by which a particular model wins in model comparison, given the data simulating model. An identity matrix would represents that the models are completely distinguishable. Any deviation from an identity matrix would entail that for some of the simulations, the best fitting model was not the model that simulated the data [@wilson_ten_2019].

## *Limitations of current internal model validation steps*

The model validation steps described above serve to increase faith in our models, their parameters, and the comparison between them. However, I will here argue that some of the metrics used to access these validations have notable flaws, with a particular focus on the problem of internal validity. First, the metrics used can be misleading, to show good model validation, by masking the actual poor validation. This problem can thus lead to false confidence in the model and overconfidence in the inference made based on it. Additionally, the metrics often lack sensitivity or specificity to provide the person building the model, information about how and were, in parameter space, the model performs well, thereby leaving valuable insights hidden. In the following section, the thesis will highlight some of the metrics commonly used in the literature for model validity, which are described in @wilson_ten_2019. As mentioned above internal validity is often accessed by simulating data from a model given a set of parameters. This simulated data is then fitted to the model, which then optimizes for the parameters. Subsequently, the CC between the estimated and simulated parameters is often computed as an estimate of internal validity [@hess_bayesian_2024; @white_testing_2018; @schurr_dynamic_2024]. In their seminal paper @wilson_ten_2019 describes that ideally, the estimated and simulated parameters should be tightly correlated, without any bias. They also highlight that a weak correlation could mean bugs in the code, or an under powered study i.e. too few trials. They also emphasize the importance of plotting a scatter plot of simulated vs estimated parameter values, to access if ranges of parameter values are problematic, but also to accesses whether there might be biases.

I will here argue that the CC is an inappropriate metric and that a version of an intra class correlation (ICC) is better suited for the task of interest. Acknowledging two important things; neither metric is perfect, and visually inspecting the simulated vs estimated parameter scatter plot is crucial. Failing to ensure sensitive and specific metrics for internal validity of the models, may result in significant resources being invested in a model that ultimately fails to perform adequately, hindering scientific progress. It could take years before researchers realize that a model is flawed, even in simulated settings, posing a significant roadblock to scientific advancement.

## *Current problems with internal model validity (parameter recovery)*

The first and perhaps biggest problem of internal validation of computational models, is that it is not universally done. This makes it hard or even impossible to know if the generative model in question, can be trusted. The second, almost ubiquitous problem in the literature using parameter recovery is the oversight of interactions between parameters. This is less of a concern for individuals using an established cognitive models, but should be a big concern in the methods papers describing and formalizing them. A prime example of this is the Hierarchical Gaussian filter [@mathys_bayesian_2011 ; @mathys_uncertainty_2014]. Where after having laid out the equations of the model, two of the most crucial parameters ($\kappa$ and $\theta$) of the model, that sets this model apart from the Kalman filter, are held constant when performing parameter recovery. Even in much simpler models, as in the PF described above, there are trade offs and interchangeability between parameters. The last problem with parameter recovery is the reliance of CCs to access it. As has been suggested elsewhere, that the CC is at best insufficient and at worst misleading [@schurr_dynamic_2024]. Three primary problems exists with using CC to examine internal validity, namely invariance to linear transformations, the domain and the interpretation of the estimate.

CCs are invariant to linear transformations. The means that two sets of variables i.e. [1,2,3] and [1,2,3] have the same correlation after transforming one of the sets with linear transformation. Consider the transformation $f(x)=2\cdot x+3$, resulting in the sets [1,2,3] and [5,7,9]. The CC between these two sets will have the same CC as before the transformation. In terms of model validation these two instance would be very different. In the first, one would have perfect internal validity, whereas in the second it would be severely lacking. This lack of sensitivity to linear transformations does not make sense for parameter recovery, as we want a metric that penalizes this behavior.

The domain of correlations is  [-1; 1]. However, this directionality is nonsensical for internal validity. A CC of -1 would mean perfect recovery of the parameters of the model, with a negative sign, meaning that you do recover the value (or the linear transformed value) just not the sign. Ideally, we seek a metric that ranges from no recovery to perfect recovery, rather than perfect recovery without the sign to perfect recovery with the sign.

Lastly, the interpretation of the CC in terms of parameter recovery poses challenges. Determining what is a sufficiently large CC for parameters and identifying what types of uncertainty is causing the correlation to be less than ideal, is not obvious. Attempts to make such distinctions have been made without much traction [@white_testing_2018]. All these issues resemble what researchers have encountered when trying to estimate the stability and/or test -retest reliability. Here the widely used solution was to use the ICC as the metric instead of the CC [@schurr_dynamic_2024].

## *ICC Parameter recovery*

Because the idea of using the ICC as a metric for parameter recovery is relatively new and has only been suggested, and not implemented anywhere in the literature, to the authors knowledge [@schurr_dynamic_2024]. I will here outline what the ICC is and how it can combat some of the shortcomings of the CC in accessing model validity. 

The ICC, in its simplest form, is a ratio of irreducible variances (uncertainties) to the total variance in the data. In practical terms, the irreducible uncertainty is the uncertainty between subjects, whereas the total uncertainty can have several components. To calculate the ICC, these variances needs to be estimated such that their ratio can be computed. The estimation of the variances can be achieved using a model that properly accounts for these different types of variance. These models are typically hierarchical models, where known structure of the data is embedded.

Taking the of a researcher trying to determine the test-retest reliability on the detection threshold of cold stimulus. The researcher will have his subjects come in for x sessions, and do the same task each visit. We will now assume that all subjects come from the same underlying distribution (i.e., the population), which is governed by a population mean and a population variance, i.e. the between subject variance. From this population level an individual subject level distribution is drawn, here each subject has their own mean and variance (within subject variances). Now for each session that each subject participates in a parameter value is drawn. This parameter value is drawn from subject level distribution, which then governs the participants' behavior on that session. This nested hierarchical structure is demonstrated in figure 3, where each of the levels are governed by the levels above and each level has an associated variance. The between subject variance is the variance of the population level distribution, and the within subject variance is the variance of each of the participant level distributions. The ICC as mentioned above is the ratio between this within and between subject variances. This can mathematically be expressed as.


$$
ICC = \frac{\sigma^2_{between}}{\sigma^2_{between} + \sigma^2_{within}}
$$ 

Where $\sigma^2_{between}$ is the between the subjects variance and $\sigma^2_{within}$ is the within subject variance. Given the of interest the model's performance, we can simulate agents that have no within subject variance i.e. the same true parameter values for each session. Then its possible to examine how the number of trials and or subjects of the cognitive task will influence the model's ability to capture that there is no within subject variation. Note, the number of sessions could also be examined.


This approach has one clear problem, it does not tell explicitly investigate how well the model estimates the true parameter values, for each participant at each session. The ICC described above only estimates how close each parameter is to itself between sessions. To capture the difference between the true simulated value and the estimated parameter value of the model, one might use the mean squared errors (MSE) between the simulated and estimated parameter values. This MSE would serve as a residual error of the model on the parameters. Including this into the ICC formulation above is straightforward, as this is just another source of variance which can be added into the denominator. This also highlights one of the advantages of the ICC, i.e. it is a partitioning of variance (uncertainty) in the model. This partitioning of variance is valuable when building and validating models, as this gives clues to where the model fails and where it might excel. In figure 3 the MSE would amount to taking the difference between the estimated parameter value (distribution) of a particular subject at a particular session and the simulated value. Formally we add the MSE into the ICC equation.   

$$
ICC = \frac{\sigma^2_{between}}{\sigma^2_{between} + \sigma^2_{within} + \sigma_\epsilon^2}
$$

Where $\sigma_\epsilon^2$ is the MSE. This conceptualization allows for putting parameter recovery for a model, into a single value for each parameter, that ranges from 0 to 1. This metric is also going to be trial and subject dependent. Here it should be noted that this formulation of the ICC would imply a nested hierarchical structure, as described above and depicted in Figure 3. This is not necessarily the case for the CC. Using the CC one could simulate numerous subjects and then calculate the CC on each of these subjects fit individually. Alternatively, the ICC used for parameter recovery could also be calculated in a non-nested hierarchical manner, where only a single session for each subject is simulated, i.e., mathematically $\sigma_{within}^2 = 0$.


```{r figure3, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "**Figure 3. Visualization of a nested hierarchical model with sessions nested within subjects within a population of parameter estimates from a psychometric function.** Displaying from the top down how the nested hierarchical model assumes structure of the underlying data. Data (points in the bottom plot) is entered at the lowest level where the cognitive or statistical model is fitted (here a psychometric function). The parameters of this model is drawn from a session distribution which is nested within a subject distribution which again is nested within a population distribution. This nesting of the parameters of the model allows for seamless estimation of the partitioning of variance within the model. For the sake of parameter recovery within subject variance is simulated to be 0, and the mean squared difference between the session level distribution and the simulated parameter value is calculated and put into the denominator of the ICC metric."}

readRDS(file = here::here("Figures","figure_5_nested_hierarchical.RDS"))
```

Figure 3 displays the conceptualization of the ICC with the additional MSE. Parameters propagate from the population level distribution to the subject level distribution and into the session level distributions, which then forms the cognitive model at the trial level. Figure 3 displays this as a psychometric function for two sessions. 

The concept of parameter recovery using this framework aims to access the degree to which the whole model can distinguish between the types of uncertainty. In this framework the within subject variance can be simulated to be zero, and the MSE can be calculated as the difference between the session level parameter estimates and the simulated parameter values. This entails that in the simulated setting the ICC-value from the above equation is 1, i.e. the only source of variation is between subjects. However, when running simulations we can investigate how the model itself ascribes this variation, as uncertainty will be inferred within subjects or session, but also in the parameter estimates themselves.


## *Standard parameter recovery.*

Turning the attention back to the three parameter PF. This cognitive model, will be the used to demonstrate this novel way of conducting parameter recovery. After having specified the model, one can simulate data from different ranges of parameter values, to select an appropriate range of parameter values. Parameter ranges are selected and simulated in accordance to table 1 and figure 4. Using the probabilistic programming language Stan and its interface with R, Rstan [@R-cmdstanr], one can invert the model from the data to derive the estimates of the latent parameters, which were used to simulate the data in the first place. Note, that for all models displayed and estimated their convergence was accessed by ensuring Rhat values were below 1.03, and that no divergent transitions were present. Ideally all chains would have been inspected, but given the vast amount of simulation presented throughout the thesis, visual inspection of each model was infeasible and summary diagnostic were used instead. Furthermore, all priors for all models presented were weakly informed. This would typically entail that most of the prior distributions were set as normal distributions with means of 0 and standard deviations of 3 between 5, in the unconstrained space. For a comprehensive list of all priors used, readers are referred to the supplementary material or the [GitHub repository.](https://github.com/JesperFischer/Master-thesis)

```{r table1, warning = F, message = F, echo = F}

table1 = read.csv(here::here("tables","table1.csv"))

table1$X = NULL

table1$Parameter = c("Alpha","Beta","Lambda")


table1 = flextable::flextable(table1)

table1 = flextable::autofit(table1)

table1 = set_caption(table1,
  caption ="Table 1: parameters of the normal distribution used to simulate agents. Columns depicts the the parameter type for the psychometric function, the mean and standard deviation of the normal distribution used for simulating the parameters and lastly the transformations for each of the parameters, from left to right.")


table1

```

```{r figure4, fig.width = 7, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 4. Displays 100 samples of the parameters of the psychometric function from table 1.** Visualization of the implications of the simulated parameters of table 1. Black lines depicting individual subjects, while the red line depicts the group mean."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_6_psychometric_simulations.png")), scale = 1)
```

100 pairs of parameters are simulated based on the values in table 1, amounting to simulating 100 subjects. The data obtained from these 100 subjects are then refitted using the same model. The pairwise scatter plot of estimated vs simulated parameter values are depicted in figure 5. Here the estimation uncertainty of each parameter is added as vertical lines. This particular simulation was done for 100 subjects over 100 trials, where each of the stimulus values were selected as the set $x_i = \in \{ -50, -49, \ldots, 49, 50\}$. Figure 5 also displays how adding estimation uncertainty, to the CC, changes the resulting size and uncertainty estimate of the CC (i.e. its own estimation uncertainty). This influence of uncertainty on the CC resembles what was also shown in section on measurement uncertainty. It should be noted that the addition of this uncertainty does not necessarily have to decrease the size and or uncertainty of the CC. One could imagine a couple of points falling way off the identity line, with high uncertainties. These points would have less weight, when accessed with uncertainty, meaning that adding estimation uncertainty could in principle also increase the CC and decrease its own estimation uncertainty. This highlights the non trivial and non linear link when uncertainties from fitted models are added to further analyses.


```{r figure5, fig.width = 7.2, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 5. Parameter recovery for the three parameters of the psychometric function in the unconstrained space.** Scatter plot of simulated vs recovered parameter values, with error bars displaying the 95 % highest density interval for that parameter on that simulation. Text on each facet shows the estimated correlation coefficient with its standard error (estimation uncertainty) (with) and (without) accounting for estimation uncertainty in the individual parameter estimates (data points), i.e., propergating uncertainty."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","parameter_recovery_v1.PNG")), scale = 1)
```

To evaluate the proposed ICC metric alongside the more standard parameter recovery approach, the same data-set as above was utilized. Crucially the data set above was simulated using only 50 simulations that were duplicated, making it eligible to compare the above standard parameter recovery with the ICC proposed. This simulation therefore implies that there is no within subject variation, as the first 50 data-sets were duplicated. One particular difference between the above single fit models and the proposed model depicted in figure 3, is the hierarchical structure embedded in the model. The hierarchical structure serves to shrink parameter estimates in relation to their distance and uncertainty from the mean of the higher level distribution, which they are drawn from. This shrinkage, sometimes called pooling, has been shown to improve predictive capacity and these models are commonly used in the Cognitive Science literature [@bates_fitting_2015; @van_boekel_pool_2021; @gomes_should_2022]. To ensure a fair comparison the two internal validity metrics, the CC and the ICC, were calculated from this hierarchical model. Two ICC values were calculated now referred to as $ICC_1$ and $ICC_2$, referring to excluding and including the MSE respectively.

```{r figure 6, fig.width = 7.2, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 6. Parameter recovery for the three parameters of the psychometric function in the unconstrained space, using the nested hierarchical model.** Scatter plot of simulated vs recovered parameter values, with error bars displaying the 95 % highest density interval for that parameter on that simulation. Text on each facet shows four metrics of internal model validity, correlation coefficient (without) and (with) accounting for estimation uncertainty, and the purposed ICC metric without and with including the mean sqaured error, ICC_1 and ICC_2 respectively."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","parameter_recovery_v2_nested.PNG")), scale = 1)
```

Figure 6 illustrates that the hierarchical model fit improves the parameter recovery metrics. This is evident from both from a visual inspection of the points falling closer to the identity line, with less estimation uncertainty, and quantitatively by comparing the correlation estimates between figure 5 and 6. This finding helps to underscore why hierarchical models in general are preferred to single fit models, as the partial pooling improves estimation of the parameters [@bates_fitting_2015; @van_boekel_pool_2021; @gomes_should_2022]. This finding is further demonstrated in figure 7, where estimation uncertainty, here the 95% credibility interval, of each parameter at each session is plotted as histograms. It is quite important to note that a single simulation like this, would be insufficient to ensure that the parameters are recovered. A good example of this, is the lambda parameter. Investigating the pairwise scatter plot, of the nested hierarchical model, one might suggest that this parameter is quite well recovered. However, back transforming a lambda value of -5, on the unconstrained scale, would entail to a lapse rate of around 1.3%. This should be difficult, if not impossible, for the model to accurately estimate, espeically given the 100 trials for each subject. Supplementary Note 2 describes this in more depth. 

Turning the attention to the ICC values of figure 5. It is observed that $ICC_1$ on each of the three parameters has an upper bound at the maximum value of one. This can also be visually inspected by looking a the variation between pairs of data-points. Here the session one estimates are hidden behind the session two estimates, with only a few estimates deviating slightly. The $ICC_2$ estimate is crucially the lowest for all three parameters. Visual inspection of the pairwise scatter plot makes this clear as well. This metric is penalized for both the degree to which the points fall away from the identity line, but also by the estimation uncertainty associated with these points and the variation between pairs. This also explains why the alpha parameter is close to being asymptotic at 1, but with a little to be desired for simulated values between 0 and 10. 

In the next section it will be shown how these metrics, especially the $ICC_2$, is influenced by different factors. It will be shown that by reducing estimation uncertainty, it is possible to increase this metric. Four different different strategies will be introduced to minimize estimation uncertainty, these include adaptive optimization design, increasing the number of trials, assuming different group mean slope values and lastly, jointly modeling of the binary responses with response times.

```{r figure 7, fig.width = 7.2, fig.height = 4, warning = F, message = F, echo = F, fig.cap = "**Figure 7 Estimation uncertainty for each parameter for the single and hierarchical fit models** Each panel represents one of the three parameters of the psychometric function with the estimated uncertainty (95% credibility interval) depicted as histograms. The color of the histogram shows whether the model was fit using the single fit or hierarchical model."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","Estimation_uncertainty_v1.PNG")), scale = 1)
```

