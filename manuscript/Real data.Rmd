## *Experimental data*

Having rigorously investigated how the psychometric function behaves and how the certainty of the parameters interacts with each other but also with the number of trials for each subject, the thesis now turns to the analysis of data collected on participants. This section introduces a the published dataset that will re-analysis utilizing the psychometric functions introduced. The goal with this re-analysis is 2-fold. Firstly, it reiterates the fact that the assumptions about the structure of the data can make big differences in the parameter estimates and their uncertainty. Secondly, it will serve as a starting point to understanding why the internal model validity steps are helpful as a metrics to gauge how trials and subjects interact on the statistical power of a model to reject a hypothesis. This last aspect of testing hypotheses will tie together how the validity steps above can help determine the ability of a particular model to do hypothesis testing. The last point of the thesis is going revolve around conducting a thorough power analysis of the current model, utilizing the published dataset described below. In this regard of conducting a power analysis I will again highlight where uncertainty creeps in and how we can deal with and account for it, as common practices are insufficient.

## *Heart rate discrimination task*

The article introducing the dataset is @legrand_heart_2022 and is an interoceptive task. Here the authors collected 223 participants who came in twice to complete the heart rate discrimination (HRD) task with 6 weeks between visits. 

In the HRD task participants are asked to internalize their own heart rate for 5 seconds, meanwhile the participant's heart rate is monitored and calculated in real time. Next, based on the observed heart rate participants will hear five auditory tones with a given frequency (not the internal frequency of the tone, but the frequency of how fast the tones is presented) that is either faster or slower than their own objective heart rate. The amount this auditory tone's frequency is faster or slower is determined by the PSI procedure introduced in the Adaptive design optimizing paragraph. This means that the stimulus value for the psychometric function for this experiment is the difference between the external tone's frequency and the observed heart rate of the participant. The responses provided by the participants are therefore either faster or slower with faster referring to the belief that the individual heart rate was faster than the tone provided. This means that a participant might have a heart rate of 50 beats per minute (BPM) at a particular trial and then hear tones in a frequency of 40 BPM and are asked to respond whether they think this 40 BPM tone is slower or faster than their own heart rate. The authors of the experiment, described above, ran a single participant level model of each subject, for each session, and then correlated the slope and threshold of the psychometric function. They found a medium correlation between the threshold r = 0.51 p \< .001 between sessions and a negligible correlation r = 0.1, p = .15 for the slope. In the next section I will show how this reliability might change given different assumptions of the structure of the data as well as employing different models.

## *The models*

In this section the models fit to this big test-retest dataset are described, in order to examine the influence of the model fit on the correlation between session one and two of the PF parameters i.e. the threshold and slope. The single fit model is going to be the references and going to be the same as the original authors did. That is estimating each individual for each of the sessions individually without a lapse rate (i.e. a two parameter PF) and then post hoc correlating the estimates between session one and two. adding the propagated uncertainty to these estimates will serve as the next model. Next, the same model as above with a lapse rate with be tested, in order to understand the influence of this parameter in this particular dataset. 


Two types of hierarchical models are going to be fit, one with a single layer amounting to modeling the two sessions from the same multivariate normal distribution with hierarchical priors for each session. This model directly models the correlation between sessions as its included in the variance - covariance matrix of the multivariate normal distribution.  The last type of model is the nested hierarchical model identical to the model presented when introducing the ICC, see Figure 5. this model assumes that all subjects have a mean level parameter which is drawn from the same multivariate normal distribution, each parameter for each session is then drawn from a subject level distribution. For this last model the ICC is the statistical metric estimated by the model itself, and the correlation will afterwards be calculated. In addition to testing the influence of the data structure in the fitted models the reaction times for each trial is also going to be included in the analysis in the same vein as described in the section about increasing information in cognitive models. finally a full model is going to be fit utilizing confidence rating readily availiable in the dataset. This model will not only incorporate the reaction times on a trial-by-trial basis, but also these confidence ratings for each trial. Confidence ratings were included in the task of the original experiment to examine the participants' interoceptive metacognitive abilities, but here they will be used to inform the parameters of the underlying psychometric function. The confidence ratings are going to be modeled in close resemblance to the reaction times, just inverted. This inversion is because in the middle of the psychometric function (at the threshold) the uncertainty about the stimulus representation is the highest and therefore reaction times should be their highest as well, but confidence should be at the lowest. Another difference between the reaction times and the confidence ratings is their range of possible values and therefore the probability density function used to describe them. The confidence ratings in the task were bounded between 0 and 100 ranging from complete uncertainty and certainty. A natural probability density function for such kind of double bounded variables is the beta distribution as its bounded between 0 and 1 [@geissinger_case_2022]. The problem with using beta distribution in this case is the edge cases of 0 and 1's which for the confidence ratings are 0 and 100. One approach to circumvent this is to model these edge cases separately using a zero-one-inflated beta distribution. This approach, however, models these edge values as separate processes which does not make sense in this case as the confidence ratings are meant to represent a continuous measure of confidence. The thesis therefore subtracts a small number i.e. 0.1 from the 100 ratings and adds 0.1 to the 0 ratings making it possible to use the beta distribution for the full range of confidence ratings after deviding all ratings by 100. This approach of modeling the bounded ratings between 0 and 100 is tenuous and new methods are slowly being developed for a more holistic approach see @kubinec_ordered_2023. Reaction times of the responses were at maximum 8 seconds and can therefore still be modeled by the shifted log normal distribution introduced above.

## *Results*

Table 3 displays the correlation coefficient between the first and second session for the threshold and slope for each model when uncertainty has been propagated using bootstrapping. For a full table of all parameters of all models as well as with and without uncertainty propagation see the [github page](https://github.com/JesperFischer/Master-thesis) linked to the thesis.

\newpage

```{r table3, fig.width = 7, fig.height = 7, warning = F, message = F, echo = F, fig.cap = "**Table 3. Results from reanalysis of legrand (2022).** Table showing the correlation between sessions of the threshold and slope parameter of the psychometric function using different model fomulations as well as hierarchical model structures."}
table3 = read.csv(here::here("tables","table3.csv")) %>% mutate(X = NULL)


table3 = flextable::flextable(table3) %>% width(c(1,2), width = 1.7)%>% width(5, width = 1.1)%>% width(4, width = 1)

table3
```

Table 3 highlights the fact that the additional assumptions of the hierarchical models both nested and unnested increases the session-by-session correlation of the slope of the psychometric function.  Additionally including the reactions times increases the correlation even more, however with overlaying 95% credibility intervals. The models with included confidence ratings perform worse than the models with only the added reaction time in the session by session correlation. This might allude to the fact that another process is present, or that the particular model used for the confidence ratings, might be inappropriate. The main difference in session-by-session correlation between the two hierarchical models can be found in the threshold as the nested hierarchical model outperforms the non-nested hierarchical model in this regard. A concern of this approach of just looking the correlation coefficients is of cause that a model with a high session by session correlation might not fit the data at all. Therefore an examination of the comparison of model fit is crucial to ensure that the nested hierarchical model also fits the data better than the unnested hierarchical model. One approach would therefore be to examine model fit using common metrics such as cross validation, information criterion etc. This difficulty here is that most of the models are incompatible. This stems from the fact that they have been fit to differing amounts of subjects in the case of hierarchical vs single fit models, and to differing amounts of dependent variables in the case of within model architectures. Another consideration for not conducting model comparison is that the difference between the models that are comparable i.e. the Hierarchical and Nested hierarchical with the same number of dependent variables is that the difference between these model lie in the assumption of the data generating process. Therefore given that it is known that each subject was accessed twice (and not a new participant), and that the nested Hierarchical model captures this assumption, one should be inclined to choose this model regardless.  

Therefore instead of directly comparing the comparable models, the posterior predictive checks were performed for the most complicated models to ensure that the models are capturing some of the underlying patterns in the collected data. For these posterior predictive checks on both group level and single subject level see supplementary Figure 8-11 together with Supplementary Note 5.
