## *Importance of uncertainty minimization*

Throughout the thesis uncertainties from measurements to estimations to the uncertainty of these estimations over time has been investigated through statistical and cognitive modeling. The focus of the section on measurement uncertainty was brief and highlighted the ways in which computational resources can be used to account for these. Next the section about estimation uncertainty showed how different approaches can be utilized, from smart design of the experiment, to including additional information present in the data to decrease this kind of uncertainty. In the last section reliability of estimates were examined using experimental data and how the approach of adding addition information to the analysis might increase the reliability of the test re-test reliability. In order to fully appreciate how these uncertainties interact and in which regard they matter for hypothesis testing, the thesis will below conduct a power analysis for the experiment that was analysed in the previous section. In conducting this power analysis measurement uncertainty is assumed to be negligible. This amounts to assuming that the heart rate of the participants is estimated with infinite precision, which was also assumed for the previous analysis as the authors of the experiment did not disclose the uncertainty in these estimates. Furthermore only the simplest form of the PF with 3 parameters is going to be analysed, and only a difference in thresholds is analyzed in order to fully capture the potential of this particular way of conducting a power analysis. The reason for the simplicity of the model is for the power analysis to fully explore the effects of combinations of subjects and trials on statistical power with the least amount of computational overhead while still exploring how uncertainties interact, but see discussion and limitations for further elaboration of this. The next sections are therefore an introduction to power analyses and how they are contextualized.


# Power analysis

When researchers are interested in the parameter values of their models, they are many times also interested in how they differ by for instance using pharmacological interventions or between healthy controls and patient populations. The question in such a scenario is how many participants and or trials do you need to reliably detect a particular size of effect, between the two conditions?  These estimations of trials and participants can in principle given some assumptions be calculated a priori to conducting the experiment. That is the question of what is the probability that our results are going to be "significant" given that there is some "real" underlying effect can be answered. Here it should be noted that significant is here referred to as the standard frequentist approach of rejecting or failing to reject a null hypothesis based on a significance level. Usually this idea of hypothesis testing is depicted in a 2 by 2 matrix with the real latent effect being in one dimension and the model results in the other dimension see table 4. The probabilities of landing in either of these 4 categories is usually described as functions of our statistical significance threshold (alpha / p-value) and the statistical power of our model and test (1-$\beta$). The underlying effect might be from the intervention of the study or a difference between a healthy and a patient population group. The framing of power analyses in this way is then to say that results are significant if the p-value is less than a particular value most often 5%, and that the probability that we detect this effect, given that it is present, is another arbitrary value with 80% being the statistical power of the test. 


\newpage

```{r table 4, fig.width = 7, fig.height = 7, warning = F, message = F, echo = F, fig.cap = "**Table 4.** 2 by 2 confusion matrix of whether the is an underlying effect (Reality) and whether a model is able to correctly identify this effect or not whether its present or not."}

table4 = read.csv(here::here("tables","table4.csv")) %>% mutate(X = NULL)

names(table4) = c(" ", "Reality (effect)","Reality (no effect)")
# Create a flextable
table4 <- flextable(table4) %>% width(j = 1, width = 2) %>% width(j = 2:3, width = 1.3)
table4
```

Moving to the more practical side of power analysis in cognitive or statistical modeling; our models fit to the data will reject and fail to reject different rates of effects given their magnitude, but also given the amount of data i.e. the number of subjects and number of trials. Here the number of subjects and trials serve to reduce the uncertainty in the estimated magnitude of effect and thereby increasing the probability of detecting said effect. With this understanding the commonly depicted table above (table 4) is quite misleading as the dimension of "reality" is a continuous variable of size of the effect, and our models have a particular probability of rejecting a hypothesis at a particular effect size with a particular set of subjects, trials and alpha level. An example of this could be that a researcher wants to detect whether there is an effect of gender on height in the human population. We assume that there is an underlying effect and observe X females and Y males and run a statistical analysis to determine whether we can reject the null hypothesis (there are no differences in height in the two genders). Compare this to the hypothesis that there is an effect of age (in late adolescents and adults) on height. The former difference might in general be much larger than the former and therefore with all else being equal (trials, subjects, statistical model etc.) this difference will be easier to detect compared to the difference in height based on age. What is therefore done when conducting power analyses is that different observed effect sizes are simulated (effect sizes in the data that is observed) with differing amounts of trials and subjects. The ability of the statistical model to reject these simulated experiments are then accessed through many simulations. Usually, this amounts to then counting the number of times, the model achieves "significant" results compared to non-significant results, which is then the power of the model at that number of trials subjects and observed effect size. This approach accurately captures how we expect the model to behave when we fit the data to the model after obtaining it. It tells us if we observe a particular effect size, we will with a particular probability be able call the results significant. The utility of this analysis is therefore to be able to examine how many subjects and or trials are needed to obtain a statistical power of usually 80% given that a particular effect size in the population is present. The assumed effect size in the population might be informed by previous studies and or meta-analyses in the field. Extra assumptions are then needed to approximate the distribution of effect sizes as these statistical metrics also have uncertainty associated with them. This extra aspect many times is disregarded or forgotten and will be expanded upon later.


The power simulations conducted in this thesis will be for a repeated measures design interested in a difference in threshold due to some intervention. Subjects, trials, and effect sizes in a variety of combinations are therefore simulated see figure 16. Here the particular effect size chosen to simulate was cohens' $d_rm$ see formula below. This particular effect size is suitable for repeated measures design as it accounts for the correlation between the two sessions of each participant i.e. the test retest reliability of the measure investigated. The simulation process followed the following procedure: first a set of agents were simulated from a multivariate normal distribution with two sessions from the group level parameters of the binary nested hierarchical model presented in section about experimental data, see supplementary table 2 for the exact values for each parameter. The second session of the agents had their threshold increased by a random variable that was drawn from the difference distribution. This difference distribution was calculated based on the two equations presented below, where the second session variance was defined as 1.5 times the variance of the first session, which made the mean of difference distribution directly related to the simulated effect size. 

To ensure a particular observed effect size, this process was repeated until an observed effect size of the desired value was obtained ($\pm$ 0.01), this step of re-sampling for a particular effect size was mainly for visualization purposes later see Figure 16 and text below. After having simulated the particular parameter values of each agent at each session the agents was put through the pathfinder algorithm to get their trial-by-trial responses and stimulus values. The full trial-by-trial data-set was then fitted using a simple hierarchical model where the threshold was parameterized as a linear combination of an intercept with dummy coding of session as a difference parameter see supplementary note 6 for the full model description.

$$
\mu_{\delta} = d_{rm} * \frac{\sqrt{Var_1 + Var_2 - 2 * \sigma_1* \sigma_2 * \rho}}{\sqrt{2 * (1-\rho)}}
$$

$$
\sigma_{dif} = \sqrt{Var_1 + Var_2 - 2 * \sigma_1* \sigma_2 * \rho}
$$

Mean and standard deviation of the difference distribution between the two sessions, where $Var_1$ is the variance of session 1 $Var_2$ is the variance of session 2. $\rho$ is the correlation between the two sessions. $\mu_{\delta}$ is the mean of the difference distribution and $d_{rm}$ is the standardized effect size between the two sessions.




## *Power analysis results*

With an understanding of the goal of conducting a power analysis it can be difficult to chose the number of combinations of trials, subjects and observed effect sizes to explore. This is due to the fact that the space of trial and subject combination is in the extreme, infinite, and at even a practical level quite huge. Therefore conducting a complete power analysis for a single model is unfeasible, however the thesis will show that the variation in how well the model rejects the null hypothesis, given subject and trial combinations are stable over observed effect sizes, making it possible to give good predictions of power even without having simulated the excat number of participants or trials before. This procedure is therefore about simulating a set of trials, subjects and observed effect sizes and then extrapolate from these simulations. 

For the current power analysis, a result is significant is here defined as if less than 5% of the posterior difference distribution of the threshold (group statistic) cross 0 like setting an alpha value of 5% in a frequentist power analysis. Furthermore 100 simulations are going to be run for each subject, trials observed effect size combination ensuring reasonable estimates of the probability of rejecting the hypothesis. To properly display the raw results of the power analysis where it is possible to compare the effects of trials and subjects visually, the beta distribution is going to be used to aggregate the 100 simulations for each effect size. This is done by utlizing that the beta distribution is a two-parameter distribution that can be parameterized such that one parameter counts how many times an event has happened and the other parameter being how many times this event did not happen. This therefore makes it possible to start with a uniform prior on the probability of rejecting the null hypothesis i.e. Beta(1,1) and then updating this probability density function with the amount of hits and misses here significant or non-significant results. This results in a probability density function that contains all the information in each of the 100 binary points (i.e. significant or not). Figure 16 shows each trial/subject combination with points representing this prior uniform beta distribution updated by the 100 datapoints that were either deemed significant or non-significant. 

A couple of important observations are worth noting when viewing Figure 16. The shape of the points (for each trial,subject combination) very closely resembles a psychometric function where subjects and trials influence both the steepness and the location of the function. This means that increasing the number of subjects (and trials to a lesser extent) has two important features, it shifts the points towards higher power with lower effect sizes, but it also seems to increase the sensitivity to the effect size, i.e. the slope of the curve is getting steeper with higher number of subjects. The extent to which trials matters for the shape is highly dependent on the number of trials, i.e. increasing trials from 10 makes a big difference in the shape of the function, but the difference in going from high to very high i.e. 100 to 150, does not matter much. The tendency of the function to be less affected by ever increasing trials is also present for the number of subjects. This observation makes sense if one takes the function to its extremes in trials and subjects. Increasing subject and trials to infinitely many, we would expect, assuming the model has been shown to become increasingly better with increasing trials (like with the ICC metric presented previously) that the model would be able to pick up on even the tiniest difference in groups. This would essentially mean that the function would consistently be at y = 1 with x approaching 0 from the positive direction and then jump to (0,0) in the (observed effect size, power) curve as no difference would entail no power. In the other extreme were no subjects or trials are present the curve should approach a flat line at y = 0 entailing no power for any amount of effect size. Essentially reaching a step function in the limit when x goes to 0 and subjects and trials goes to infinity I.e.

$$
\lim_{{(s,t)\to\infty}} \left(\Psi(x, \alpha, \beta, s, t) = \begin{cases} 0 & \text{if } x \leq 0 \\ 1 & \text{if } x > 0 \end{cases}\right)
$$

These observations are what is going to be used in the next section in order to extrapolate the results from figure 16. Making it possible to construct a model that will map trials, subjects and effect sizes to power.


```{r Figure 16, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "Figure 16, depicts power as a function of observed effectsizes in different combinations of trials and subjects."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","poweranalysis_scatter.PNG")), scale = 1)
```

## *Modeling of power analysis*

To use the information from above, the latent psychometric function describing the relationship between subjects, trials and effect sizes needs to be investigated. Ideally a psychometric function that enforces the curve going through the origin, as an observed effect size of 0 should always entail no power. Next the parameters of these psychometric functions i.e. the threshold and slope need to be parameterized by the number of trials and subjects such that when trials and subjects move towards infinity the function approaches a step function right as x becomes greater than 0. Before fitting the general case that can be used for extrapolation and also ensuring that a psychometric function is well fitting function to the problem at hand, each set of trials and subject combinations are fit independently to the parameters of the psychometric function. This amounts to estimating a threshold and slope of the psychometric function for each trial and subject combination. This will help ensure that the fitted functions do pass through the points depicted in figure 16 and increase the faith in the next type of modeling. Several types of psychometric functions might be used for this practice, as this problem is defined in terms of finding the function that has the best performance on out of sample predictability. This means that the winning model should be selected based on leave one out cross validation as the ideal model is the model that can best describe new data. This is because the overall goal with this power analysis is to use the quite sparsely simulated space of trials,subjects and effect sizes depicted in figure 16 to inform a model that can predict outside the realms which it has been tested on. Therefore these models were compared using the Pareto smoothed importance sampling leave one out cross validation[@yao_using_2018; @vehtari_practical_2017; @vehtari_pareto_2024]. 

Three types of psychometric functions were fit in the current thesis, the cumulative normal, the cumulative logistic and the cumulative Weibull function. The main differences between the normal and logistic function is that the logistic function has heavier tails than the normal allowing for more disperse observations. The difference between the Weibull and the two other distributions is that the Weibull function is forced through the origin and its shape therefore quite different from the two other functions. The choice of the cumulative normal or logistic function does not necessarily violate the assumptions laid out above because of the way that the parameters are going to be dependent on the trials and subjects. This is clear if one considers an asymptote at 0 for the slope and threshold (i.e. a step function also for the cummulative normal and logistic function) when trials and subjects move to infinity. This exactly matches the observation from above that the psychometric function moves closer and closer to a step-function (as the slope gets closer to 0) and that the location of this step function approaches x = 0 but never reaches it. The results of this preliminary independent analysis on trials and subjects can be seen in figure 17 where the independently fit logistic psychometric functions are overlaid on the observed datapoints from figure 16. The figure clearly highlights a good fit for most of the trials and subject combinations.

```{r Figure 17, fig.width = 6, fig.height = 6, warning = F, message = F, echo = F, fig.cap = "Figure 17; depicts power as a function of observed effectsizes in different combinations of trials and subjects. With lines being independently fit logistic psychometric functions to each trial by subject combination. "}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","power_individualfits.PNG")), scale = 1)

```

## *Continuous mapping of the power analysis*

Moving to the continuous mapping of subjects and trials to the psychometric function's parameters. This mapping should be defined as a function that relates subjects and trials to the parameters that determine the shape of the psychometric function (i.e. the threshold and slope). Given the observations above, that the steepness of the function increases with increasing trials and subjects and that the threshold moves towards 0, a first choice of this mapping function would be to model the two parameters as exponentially decreasing by trials, subjects, together with their interaction. An exponentially decreasing function in the complete general case would mean the following relationship.

$$
\Theta = \beta_0 * exp(-\beta * X) + \alpha_{asym}
$$

Where $\Theta$ represents the parameters of the psychometric function, $\alpha_{asym}$ is the value of the parameter when the number of trials and subjects approach infinity. $\beta$ is vector of parameters determining the steepness of the exponential decrease from the covariates in the matrix X, here trials subjects and their interaction. The parameter $\beta_0$ serve, together with $\alpha_{asym}$, as the value of the parameter when trials and subjects are 0. Another formulation of the dependency might be a power law equation as shown below.

$$
\Theta = \beta_0 * X^{\beta}
$$

Both approaches can produce the observed behavior and their difference depends on the underlying relationship between the parameters and the the matrix X i.e. (trials and subjects and perhaps their interaction). The exponential equation assumes that as trials and subjects increase by a fixed amount the parameters will decrease by a percentage, whereas the power law assumes that as trials and subjects increase by a percentage the parameters will decrease by a percentage. There are several ways of investigating which of these two approaches results in the better fit, firstly plotting the parameters of the independent fits (figure 17) vs trials and or subjects, which was conducted in the section above, on two different coordinate systems either in (log(y),x) or (log(y),log(x)). Which of these coordinate systems produces the best-looking linear line would be the best candidate. Figure 18 displays the three functions fitted independently on each of the two coordinate scales. As can be seen both apporach do produce partly linear relationships, making the distinction dificult.

```{r Figure 18, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "Figure 18. Parameters estimates of the individually fit  psychometric functions (columns) on trials and subjects. The top row depcits a (log(y),x) coordinate system whereas the bottom row a (log(y), log(x)) coordinate system. A straight line relationship between subjects (trials) and the log of the parameter value (top row) would indicate exponential relationship whereas a straight line in the log;log coordinate system would imply a power law relationship in the native (x,y) space."}


cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","power_individal_log_loglog.PNG")), scale = 1)

```

Another approach would be to fit both types of models and then compare them on leave one out cross validation as described above. Implementing this, displayed problems with 15, 25 and 3 % of observations for the normal, Weibull and logistic function respectively as the pareto k diagnostic value was above 1 for these percentages of datapoints. This essentially makes it meaningless to compare the functions [@vehtari_pareto_2024]. Therefore the thesis moving forward uses the logistic cumulative function as this produced the least amount of problems with pareto k values when fitting trials and subjects as continuous informing the parameters of the latent psychometric function. For a complete set of models described for the cummulative logistic function below see supplementary Note 7. The first model fit used the exponentially decreasing function equation above. Four other models were fit with different ways of parameterizing the power law equation above. These four models displayed different ways of how the trials and subjects interact as there is no straightforward way of combining X and β. The first was an additive model with the following parameterization.

$$
\beta_0 \cdot X^\beta = \beta_{01} + s^{\beta_1} + t^{\beta_2} + (t \cdot s)^{\beta_3}
$$

The second was with a combination of additive and multiplicative operations: 
$$
\beta_0 \cdot X^\beta = \beta_{01} * (s^{\beta_1} + t^{\beta_2} + (t \cdot s)^{\beta_3})
$$ 
The third was a multiplicative model without an interaction.

$$
\beta_0 \cdot X^\beta = \beta_{01} \cdot s^{\beta_1} \cdot t^{\beta_2}
$$

The last was the multiplicative model with an interaction but defined as the sum of subjects and trials as the normal interaction of multiplying trials and subjects would lead to a similar model of the model without an interaction.

$$
\beta_0 \cdot X^\beta = \beta_{01} \cdot s^{\beta_1} \cdot t^{\beta_2}\cdot (t+s)^{\beta_3}
$$

Comparing these five models using leave one out cross validation indicated that the best model was the last model but closely followed by the second model, which can be seen in table 5. Importantly for these reported models the diagnostic values were all below 0.7.

```{r table 5, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "Table 5"}

table5 = read.csv(here::here("tables","table5.csv")) %>% mutate(X = NULL)

table5[,2:3] = round(table5[,2:3],2)

table5$elpd_ratio = round(-table5$elpd_diff/table5$se_diff,2) 

table5 <- flextable(table5) %>% width(j = 1:4, width = 1.5)
table5
```

This indicates that as the trials and subjects increase by a percentage the parameters of the psychometric decrease by a percentage as the top two models are both variations of the power law. To ensure that the tested models still capture the underlying data, figure 19 displays the winning model superimposed on the data with 95 credibility intervals of the mean of the psychometric function. As can be seen this closely resembles the individual independent fits, with the most drastic deviation in the 5 subjects, 10 trials condition.

```{r Figure 19, fig.width = 6, fig.height = 6, warning = F, message = F, echo = F, fig.cap = "Figure 19; depicts power as a function of observed effectsizes in different combinations of trials and subjects. With lines being the dependently fit logistic psychometric functions to each trial by subject combination."}


cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","poweranalysis_powerfit.PNG")), scale = 1)

```

The marginal posterior distributions of the parameters of the winning model are displayed below in figure 20.

```{r Figure 20, fig.width = 7.2, fig.height = 6, warning = F, message = F, echo = F, fig.cap = "Figure 20. Marginal posterior distributions for the winning model's parameters"}

# cowplot::ggdraw() +
#     cowplot::draw_image(magick::image_read(here::here("Figures","marginals_histogram.PNG")), scale = 1)
readRDS(here::here("Figures","marginals_histogram.RDS"))


```

Meaning that the resulting best guess of the underlying function transforming trials, subjects and observed effect sizes into a probability of rejecting the null hypothesis of no difference in threshold is as follows:

$$
\Psi(d_{\text{obs}}, \alpha, \beta \mid t, s) = \frac{1}{1 + \exp\left(-\frac{1}{\beta(t, s)} \cdot (d_{\text{obs}} - \alpha(t, s))\right)}
$$ 

Where

$$
\beta(t, s) = \beta_I \cdot s^{\beta_{1}} \cdot t^{\beta_{2}}\cdot (t+s)^{\beta_{3}}
$$

$$
\alpha(t, s) = \alpha_I \cdot s^{\alpha_{1}} \cdot t^{\alpha_{2}}\cdot (t+s)^{\alpha_{3}}
$$

Where each of these parameters are given by the distributions depicted above in figure 20.

## *Utility of the power analysis*

As alluded to in the beginning section of the power analysis, the work presented here would be able to help an independent researchers determine the probability of rejecting a particular observed effect size using this model, given trials and subjects. However, if this researcher wants to know the probability of rejecting a null hypothesis given that they assume a particular effect size in the population, further assumptions needs to be made. This is because as the effect size when conducting an experiment is not a fixed quantity. In practice this means that when conducting an experiment, an effect size is drawn from the latent effect size distribution in the population. Mathematically this means that the effect size that is observed in an experiment is given by a probability. The mean and standard deviation of this probability density function is given analytically by Cohen which could also be derived from bootstrapping as was done with the measurement uncertainty [@goulet-pelletier_review_2018; @lakens_calculating_2013; @hedges_statistical_2014]. Below are the equations for the mean and variance of the effect size measure used in the power analysis.

$$
\mu_{d_{rm}} = \mu_{\delta}  * \frac{\sqrt{2 * (1-\rho)}}{\sqrt{Var_1 + Var_2 - 2 * \sigma_1* \sigma_2 * \rho}}
$$

$$
\sigma_{d_{\text{rm}}} = \sqrt{\frac{1}{n} + \frac{\mu_{d_{rm}}^2}{2 \cdot n}}
$$
As can be seen the equation for the mean effect size $\mu_{d_{rm}}$ is mathmatically identical to the definition shown in the "Power analysis" section. The standard deviation of this metric $\sigma_{d_{\text{rm}}}$ is defined as a function of the number of subjects n and the size of the effect itself $\mu_{d_{rm}}$. Assuming that the effect size is normally distributed:
$$
d_{obs} \sim N(μ_{d_{rm}} ,\sigma_{d_{rm}})
$$

The probability of rejecting this sampled effect size is given by the function that was obtained above.

$$
P(R \mid d_{obs}) = \Psi(d_{\text{obs}}, \alpha, \beta, t, s)
$$ 

What one ideally wants to know is the probability of observing a particular effect size AND that reject the null hypothesis given this observed effect size. Probability theory and particularly conditional probabilities gives us the relationship between these quantities.

$$
P(R \mid d_{obs}) = \frac{P(R \cap d_{\text{obs}})}{P(d_{\text{obs}})}
$$ 

Here $P(R \cap d_{\text{obs}})$ represents the probability that we are interested in, i.e. rejecting, AND observing a particular effect size.

$$
P(R \cap d_{\text{obs}}) = P(R \mid d_{obs}) \cdot {P(d_{\text{obs}})}
$$

Integrating over all possible values of the effect size is now necessary to essentially integrating out the effect size, also known as marginalizing.

$$
P(R) = \int_{-\infty}^{\infty} P(R \mid d_{\text{obs}}) \cdot P(d_{\text{obs}}) \, d({d_{\text{obs}}})
$$

Which becomes 

$$
P(R) = \int_{-\infty}^{\infty} \Psi(d_{\text{obs}}, \alpha, \beta, t, s) \cdot N(μ_{d_{rm}} ,\sigma_{d_{rm}}) \, d({d_{\text{obs}}})
$$

Instead of trying to analytically solve this integral, one can use the power of the computational resources to approximate the integral by taking draws of the normal distribution of the observed effect size and then putting them through $\Psi(d_{\text{obs}}, \alpha, \beta, t, s)$ which will give draws from a probability distribution of rejecting the null hypothesis. As a last step it is then possible to calculate the proportion of rejected null hypotheses to the total number of draws giving us the power of the study assuming the mean difference and variance in the two sessions.

## *Sampling varability of the effect size.*

The above high-level explanation of calculating power for an experiment might be quite difficult to understand and therefore implement for independent researchers. To make this more accessible I will below demonstrate how this can be done using what has been explained up until this point. These sections will therefore hopefully provide a practical understanding of what different parts should go into a power analysis and how different factors will influence power. Firstly, one needs to investigating how the sampling distribution of the effect sizes changes based on subjects and session by session correlation. I will here assume that the group mean difference of the threshold in the psychometric function is -5 and the variance in the second session is 1.5 times the variance of the first session, i.e. assuming that the intervention increases variation in the threshold, but that there is a clear effect (for reproducibility and ease of use the GitHub repository provides functions that does what is described below by inputting these assumptions).

Firstly, investigating the assumptions for the choice of mean difference and difference in variance can be visualized by repeated sampling from a multivariate normal distribution with the following parameterization:

$$
\begin{equation} 
  \tag{Eq. 1}
  \begin{pmatrix}
  x_i \\
  y_i 
  \end{pmatrix} \sim \mathcal{N}\left(
  \begin{pmatrix}
  \mu_1 \\
  \mu_2 
  \end{pmatrix}, \begin{bmatrix}
  \sigma_1^2 & \sigma_1 \cdot \sigma_2 \cdot \rho_{12} \\
  \sigma_1 \cdot \sigma_2 \cdot \rho_{21} & \sigma_2^2 
  \end{bmatrix} 
  \right)
\end{equation} 
$$

Here $\mu_1$ and $\sigma_1$ are given by the test-retest reliability analysis and were -8, 8 respectively. Given our assumptions $\mu_2$ and $\sigma_2$ are therefore -3, 10. We can then vary the number of subjects i.e. draws from this multivariate normal and the correlation coefficient ρ to see the effect on the distribution of effect sizes i.e. p(d_obs). This highlights the sampling distribution of the effect size, and the factors influencing it. The results can be seen in figure 21 highlights the fact that both the sample size i.e. subjects, but also the correlation between sessions is vitally important for the variances of the observed effect size.

```{r Figure 21, fig.width = 7, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "Figure 21. Sampling distributions of effectsizes across subjects (facets) and session by session correlations (colors)"}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","histogram_observedeffectsize.PNG")), scale = 1)
```

Now we can visualize how these observed effect size distributions fit into the probability of rejecting the null hypothesis i.e. $\Psi(d_{\text{obs}}, \alpha, \beta, t, s)$ Note that the observed effect sizes above are not dependent on the number of trials in the experiment i.e. assuming that they are observed with perfect precision. What the function derived from the continuous power analysis function found above does, is that it incorporates this information, together with other factors that might change the precision of the parameters. As shown above the implications of the function $\Psi(d_{\text{obs}}, \alpha, \beta, t, s)$ can be visualized as psychometric functions in a ($d_{\text{obs}}$ , $\Psi(d_{\text{obs}}, \alpha, \beta, t, s)$) coordinate system with trials and subjects being fixed at values. Another more informative way for investigating varying trials and subjects is to visualize these implications in a 3-dimensional grid of (Subjects OR trials , $d_{\text{obs}}$ , $\Psi(d_{\text{obs}}, \alpha, \beta, t, s)$) with facets of the last variable of either subjects or trials. This visualization can also serve the purpose of projecting the above distributions of observing a particular effect-size unto the space of $\Psi(d_{\text{obs}}, \alpha, \beta, t, s)$ Figure 22 displays the projection of the histograms as ellipse where the vertical width of the ellipse (the major axis) is given by the 95% Highest density interval of the histograms above and the horizontal width (the minor axis) is for visualization purposes, to see the underlying probability of rejection. The correlation is of cause informed by the test-retest reliability study which was found to be 0.54 [0.49; 0.58] for the current model.

```{r Figure22, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "Figure 22. Visualization of how the power of a particular study is informed by the session by session correlation and the number of trials and subjects."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","power_area_with_ellipses.PNG")), scale = 1)

```

## *Practical implementation of the power analysis*

Now turning to a particular example of a researcher wanting to conduct a power analysis utilizing the information provided. Two assumptions have been made, either a mean effect size or a mean difference of the intervention is assumed, and the variance introduced by the intervention. Here we expect a medium effect size of the intervention of $d_{rm} = 0.5$ and that the intervention does not increase variability meaning that the variance in both groups should be equal. To fully appreciate the power of this approach one could even imagine sampling these values i.e. 0.5 and the variances of the second session as random variables and not as point estimates. Using the effect size equations above, it is possible to derive the mean difference and therefore simulate observed effect sizes which are then put into $\Psi(d_{\text{obs}}, \alpha, \beta, t, s)$ and the probability of rejecting that draw is calculated.  Repeating this process over the 4000 draws of the posterior distribution distribution of the parameters of $\Psi(d_{\text{obs}}, \alpha, \beta, t, s)$. and calculating the ratio of rejected to failed rejected null hypotheses gives an estimate of power. 

In the case of not including the sampling varability (prior probability) of the effect size the effect size estimate is just repeatedly entered as 0.5. As can now be seen in figure 23 the observed effect size has been "integrated" out and a grid of subjects by trial span the space of power to reject the null hypothesis. The left and right column of figure 23 quite clearly display the difference between accounting for the sampling process of effect sizes with the left not accounting for the sampling process. As a reference frame in figure 23 the red dashed line at subjects = 20 depicts the results from plugging the same assumptions, here $\mu_1 = -8$ $\sigma_1 = 8$ $\mu_2 = -3$ and $\sigma_2=10$ and $\rho = 0.54$, into the widely used and cited statistical software tool G\*power [@faul_gpower_2007]. Further reiterating how and why uncertainty propagation is vital to designing studies of adequate power as this line seems to be overly optimistic about the number of subjects / trials needed to power the study at 80% at an alpha level of 5%.

```{r Figure23, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "Figure 22."}


cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","power_sampling_varability_and_gpower.PNG")), scale = 1)

```

\newpage
