# Uncertainty minimization

## *Adaptive design optimization*

An import consideration for parameter recovery, is the design of the experiment that the agent goes through. Referring back to figure 4, providing stimulus values in the far ends of the psychometric functions, i.e. in the ranges of [-50 ; -25] and [25 ; 50] will in most cases, give limited information on the shape of the psychometric. This could entail thatthe threshold and slope could be more informed by sampling stimulus values in the [-25 ; 25] range, see supplementary note 2 for discussion on the lapse rate. Therefore, selecting the input stimuli, in this interval could be assume to be better for decreasing the estimation uncertainty in the two parameters, compared to randomly or uniformly exploring the input space. One might even go a step further; instead of selecting inputs that are more appropriate based on the mean of the population, each experiment could be individualized to each subject.

This practice of individualizing the experiment is called adaptive design optimization (ADO). The concept revolves around selecting inputs that are optimal given a specific criterion [@watson_quest_2017; @prins_psi-marginal_2013]. Many of these criterion exists, including minimizing entropy, minimizing the posterior variance or maximizing mutual information. What they all have in common is that they decrease estimation uncertainty, either of all or certain parameters. One of the main challenges of utilizing ADO is that the experiment has to be updated and individualized on a trial by trial basis. In the extreme this would requires the algorithm, to run in tandem with the experiment. This puts significant constraints on the computation time of the algorithm. This particular issue has been partly solved in the existing packages by calculating a grid, of a particular resolution, of parameter values before the initialization of the experiment. This solution puts the heavy computation time before the experiment, ensuring that when the experiment is run, only a single look up is needed to provide the next stimulus value on each trial. This approach works great for experiments where each trial is independent of the next, like in a psychophysical experiments. However, if trials were mutually dependent as in a learning experiment, then the algorithm would need to calculate all possible lines of stimuli and responses, up until a certain point. This dependent structure would therefore become a daunting task, due to the combinatorial complexity. I will describe how an ADO can be implemented, utilizing the single-subject model, which was used for the single-subject parameter recovery. The goal of demonstrating how easily such an ADO can be implemented is to show and examine the flexibility in the cognitive modeling framework. The advantages of being able to write such a custom algorithm is two-fold. First, as long as the model can be written to invert observed data to parameter values (i.e. fit a model to data), then it can also be used to simulate stimulus values, this therefore increases the flexibility. Secondly, as this approach is not "optimal" for stimulus selection, the method can be extended to mutually dependent experiments. Building such an ADO can be done using variational inference algorithms. These algorithms can quickly estimate an approximate posterior distribution of the parameters of interest, here the pathfinder algorithm implemented in Rstan is used [@zhang_pathfinder_2022]. This algorithm locates normal approximations to the targeted density of the posterior distribution with its quasi-Newton optimization. Using this approximate normal, the pathfinder algorithm draws samples from it to provide approximate posterior samples. The rationale behind this approach to ADO is to iteratively fit the model as responses from the participant is collected. The parameter estimates from the model are then updated, and a new stimulus value is then selected based on these estimates, together with the knowledge of which stimulus values are the most informative for particular parameter values. For a full description of how the pathfinder algorithm was implemented, see supplementary note 3.


Figure 8 shows how the posterior distribution of the three parameters of the PF varies as a function of trials. This is visualized by using the previously uniform selection of stimulus values and the implemented pathfinder approach. As can be seen, both approaches makes the parameters converge towards the real simulated values (black line), with increasing trials. However, the speed towards convergence is quite different, especially for the two parameters that the pathfinder algorithm is optimizing for, alpha and beta. After just 20 trials, using the pathfinder optimization, these parameters have found the simulated parameter value and is decreasing their estimation uncertainty (95% credibility interval). In contrast, even after 50 trials the uniform approach still has a bias in the estimation (the individual points are not on the black line), but also a substantial estimation uncertainty associated with it.  For completeness, a PSI-algorithm was also used to compare the feasibility of this new approach due to the high constraint on computation time [@kontsevich_bayesian_1999]. Interestingly, the pathfinder algorithm completed the 50 trials in 14 seconds, whereas the PSI algorithm took 30 seconds. This highlights the feasibility of this approach, as experimental designs have to be run relatively quickly, in order to keep the attention of the subject [@kwon_adaptive_2023].

<!-- pathfinder vs psi vs uniform plot -->
```{r figure 8, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "**Figure 8 comparison of algorithms to obtain stimulus values of the psychometric function.** Columns and colors, display the three parameters of the psychomrtic function, while rows depict the (adaptive) algorithm used to obtain the parameter estimates. The figure displays how the Pathfinder algorithm quickly converges to the simulated value (black line) for the three parameters alpha, beta and the lapse rate. This is in contrast to the two other methods that take more trials to converge."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","Pathfinder.PNG")), scale = 1)
```

To more formally show the differences in these ways of selecting stimulus values across a range of trial numbers, the algorithms were run 100 times. This was done for trials ranging from 20 to 100 in a sequence of 10 trials (using the same range of parameter values, as depicted in table 1). In order to ensure fair comparison, each algorithm were only used to generate the stimulus sequence. This meant that data-sets were refitted using the same single fit Bayesian model used for the single fit parameter recovery, ensuring the same priors for all models. The inputs for the following analysis were therefore, the posterior distribution of these refitted parameter values. For complete details on the fitting and optimization strategy, see supplementary note 3 and 4, including prior initialization for PSI and Pathfinder. Figure 9, shows the results of this simulation, with the top panel showing the bias, i.e. the difference between the estimated and simulated parameter values and the bottom panel the uncertainty in the estimated parameter value. Interestingly, overall the PSI-ADO performs the worst both in terms of bias in the slope (beta) and lapse rate (Lambda) parameter, and especially in the estimation uncertainty for all parameters. The main difference between the uniform and Pathfinder approach appears in the estimation uncertainty, especially in the threshold (alpha), where estimation uncertainty is significantly lower for all trial numbers.


<!-- pathfinder vs psi vs uniform plot over many iterations-->
```{r figure 9, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "**Figure 9.** shows how the estimation bias (upper row) and uncertainty (lower row) changes according to the number of trials (x-axis) and parameter value estimated with the different methods of selecting the input stimulus (colors)."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","Pathfinder_bias_estimation.PNG")), scale = 1)


```

Having tested and compared the pathfinder algorithm, one can now examine three other focal points of minimizing estimation uncertainty, namely subjects, trials and the influence on the mean simulated slope value. The last point is less obvious than the others, but stems from the fact that increasing the slope (decreasing the steepness) of the PF will make it harder to estimate the parameters of the function. This means that estimation uncertainty is increased if other factors are held constant, the reason for this will become clear below. The number of subjects might also influence estimation uncertainty, as the partial pooling effect of the hierarchical model will be stronger with more subjects. To investigate these three focal points, trials ranging from 20 to 200 in increments of 20, subjects between 10, 30 and 50 and lastly mean slope values of 1,2 and 3 in the unconstrained space are simulated. All other parameter values being identical to table 1. To guard against simulations that are not representative, due to either bad convergences in the ADO or in the fitting procedure, each combination was run five times. Figure 10 displays the result of this parameter recovery, across trials and group mean slope levels  (i.e. simulated beta values). Figure 10 only displays the correlation approach with the inclusion of estimation uncertainty, in the upper panel, and the developed $ICC_2$ in the lower panel. For the two other metrics i.e. the correlation without proper uncertainty propagation and the $ICC_1$, see supplementary figure 3. Due to the limited influence of subjects, these have been aggregated in Figure 10. Supplementary figure 4 and 5 displays the individual subject numbers simulations separately.
 
```{r figure 10, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "**Figure 10 comparison of parameter recovery metrics, across trials and simulated steepness of psychometric function.** First row depicts how the correlation coefficent between simulated and estimated means change as a function of trials (x-axis) and the simulated mean slope (color) for each parameter of the psychometric function (columns). The bottom row shows how the estimate of $ICC_2$ changes based on the same metrics as the correlation coefficient. Note that the correlation coeffecient has been uncertainty propergated using bootstrapping."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","ICC_vs_correlation.PNG")), scale = 1)
```

From Figure 10, the main differences between the two approaches are that the ICC metric is generally lower, than the correlation approach. Both approaches do asymptotically move towards one, with increasing trials and/or simulated mean slopes of the psychometric function. One way to highlight the difference, and significance of this difference, is to plot the pairwise scatter plot of simulated vs recovered parameter estimates. These pairwise scatter plots are what both metrics in Figure 10 attempt to describe. Picking the instances where the difference between the correlation and ICC approach is the greatest, will give insight to which metric might be more suitable. Figure 11, shows the pairwise scatter plot of the threshold (alpha) in three selected trials (40, 120 and 200) for both steep and shallow slopes (means of 1 and 3, respectively for beta). These instances were chosen, because the CC and ICC were similar for the steep slopes, but remarkably different with shallower slopes. Figure 11, clearly shows why there is such a difference between the two metrics. The ICC metric is penalized considerably more by the increased estimation uncertainty and the deviation from the identity line.  This is especially evident in the threshold when shallower mean slopes are used. This observation indicates that the ICC metric is more sensitive to the uncertainty, compared to the CC. The same reasons apply for the difference in the slope estimate itself, and pairwise scatter plots can be found in supplementary figure 6. Lastly, both approaches suggest that the lapse rate is below the two other metrics, without much improvement with increasing trials, but with the ICC being more conservative. 

```{r figure 11, fig.width = 7.2, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 11.** Showing the pairwise scatter plots of simulated vs recovered threshold (alpha) parameters, when the simulated slope (beta) value is low (beta = 1) and high (beta = 3) for trials (columns)."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","scatter_plot.PNG")), scale = 1)
```


As conveyed by the pairwise scatter plots in figure 11, the conservative ICC metric capture the fact that estimation uncertainty is a source of variability, which can be reduced, even when the CC suggests a close to perfect fit. This aligns with the behavior one would like when trying to understand and validate their model. Furthermore, the values of the ICC have a natural interpretation, as the ratio of between subject variance to total uncertainty, whereas for the CC the interpretation is not straightforward. This means that an ICC value of 0.8, indicates that 80% of the variance in the model, is governed by the between subject level variance, and therefore only 20% is in the estimation or test -retest uncertainty. The ICC could be further decomposed into these constituent parts to explain what is deriving these last 20%, see supplementary figure 3. This straightforward and nuanced interpretation is not present for the CC, especially because of the arguments laid forth in the “Current problems with internal recoverability of models” section. Another import considerations, sometimes neglected in parameter recovery, is the hierarchical structure, as mentioned in the previous section [@hess_bayesian_2024; @hubner_improving_2020; @harrison_interoception_2021]. Lastly, this approach highlights that parameter values, in a cognitive model, do not necessarily have to improve with increasing trials. This is the case for the lapse rate (lambda parameter) in this particular PF, and could perhaps have been improved if the ADO algorithm was build for estimating this parameter. Nevertheless, mindlessly increasing the number of trials to hopefully decrease estimation uncertainty on a parameter, should only be done after having conducted such an analysis. This would be in order to ensure that resources are not wasted trying to decrease estimation uncertainty on a parameter, to a degree that is not possible, even in principle. 

## **Increasing information in cognitive models**

The previous section highlighted how the number of trials and the group level slope, but not the number of subjects, could influence the parameter recovery metric. In this section, it will described how using data and/or information about the underlying experiment, can reduce estimation uncertainty further. Here, the incorporation of reaction times of the agents’ responses are going to be used, these will serve as sources of information about the underlying PF. The focus on the reaction times is twofold; first they have a long and rigorous history in the cognitive science literature, but more importantly, are present in many experiments conducted today [@sternberg_memory-scanning_1969; @pirolli_role_1985; @macleod_training_1988; @hess_bayesian_2024; @legrand_heart_2022]. 


To incorporate the reactions times into the current formulation of the generative structure of the task, it is helpful to think of the output of PF as a probability of responding 1. Thus, in either end of the tail of the PF, the certainty with which you respond is the highest, and the midpoint between the extremes (the threshold) is the most uncertain. This descriptive formulation is what the variance of the Bernoulli distribution describes, which is the distribution that converts the probabilities from the PF, to binary choices.

$$
Var(Bern(p_t)) = p_t \cdot (1-p_t)
$$
Here $Var(Bern(p_t))$ is the variance of the Bernoulli distribution at $p_t$, which is the probability of responding 1 at trial t.

Using this information, together with the assumption that participants will respond slower when more uncertain and faster when certain, one can model the reaction times as a linear combination of this Bernoulli variance. This linear combination is thus formalized as an intercept, to account for the individual differences, and a slope that scales the influences of the uncertainty to the variances of the underlying PF. Mathematically this would entail.

$$
RT_t \sim intercept + \beta_{RT} * Var(Bern(p_t))
$$

where RT is the reaction time at trial t, intercept represents the intercept and $\beta_{RT}$ represents the degree to which the uncertainty from the psychometric function influences the reaction times. Figure 12, shows a visualization of this mapping. 

In order to stochastically model the reaction times with this formulation, a probability density function is needed to account for the noise in reaction times observed. Due to the non-negative nature of reactions times, and physical constraints of information processing (i.e. a delay from the time the stimulus is presented to which it reaches the brain of the agent), a sensible choice of this probability density function would be the shifted log normal distribution. This introduces two more variables, a non decision time ($\tau$) and a standard deviation ($\sigma$) for the log normal distribution [@ranger_modeling_2020; @jain_comparative_2015]. This formulation of the reactions times follows the mathematical relationship described below, where the crucial link between the psychometric function and the reaction times is the Bernoulli variance.

$$
RT_t \sim LogNormal(intercept + \beta_{RT} * Var(Bern(p_t), \sigma) + \tau
$$

To show how incorporation of these reaction times could help with recovery of the parameters, agents with the parameter values displayed in table 2 were simulated. 


```{r Table 2, warning = F, message = F, echo = F}
table2 = read.csv(here::here("tables","table2.csv")) %>% mutate(X = NULL)

table2$Parameter = c("Alpha","Beta","Lambda","Intercept","BetaRT","Sigma","Non-decision-time")

table2 = flextable::flextable(table2) %>% flextable::width(c(1), width = 1.3) %>% 
  flextable::width(c(4), width = 1.8) %>% 
  flextable::width(c(2:3), width = 1.2) 


table2 = set_caption(table2,
  caption ="Table 2: Parameter distributions for reaction time simulations. Parameter distributions for the simulated agents and the transformations for each of the parameters when including the reaction times in the psychometric function.")

table2
```

```{r figure 12, fig.width = 7.2, fig.height = 5, warning = F, message = F, echo = F, fig.cap = "**Figure 12. Visualization of the psychometric function with Reaction times.** Upper panel depicts 10 psychometric functions where parameters were drawn from table 2 (Beta = 3 and BetaRT = 1.5). Lower Panel depicts the assumed relationship between the stimulus value (x) and the reaction times (y), which as can be seen is dependent on the shape of the psychometric function in the upper panel. The reaction time functions peak around the psychometric threshold and tapers off when the psychometric function asympotes at 0 or 1."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","plot13_psychometric_RT.png")), scale = 1)
```


To understand the influence of the size of coupling between the PF and the reaction times ($\beta_{RT}$), this parameter was simulated with either a high or low group mean, 1.5 and 1 respectively. The steepness of the slope of the PF was also varied between high and low, 1 and 3 respectively. These particular values were used after having simulated and visualized the implication of them. This can be seen in figure 12, where ten simulated subjects are visualized. The figure clearly shows the relationship between the PF and the reaction time function. At high stimulus values, i.e. the most extreme x-values, the reaction times are fast and the psychometric function is approaching 0 or 1. As the PF increases from very low stimulus values (the left side), the reactions times increase up until the threshold for that agent is reached, and then the reaction times decreases again. 

Next, utilizing this model, a parameter recovery analysis can be conducted that investigates the influence of these reactions times on the recovery of the parameters. Here only the $ICC_2$ is depicted for the 8 combinations of slope, size of the RT coupling, and inclusion of reaction time is depicted. Similar results were obtained by using the CC, which can be seen in supplementary figure 7. Figure 13 displays the difference in parameter recovery between inclusion of reaction times in the modeling, on the 3 parameters of the psychometric function. The plot highlights increased $ICC_2$ values for the two parameters of particular interest, i.e. the threshold (alpha) and the slope (beta). This difference is particularly present in the slope parameter for both slope conditions (i.e. steep and shallow simulated mean slopes i.e. 1 and 3), but also in the shallow simulated slope (beta = 3), on the threshold. In these conditions the $ICC_2$ metric has not reached its asymptote of 1, as is the case in the steep slope simulation on the threshold. This means that inclusion of the reaction time can reduce estimation uncertainty in the slope and threshold parameter.

```{r Figure 13, fig.width = 7.2, fig.height = 7.2, warning = F, message = F, echo = F, fig.cap = "Figure 13 Parameter recovery of the psychometric function for the Intra class correlation for each parameter (columns), in each combination of including and not including rts and its size (color), and the simulated mean slope (rows) for differing number of trials x-axis. Stronger coupling is associated with greater intra class correlation values for both threshold slope, with a strength dependent association i.e. the higher the coupling strength the more improve recovery when comparing to not uterlizing reaction times."}

cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","ICC_rtplot.PNG")), scale = 1)
```
