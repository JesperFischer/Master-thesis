# Discussion

The thesis has investigated improvements in uncertainty handling in the field of cognitive science, particularly in the developing field of cognitive modeling. This was done with the use of simulations, highlighting that a deep mathematical understanding is not necessary to understand and/or do calculations with uncertainties. The thesis outlined three types of uncertainty; measurement uncertainty being the lowest level, often overlooked or disregard in the field, despite of the unpredictable influence it can have on the resulting statistical metrics. Researchers should firstly be aware of measurement uncertainty, and examining the extent to which it can be safely ignored in their statistical models. Even in measures like reaction times, commonly used in Cognitive Science [@sternberg_memory-scanning_1969; @macleod_training_1988; @pirolli_role_1985], measurement uncertainties are present, and depend on the soft and hard-ware the experiment [@holden_accuracy_2019; @crocetta_problem_2015; @ohyanagi_solution_2010]. 

Estimation uncertainty, introduced as the uncertainty associated with doing computations, often displayed as the standard error of statistical metrics. The main focus of the thesis was to investigate this type of uncertainty, in the field of cognitive modeling and revise some of the statistical metrics used to validate a particular models. This was demonstrated using a psychometric function, that maps stimulus values to probabilities by three parameters the threshold ($\alpha$), slope ($\beta$) and ($\lambda$). It was argued that the statistical metric commonly used, the correlation coefficient, between simulated and recovered parameters values was not a sensible metric to determine the extent of internal model validity [@schurr_dynamic_2024]. Two important aspects of the correlation coefficient made it insensible for internal model validity. Firstly, the decision of choosing what size of correlation coefficient should be deemed enough, is not straightforward, because the interpretation of the correlation coefficient itself in the regard of model validation is not straightforward.  This is particularly true when highlighting that the correlation coefficient is invariant of a linear transformations. Secondly, it was shown that in instances where the simulated and recovered parameter values did show good dependency, the correlation coefficient rapidly approached an asymptote at 1. This occurred when more information could be gained by increasing the number of trials, demonstrating its limited inclusion of the estimation uncertainty. The thesis therefore suggested using a variant of the intra class correlation coefficient (ICC) as the statistical metric for examining internal model validity, as recently proposed in the literature [@schurr_dynamic_2024]. 

It was shown that the ICC metric was more sensitive to estimation uncertainty in the parameters, with a sensible interpretation of the ratio between desirable and undesirable uncertainty. With this new metric, the thesis explored ways to decrease the undesirable uncertainty and thereby increase the ICC metric. Two particular ways was investigated, revolving around either incorporating smart experimental designs that are optimized for each individual or incorporating reaction times into the cognitive model. These methods are neither mutually exclusive or incompatible and could be implemented in experiments to decrease estimation uncertainty, in the parameters of the psychometric function.



The second approach of jointly modeling several dependent variables and their interactions, has been incorporated in the cognitive modeling literature for quite a while, however is now slowly re-gaining traction [@stone_using_2014; @pedersen_drift_2017; @hess_bayesian_2024]. What these two methods have in common is that they do not increase the efficiency of the study by increasing trials, which is usually is the default for decreasing estimation uncertainty of subject level parameters. 


The obvious problems with increasing the number of trials are resource costs. This is both in terms of money, but also in the time spent for the participant and the experimenter. From an ethical perspective, this is especially true of the time investment from the participants' side, and particularly when patient populations are investigated. However, the most problematic aspect with mindlessly increase the number of trials, becomes more obvious when we carefully consider what we are studying. In Cognitive Science, we are studying a complex system that has its own goals, desires and motivations, and it is not trivial to know how this participant will behave if the task is twice as long. Will the participant employ a different strategy, knowing that the experiment is going to take longer, or will they halfway through the experiment employ a different strategy, due to boredom. Even if participants keep the same underlying cognitive strategy, that we are trying to model, then one would still expect that attentional lapses increase and overall engagement in the task to decrease. This would entail that each additional trial, perhaps after a certain point, would be less informative. 
The thesis went on to investigate the last type of uncertainty, test retest uncertainty, by re-examining a data-set from a test-retest reliability study. Here it was showed that a re-analysis of the original data, could achieve better test-retest reliability. This was done by incorporating knowledge about the structure of how the data was gathered, together with incorporating information already represent in the data i.e. reaction times. 


The re-analysis was then used to exemplify of how a power analyses of a cognitive models can be conducted. This was achieved by simulating and then fitting the cognitive model to different observed effect sizes in combinations of different trials and subject. This approach allowed for modelling of the latent power curve, relating observed effect size, trials and subjects to the probability of rejecting a null hypothesis, in an experiment. Using posterior predictive checks and leave one out cross validation, a particular power law related the parameters of the power curve, to subjects and trials with good predictive abilities. With this analysis it was shown that the number of trials in an experiment, can be added to a power analysis, which is not standard practice in widely used statistical software tools like G\*power [@faul_gpower_2007; @ioannidis_why_2005; @aarts_estimating_2015]. This power analysis showed that G\*power's estimation of sample size for the purposed test, was more liberal requiring 25 subjects, whereas the full uncertainty propagated power analysis, based on simulations from the fitted model, suggested approximately 30 subjects. Crucially, this was only the case if the number of trials were larger than at least 150.


The rest of the discussion of the current thesis will revolve around the implications of improperly accounting for uncertainties in the science and how this might be a contributing factor to the replication crisis.



## **Power analyses, certainty and replication crisis.**

In recent years, some scientific fields especially psychology, social science and medicine, have been under scrutiny due to a lack of and failure of replication of previous studies [@wiggins_replication_2019; @forbes_chapter_2023]. Many contributing factors has been identified, such as publication bias and questionable research practices (QRP). These QRP, invovles p-hacking (conducting statistical analyses until significant) or HARKing (hypothesizing after the results are known) [@head_extent_2015; @kerr_harking_1998]. 

A quite paradoxical aspect of the replication crisis is the use of power analyses, which are advised as a mean to increase replicability. The argumentation of conducting power analyses before data collection, is that many studies in social science generally have low to very low power, to detect a small to medium effect size [@felix_singleton_statistical_2023]. Power analyses are therefore promoted, to ensure sufficient statistical power to detect the size of the effect of interest. The argument is sound, as long as the analysis of statistical power is accurate or accurate enough. What this thesis has highlighted, is that the use of very popular tools like G\*power, for conducting these types of power analyses, underestimate the number of subjects needed, by not including the effect of trials in the estimation process. Therefore the assumption from above might be misleading and problems might arise where researchers have too much confidence in their experiment, due to having conducted a power analysis, than is actually justified, hence the paradoxical aspect. This mimics the false sense of certainty on measurements, that are assumed by these popular softwares or measurements in Cognitive Science in general. Therefore, instead of increasing replicability and certainty in the effects observed, utilizing these tools might paradoxically decrease them, as researchers might be tricked into conducting less powered studies, due to the recommendations of the software.

### *Ways of combating the replication crisis.*

A significant number of scientists have suggested to move the arbitrary statistical significance threshold from 0.05 to 0.005, as a means to combat the replication crisis [@benjamin_redefine_2018]. Interestingly, lowing of the statistical threshold for significance would, in principle, lead to the conclusions drawn from this thesis of including and propagating uncertainty. The comparability of these two approaches depends on the structure and uncertainty of the data. However, in most cases including and propagating uncertainties, would have the effect of lowering the resulting statistic and therefore increase the resulting p-value. These two approaches, i.e. increase the statistical significance threshold or properly propagating uncertainty, have very different reasons, even though they share the same goal. Lowering of the significance threshold would be a means to an end, instead of addressing the underlying problems, which the authors also do acknowledge [@benjamin_redefine_2018].



Another interesting idea that coincides with the general theme of the thesis, and to combat the replication crisis, is that of preregistration, registered reports, and blind analyses [@maccoun_blind_2015; @klein_blind_2005; @evans_improving_2023; @chambers_past_2022]. The common theme of these interventions is that they acknowledge the subjectivity not only the data collection, but also in the data analysis pipeline. This subjectivity is both what introduces biases, but also what drives novel ideas, meaning that it becomes a trade-off between exploration and exploitation. This trade-off needs to be addressed, in order to partly guard against unwanted subjectivity. The interventions guard against this unwanted subjectivity by have the analysis pipeline either fixed before data collection or scambling the data such that the results of the analyses, cannot be known when producing the analysis pipeline. The rigorous checking, testing and validating of cognitive models shown here is not at stake with these interventions, but instead facilitates them. This is due to the fact that most of the checking, testing and validation should be done on simulations. 


However, there are still considerations when analyzing experimental data, especially on the model convergence side, where in or excluding covariates or reparameterization of the models might be necessary. In this regard, the blind analysis intervention might be a valuable insight from physics. Here the experimental data is scrambled in various ways, such that models and analysis pipelines can be done on data that resembles the collected data, but without being able to know the results before the data is un-blinded [@maccoun_blind_2015; @klein_blind_2005]. Decisions are therefore made on scientific justifications, instead of on completely subjective criteria that could make the experimental results fit a research paradigm or perhaps even worse, produce significant results, were none are present. While the distinction between decisions based on scientific justification and subjective nonsense may be fuzzy and narrow, interventions, like those described, can help mitigate unwanted incentives such as publishing pressure and the temptation to fit results to a particular research paradigm or hypothesis [@quaia_finding_2022].  This approach could give rise to more rigorous methods and analysis pipelines, as it hinders arbitrarily stopping the development of the pipelines, when the results fit the preconceived notions of a scientific paradigm. Instead, it forces researchers to stop only when they are satisfied with the assumptions and implementations made. This process might also help researchers understand the uncertainty that is associated with many of the methods or practices commonly used in the literature. 

## *Why and how computational tools are becoming vital in science.*

Cognitive or even computational modeling could serve as a fresh start needed in the sciences that has been troubled by the replication crisis. The more sophisticated models embedded in these frameworks, might be the steppingstone to engage in more theoretically driven analyses, hopefully reducing the number of non-reproducible studies. However for this movement to succeed, it is essential that rigorous metrics are enforced to assess the model's internal validity. Models lacking any type of internal validity or identify-ability can therefore be discarded from the beginning. There may be intances where mathematical formulation of theories are developed, but that in practice the formulation is computationally intractable. It would be a shame to spend years investigating these kinds of models, and their implications in a field of research, only to discover that the model is intractable in practice [@ho_cognitive_2021; @mcclelland_place_2009; @zuidema_five_2020]. One might think that a deeper mathematical understanding is a necessity for understanding and building these more complicated models. However, what this thesis has argued is that this is not necessarily the case, simulations allow researchers to observe the implications of their assumptions. Another argument for why more sophisticated models are not necessarily off the shelf due to high level mathematically understanding is the increase in adaption of sophisticated hierarchical models which are mathematically much more complex than single level models, yet they have been widely adopted in the literature [@dedrick_multilevel_2009].


This is not to say that a better understanding of the machinery and mathematics itself would not be beneficial, but perhaps no longer a necessity. This would therefore also imply that the way that statistical methods and tools are taught might need to change. In fields where mathematical methods are not commonplace, students and researchers could be taught statistical methods with the use of coding and simulation examples, instead of flowcharts for which statistical analysis to conduct when. This would involve providing individuals with the tools for understanding and reflecting on these statistical models and their assumptions. This is similar to how a good scientific program does not merely teach students the right theories or hypotheses, but rather teaches them to think in a scientific way, such that the individual can decide and test these themselves. In this context the tools for understanding, reflecting and experimenting with statistical models and concepts, would be programming experience in statistics. This would allow the researcher to more concretely grasp the assumptions that are being made, but would also provide the tools for examine what happens when they are broken. Moreover, this framework would also necessitates a more generative approach to modeling, making the researcher more closely engaged in the statistical process of analyzing the data, instead of just picking an off the shelf model from a flowchart [@velarde_camaqui_flowchart_2023].


## *Standing on the shoulders of giants*

All of the models used in the current paper were fitted using Stan with the cmdstanr interface, which uses full Bayesian statistical inference with Markov chain monte carlo sampling [@R-cmdstanr]. As described in the section about modeling definitions, fitting and building models in this framework is extremely flexible. An additional benefit of this framework is that the code for simulating the generative process is close to identical in nature, to the code that specifies the model. This similarity makes it easy for users with a generative framework to code up these types of models. The additional benefits to using especially Stan and the its Hamiltonian Monte Carlo (HMC) algorithm is that when issues arise the algorithm will complain. This helps reduce the risk for erroneous inference, due to the sampling algorithm or typos in the code [@vehtari_rank-normalization_2021]. 

The thesis used Bayesian inference and Stan, due to its flexibility in model formulation, rather than the inherent differences between Bayesian and frequentist statistics. However, Bayesian inference does allow for a more optimistic way to interpret the replication crisis discussion above. Instead of starting each experimental analysis from the perspective that nothing or very little is known about the parameters of interest, perhaps incorporating information from previous studies would be beneficial. This is what the priors in the Bayesian inference scheme identifies. This is in essences what science is about, a hierarchical organization of knowledge, where each step rests on the step below, i.e. on auxiliary assumptions as put by the Duhem--Quine thesis [@ariew_duhem_1984]. Here priors can be thought of as in the top level of the hierarchy, that then informs the lower level implications, but that the strength and location of these priors are informed by lower levels, i.e., emperical evidence. This view on science also matches that of uncertainties, as these are also hierarchically organized. So in the same way that the results of a scientific theory is only as strong as its auxiliary assumptions; the strength of an analysis, that builds on a theory, is also only as strong as the (un)certainty of the data.

What the Bayesian inference allows, is that prior information from similar studies can be used in the modelling, allowing researchers to not start their scientific studies from scratch, but pick up where others left off. This would essentially entail, instead of collecting a larger number of subjects to achieve the a desired statistical power, this could be done by two independent laboratories. Here we can think that the second lab, uses the information provided by the first in their priors. This essentially is already what is being done when conducting meta-analyses. This approach incentives publications of all types, as the findings of one researcher would serve as a stepping stones for the next, making the problem of publication bias, where null findings are unpublished, less incentivized [@laitin_reporting_2021].

## *Limitations*

One of the main focal points of the thesis was investigating the correlation coefficient as a statistical metric for internal model validation of cognitive models. It was shown that a modified version of the intra class correlation was more sensible as a statistical metric for model validation. However, due to limitations on time and computational resources, no analysis was conducted, comparing how these statistical metrics to the power analysis displayed in Figure 20. Future studies should investigate the link between how these metrics behave and compare them to the power analysis conducted. A clear link between these quantities would make the need for conducting a power analysis superfluous, as one could imagine that the information for a power analysis could be contained in the validation analysis. A thorough investigation of this link would mean that the somewhat arbitrary choice of trials, when designing an experiment, would no longer be arbitrary. It would instead be informed by how estimation uncertainty in the parameters of interest, change based on the number of trials [@miller_how_2024].

Another limitation of the current study is the limited power analysis conducted. Ideally, the thesis would have investigated, other parameters of the psychometric function. A particular interest would be on the slope of the psychometric function, as it was shown that changes to this parameter changes the estimation uncertainty of all the other parameters. One might suspect that an intervention that increases the steepness of the slope, would also make it easier to detect a change in the threshold. As both the correlation coefficient and ICC metric showed that with increased steepness of the function, less estimation uncertainty was present in the threshold. This highlights how the parameters of the model interacts, which can be accounted for by performing these simulations. Therefore, future investigations should expand upon this power analysis to include other parameters, especially the slope of the psychometric function. 

Future studies should also investigate how incorporating the reaction times, into the power analysis would change the statistical power function. This research would not only help elucidate the question posed above, about the relationship between the internal model validation metric, trials and power, but could also give an estimate of the increased efficiency pf incorporating information already present in most experiments. The reasoning for only conducting the single power analysis on the threshold in the current thesis highlights one of the main hurdles of the framework purposed: computational resources. Fitting models using HMC and Bayesian inference is both time and computational resource intensive, compared to frequentist inference in packages such as lme4, lmertest or GAMLSS implemented in R [@stasinopoulos_generalized_2008; @kuznetsova_lmertest_2017; @bates_fitting_2015]. This additional invested time for doing computation can partly be negated with an access to bigger machines. Here parallelization of the computational burden, especially when several chains are needed to ensure convergence, is essential. Fortunately the access to bigger machine, both privately but also on an institutional level, is something that is growing in accessibility and already available to many universities or research centers. This increase in computational availability has also been correlated with research competitiveness [@apon_high_2010].

\newpage
