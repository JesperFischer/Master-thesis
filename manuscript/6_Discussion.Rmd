# Discussion

The thesis has investigated how the handling of uncertainty in the field of cognitive science and especially in the developing field of cognitive modeling can be improved. The thesis has done this by demonstrating that using computational resources in the form of simulations, a deep mathematical understanding with rigorous closed end solutions is not necessary to get an understanding of how uncertainties on each level can and will influence every statistical metric. The thesis outlined three critical types of uncertainty; measurement uncertainty being the lowest level of uncertainty that is often completely neglected in the field, even though it influences the resulting statistical metrics in very unpredictable ways. Researchers should firstly be aware that measurement uncertainty is always present and examining the extent to which it can be safely ignored in their statistical models. Even in measures like reaction times which is commonly used in cognitive science [@sternberg_memory-scanning_1969; @macleod_training_1988; @pirolli_role_1985], measurement uncertainties are present, which depending on the soft and hard-ware the experiment might be a factor to account for [@holden_accuracy_2019; @crocetta_problem_2015; @ohyanagi_solution_2010]. Estimation uncertainty was introduced as the uncertainty associated with doing computations and is often displayed as the standard error of statistical metrics. The main focus of the thesis was to investigate this type of uncertainty in the field of cognitive modeling and revise some of the methods and metrics used to validate one of such models. This was done using a psychometric function that maps stimulus values to probabilities by three parameters the threshold ($\alpha$), slope ($\beta$) and ($\lambda$). It was argued that using correlations, which has been used in many previous studies [@schurr_dynamic_2024], between simulated and recovered parameters values was not a particularly sensible metric to determine the extent of internal model validity. Two important things about the correlation coefficient made it insensible for internal model validity, the decision of choosing what size of correlation coefficient to deem model parameters sensible is not straightforward, because the interpretation of the correlation coefficient itself in the regard of model validation is not straightforward. This is particularly true when highlighting that the correlation coefficient is invariant of a linear transformations. The other reason was that in instances where the simulated and recovered parameter values did show good dependency the correlation coefficient rapidly approached the asymptote at 1. This was even the case when more information could be gained by increasing the number of trials, due to its limited inclusion of estimation uncertainty. The thesis therefore suggested that using a variant of the intra class correlation coefficient (ICC) as the statistical metric for examining internal model validity which has recently been suggested in the literature, would be more sensible [@schurr_dynamic_2024]. The thesis found that this metric was much more sensitive to estimation uncertainty in the parameters, with a sensible interpretation of the ratio between desirable to undesirable uncertainty as its interpretation. With this new metric the thesis explored ways to decrease the undesirable variance and thereby increase the ICC metric for the psychometric function's parameters. Two particular ways was investigated which revolved around, either by incorporating smart experimental designs that are optimized for each individual on a trial-by-trial basis or incorporating reaction times into the cognitive model describing how stimulus intensities are transformed to binary forced choices. These ways are neither mutually exclusive or incompatible and could easily be implemented in experiments to decrease estimation uncertainty in the parameters of the psychometric function without a need for more trials in the experiment itself. The second approach of jointly modeling several dependent variables and their interactions, has been around for quite some time but only now is slowly gaining traction in cognitive science literature [@stone_using_2014; @pedersen_drift_2017; @hess_bayesian_2024]. What these two ways to optimizing either experimental design and or analysis have in common is that they do not increase the efficiency of the study by  mindlessly increasing trials as this could in principle have troublesome consequences in non-obvious ways. The obvious problems with increasing the number of trials are resource costs, both in terms of money, but also in the time spent for both the participant and the experimenter. From an ethical perspective this is especially true of the time investment from the participants' side, and perhaps even more so if patient populations are investigated. However, the most problematic aspect becomes more obvious if we take a step back and think carefully about what we are studying. We are studying a complex system that has its own goals, desires and motivations and it is not trivial to know how this participant will behave if the task is double the length. Firstly, will the participant employ a different strategy knowing that the experiment is going to take X time longer, or will they halfway through employ a different strategy because of boredom. Even if the participant keeps the same underlying cognitive strategy that we are trying to model, then one reasonable assumption would be that attentional lapses and engagement in the task will decreasing, making each additional trial after a certain point less informative. The thesis highlights how the above implementations and considerations do not necessarily have to rest on heavy mathematical understandings and proofs as computational resources and especially simulations has made it possible for people with coding experience to gain these insights by the power of (re) sampling; some of the implications of this will be discussed below. The thesis went on to investigate the last type of uncertainty test retest uncertainty by exmaining a data-set from a test-retest reliability study. Here it was showed that a re-analyzis of the original data could achieve better test-retest reliability by incorporating knowledge about the structure of how the data was gathered together with incorporating information already represent in the data i.e. reaction times. This data-set was then used as an example of how power analyses of cognitive models can be conducted. This was done by first simulating and then fitting the cognitive model to many different simulated effect sizes in different trials and subject combinations. This approach allowed for modelling of the latent power curve that relates observed effect size, trials and subjects to the probability of rejecting a null hypothesis in an experiment. Using posterior predictive checks and leave one out cross validation a particular power law related the parameters of the power curve to subjects and trials with good predictive abilities. Using this equation and approach it was shown how the number of trials in an experiment can be added into a power analysis which is not standard practice when using widely used statistical software tools like G\*power [@faul_gpower_2007; @ioannidis_why_2005; @aarts_estimating_2015]. This comparison showed how G\*power's estimation of sample size for the purposed test was a lot more liberal requiring around 25 subjects whereas the full uncertainty propagated power analysis based on simulations from the fitted model suggested around 30 subjects if the number of trials were larger than at least 150. Lastly this section also highlighted why and where the test re-test reliability of these metric matters as increasing test re-test reliability shrinks the influence of sampling variability in the observed effect sizes. The rest of the discussion of the current thesis will revolve around the implications of improperly accounting for uncertainties in the science and how this might be a contributing factor to the replication crisis.



## **Power analyses, certainty and replication crisis.**

In recent years many scientific fields, and especially psychology, social science and medicine has been under scrutiny due to a lack of and failure of replication of previous studies [@wiggins_replication_2019; @forbes_chapter_2023]. Many contributing factors has been laid out such as publication bias, questionable research practices like doing statistical analyses until significant (p-hacking) or hypothesizing after the results are known (HARKING) [@head_extent_2015; @kerr_harking_1998]. A quite paradoxical aspect of this replication crisis and the use of power analyses is that many times it is advised as one of the ways to increase the replicability of studies, as (meta) analyses of power of detecting small to medium effect size in social sciences have been found to be low to very low [@felix_singleton_statistical_2023]. It is therefore argued that a reason for such low replicability in these fields, might partly be due to very low probability of being able to detect the underlying effect in the first place i.e. low statistical power. The argument is sound as long as the analysis of power is accurate or accurate to a certain degree. What this thesis has highlighted is that the use of very popular tools like G\*power for conducting these types of power analyses might underestimate the number of subjects by not including the effect of trials in the estimation process. However if this assumption is not true, then a problem might arise where researchers have more confidence in their experiment due to having conducted a power analysis than is actually justified. This mimics the false sense of certainty about the measurements assumed by these popular softwares or measurements in cognitive science in general. Therefore instead of increasing replicability and certainty in the effects observed, utilizing these tools might paradoxically decrease them, as researchers might be tricked into conducting less powered studies due to the recommendations of the software.

### *Ways of combating the replication crisis.*

Interestingly, quite a large number of scientists have suggested that moving the arbitrary statistical significance threshold to 0.005 instead of the commonly used 0.05, arguing that this could be an approach used to combat the replication crisis [@benjamin_redefine_2018]. Interestingly, lowing of the statistical threshold for significance would in practice lead to the conclusions drawn from this thesis of including and propagating uncertainty in most cases. This of cause depends on the structure and uncertainty measures of the data, however in most cases including and propagating uncertainties will lower the resulting statistic and therefore increase the resulting p-value of a particular test. These two approaches, i.e. increase the statistical significance threshold or properly propagating uncertainty, however have a very different reason to making these adjustments. Lowering of the significance threshold would be a means to an end, instead of addressing the underlying problems, which the authors also do acknowledge [@benjamin_redefine_2018].

Another interesting idea that coincides with the general theme of the thesis to combat the replication crisis is that of preregistration, registered reports, and blind analyses [@maccoun_blind_2015; @klein_blind_2005; @evans_improving_2023; @chambers_past_2022]. What all these types of interventions have in common is that they acknowledge the subjectivity in not only the data collection, but also in the data analysis pipeline of the scientific inquiry. This subjectivity is both what introduces biases, but also what drives novel ideas, and therefore is a tradeoff between exploration and exploitation, that needs to be addressed to partly guard against unwanted subjectivity. What these interventions try to do is to have the analysis pipeline either fixed before data collection or have the data scrambled such that the results of the analyses are not known when producing the analysis pipeline. The rigorous checking, testing and validating of cognitive models the thesis outlined is not at stake with these interventions but facilitates them as they are build on simulations. However, there are still considerations when analyzing the experimental data especially on the model convergence side, where in or excluding covariates or reparameterization of the models might be necessary. This is where the blind analysis intervention might be a valuable insight from physics, where experimental data is scrambled in various ways such that models and analysis pipelines can be done on data that resembles the collected data, but without being able to know the results before the data is unblinded. Decisions are therefore made on scientific justifications instead of on completely subjective citeria that perhaps could make the experimental results fit a research paradigm or perhaps even worse, produce significant results, were none are present. The distinction between decisions based on scientific justification and subjective nonsensical rationale is fuzzy and narrow, however keeping incentives, such as publishing pressure, fitting a hypothesis or research paradigm out of the equation can help with this distinction [@quaia_finding_2022]. This might even give rise to more rigorous methods and analysis pipelines because it hinders arbitrarily stopping the development of the pipeline when the results fit the preconceived notions of the scientific paradigm. Instead it forces researchers to stop when they are satisfied with the assumptions and implementations made in the analysis pipeline. This process might also help researchers understand the uncertainty that is associated with many of the methods or practices commonly used in the literature, which are taken as either ground truths or good approximations when they are at best noisy estimates. This could for instance be the difference between taking an observed effect size from a previous study instead of relying or even incorporating some further scientifically justified assumptions that the researchers hold in their domain, that might give a much better approximation for the size of the underlying effect.

## *Why and how computational tools are becoming vital in science.*

Perhaps cognitive or even computational modeling is the fresh start that is needed in the sciences that has had trouble with replication. These more sophisticated models, compared to the general linear models employed in statistical modeling, might be the steppingstone to engage in more theoretically driven analyses, hopefully reducing the number of irreproducible studies. However for this movement to succeed, it is essential that rigorous metrics are enforced to access their internal validity, such that models without any even provable, in principle, parameters or behaviors are discarded from the beginning. Examples might arise where mathematical formulation of theories are developed, but that in practice the formulation is not tractable from a computational perspective. It would be a shame to spent years investigating these kinds of models and their implications in a field of research, just to discover that it in fact is intractable in practice [@ho_cognitive_2021; @mcclelland_place_2009; @zuidema_five_2020]. One might think that a necessity of these more complicated models is a need for deeper mathematical understanding. What this thesis has argued is that this is not necessarily the case, as simulations allow researchers to observe the implications of their assumptions as well as investigate when they break. Another argument for why more sophisticated models are not necessarily off the shelf due to high level mathematically understanding is the increase in adaption of sophisticated hierarchical models which are mathematically much more complex than single level models in cognitive science, which have widely been adopted in the literature [@dedrick_multilevel_2009]. This is not to say that a better understanding of the machinery and mathematics itself would not be helpful for researchers of various fields, but perhaps not a necessity anymore. This would therefore also imply that the way that statistical methods and tools are taught might need to change. Such that fields were mathematical methods are not common place, students and researchers could be taught statistical methods with the use of coding and simulation examples instead of flowcharts for which statistical analysis to conduct when. This would entail getting the tools to understand and reflect on these statistical models and therefore also the tools to understand when they break. In the same way that a good scientific program does not teach students the right theories or hypotheses, it teaches them to think in a scientific way such that the individual can decide and test these themselves. Here the tools for understanding, reflecting and experimenting with statistical models and concepts would be programming experience in statistics, which would facilitate the types of data simulations presented in the current thesis. This would allow the researcher to understand the assumptions that are being made concretely and examine what happens when they are broken in various ways. This approach also requires to think more generatively about the process of how the data has been generated, because in order to simulate it is necessarily to have a model of how it was generated, which might even help spark new scientific ideas. This approach would have researchers more closely engaged in the statistical process of analyzing the data, instead of just picking an off the shelf model from a flowchart.

## *Standing on the shoulders of giants*

All of the models used in the current paper were fitted using Stan with the cmdstanr interface, which uses full Bayesian statistical inference with Markov chain monte carlo (MCMC) sampling [@R-cmdstanr]. As described in the introduction section about modeling definitions, fitting and building models in this framework is extremely flexible as the sampling algorithm essentially serve as the optimizing process for the parameters of interest. An additional benefit of this framework is that the code for simulating the generative process is close to identical in nature, to the code that specifies the model, making it easy for users with a generative framework to code up these types of models. The additional benefits to using especially Stan and the its Hamiltonian Monte Carlo (HMC) algorithm is that when issues arise the algorithm will complain and let you know, reducing the risk for erroneous inference due to the sampling algorithm or typos in the code [@vehtari_rank-normalization_2021]. 

The thesis used Bayesian inference and Stan, mainly due to its flexibility in model formulation and not because of the inherent differences between Bayesian and frequentist statistics. However Bayesian inference does allow for a more optimistic way to interpret the replication crisis discussion above. This interpretation is that perhaps instead of starting each experimental analysis from the perspective that nothing or very little is known about the parameters of interest, perhaps incorporating information from previous studies would be beneficial. This is in essences what science is about, a hierarchical organization of knowledge, where each step rests on the step below, i.e. on auxiliary assumptions as put by the Duhem--Quine thesis [@ariew_duhem_1984]. This view on science also matches that of uncertainties as these are also hierarchically organized and when doing analyses on data with uncertainties these uncertainties has to be accounted for. So in the same way that the results of a scientific theory is only as strong as its auxiliary assumptions; the strength of an analysis is also only as strong as the certainty with which the data is measured with. What the Bayesian framework of inference allows, is that prior information from similar studies can be used in the modelling allowing researchers to not start their scientific studies from scratch, but pick up where others left off. This would essentially mean that instead of having to collect a larger number of subjects to achieve the actual desired power of the study, this could be done by two independent laboratories, the second using the information provided by the first. This essentially is already what is being done when conducting meta-analyses. This approach incentives publications of all types of finding as they serve as the stepping stones for the next researcher, making the problem of publication bias where null findings are unpublished less incentivized [@laitin_reporting_2021].

## *Limitations*

One of the main focal points of the thesis was investigating how the correlation coefficient is an inappropriate metric to examine internal model validation of cognitive models. The modified intra class correlation (ICC) was purposed as a more sensible metric. As is the case in Figure 11, the correlation coefficient (otherwise commonly used metric) quickly becomes asymptotic whereas there is more granularity in the ICC. Another reason to favor the ICC compared to the correlation coefficient would be its more straightforward interpretation of a ratio of variances. This interpretation benefit also comes with the ICC's sensitivity to linear transformation, not found in the correlation coefficient. Lastly future studies should investigate the link between how these metrics behave and compare them to the power analysis conducted, Figure 23 displayed how both trials and subjects effected the power of the study and the link between power and trials might be related to the link between trials and either of these two internal validity metrics.
A clear link between these quantities would make the need for conducting a power analysis superfluous as the information would be contained in the ICC or correlation coefficient analysis. A thorough investigation of this link would mean that the somewhat arbitrary choice of trials when designing an experiment would no longer be arbitrary, but informed by how estimation uncertainty in the parameters of interest change based on the number of trials [@miller_how_2024].

Another limitation of the current study is the quite limited power analysis conducted. Ideally the thesis would have investigated how other parameters of the psychometric function behaves as a function of trials and subjects. A particular interest would be on the slope of the psychometric function as changes in this parameter changes the estimation uncertainty of all the other parameters. One might suspect that an intervention that increases the steepness of the slope would also make it easier to detect a change in the threshold as both the correlation coefficient and ICC metric showed that with increased steepness of the function less estimation uncertainty was present in the estimation of the threshold and slope itself. This highlights how the parameters of the model interacts which can be accounted for by performing these simulations. Therefore future investigations should expand upon this power analysis for the other parameters and especially the slope of the psychometric function. 

The thesis also did not investigate how incorporating the reaction times into the power analysis would change the number of subject and trials needed for observing a particular effect-size. Future studies investigating this would not only help elucidate the question posed above about the relationship between the internal model validation metric, trials and power, but could also give an estimate of the increased efficiency by incorporating information already present in most experiments. The reasoning for only conducting the single power analysis on the threshold in the current thesis highlights one of the main hurdles of the framework purposed, computational resources. Firstly fitting models using HMC and Bayesian inference is both more time and computational resource intensive compared to frequentist inference in packages such as lme4, lmertest or GAMLSS to name a few quite flexible models fitting packages in R [@stasinopoulos_generalized_2008; @kuznetsova_lmertest_2017; @bates_fitting_2015]. This added time for doing the optimization of posterior distributions of parameters has drawbacks in a need for access to bigger machines to necessitate the need for parallelization of the computational burden especially when several chains are needed to ensure convergence. Fortunately the access to bigger machine both privately but also on an institutional level is something that is growing in accessibility and already available to many universities or centers of research and has been correlated with research competitiveness [@apon_high_2010]. The current thesis was supported by Ucloud (see acknowledgement).

\newpage

