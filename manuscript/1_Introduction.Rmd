# Introduction

Most scientific inquiry revolves around measurements of the physical world, whether that it is the time it takes for a cup to fall to the ground or for a person to react to a visual stimulus on a computer screen. These measurements will be associated with uncertainty, as repeatedly measuring the same thing will result in different measurement values. This makes uncertainty a fundamental aspect of scientific inquiry and theories. It is therefore also a role of science to quantify such uncertainties.

In this thesis, I will investigate uncertainty handling in Cognitive Science and provide ways to properly account for these uncertainties. The thesis will rely on Monte Carlo simulations, which provide a robust method for accounting for uncertainties, in analyses and models. Specifically, the thesis will introduce a partially novel approach to testing and validating the parameters of cognitive models. This apporach is going to be used with a focus on the psychometric function, a commonly used cognitive model. It will be shown that the parameters of the model and their uncertainties, can be reduced by several different interventions. These include optimization of the experimental design, and by incorporating additional information already available in most experiments. The thesis will demonstrate how modeling and incorporation of such concepts can decrease uncertainty in the estimation of parameters of already published data. Lastly, using this re-analysis of published data, the thesis will highlight opportunities to conduct power analyses, utilizing a novel modeling framework. This framework can help make power analyses for a particular model more rigorous by incorporating and propagating uncertainty. Comparison of this novel power analysis framework will be compared to popular tools such as G\*power.

## *Uncertainties in science*

Science can be thought of as a systematic way to organize knowledge in hierarchies, leading to testable hypotheses. Knowledge can be hard to define, but most often it is something that is achieved though experience. Imagine a cup being dropped, most people will have the knowledge that it will fall towards the ground and reach our foot at a particular speed because of our previous experiences with dropping a cup. This is to say that knowledge is the relationships that we believe to be true with differing amounts of certainty. In reality, even though we might say we are completely certain that the cup will fall to the ground, and reach it at a particular speed, this assumption is only true most of the time. Given that the natural world is bounded on probabilities, complete certainty is unwarranted, both in the assumption of the cup hitting our foot, but especially the speed at which it hits our foot with. Here, the interest is not in the unforeseen events, but instead in the predictability and (un)certainty of the expected. Science would normally be interested in the acceleration of the cup and the uncertainty in this estimate. Scientists have shown that objects dropped on Earth will accelerate towards the ground with an acceleration of $9.81\frac{m}{s^2}$ [@johannes_fundamentals_2009].  However, this number does not mean anything without an estimate of the uncertainty, and an understanding of the assumptions entailed with these numbers The first proposition is well studied and the 95% confidence interval of the value is estimated to be $[9.78 ; 9.84]\frac{m}{s^2}$ [@johannes_fundamentals_2009]. The second proposition is also well studied, as we believe that the density, the shape and weight of the cup if dropped outside of a vacuum are important. To estimate this acceleration, measurements must to be made. These measurements include the distance the cup travels and the time it takes. With these measures of distance and time, uncertainty is introduced and propagated to get the an estimate for the acceleration, but also the uncertainty associated with it. This example highlights two main points, that this thesis will explore. Firstly, uncertainties are organized in hierarchies and are just as important as beliefs. Secondly, taking these uncertainties seriously and herein estimating and propagating them should not be a choice, or something that can be avoided. After examining these potential issues, the thesis will propose methods for incorporating and minimizing uncertainties through simulations. The goal is to shed light on the often overlooked uncertainties in the data collected on human behavior and cognition, while also offering strategies for addressing them. It will be argued that accounting for uncertainties is more important than ever, especially in research of complex systems such as humans. This urgency arises due to the availability of computational resources having made it possible to easily develop more sophisticated analyses and models that have dependencies on lower-level analyses. This hierarchical dependency underscores the necessity for proper uncertainty handling. Without propagating uncertainties, resulting estimates or beliefs become overly confident. Furthermore, these computational resources enable uncertainty propagation without a deep understanding of the underlying mathematics, thus making it accessible to most researchers, with some coding experience. To effectively convey both the statistical models, as well as the underlying uncertainties associated with doing computations on data, the thesis will start by exploring different types of uncertainty.


## *Levels of uncertainty and uncertainty propagation*

I will here broadly define three different types of uncertainty, namely  measurement, estimation, and test-retest uncertainty. These definitions are going to be used throughout the thesis. These definitions are not exhaustive and will be centered around how experimental studies in cognitive science are conducted, from data collection to data analysis. Before examining these three types of uncertainty it is imperative to acknowledge that uncertainties can be defined in hierarchies, and that uncertainty propagates through these hierarchies. This uncertainty propagation means that as calculations are completed based on measures with uncertainty, the uncertainty propagates to the results of the calculations. Simulations will be used to show how uncertainty propagation can be understood, and handled, without a need for rigorous mathematical proofs. For a more mathematical treatment see [@saccenti_corruption_2020]. 

The lowest level of uncertainty is in the measurements themselves i.e. measurement uncertainty. Measurement uncertainty reflects the uncertainty in how well one can, for instance measure the reaction time on a computer or the time, it took a falling cup to reach the ground. This level of uncertainty is sometimes neglected in Cognitive Science when applying statistical models, because they are sometimes thought to be minuscule. An example could be reaction time tasks, commonly used in cognitive science [@stone_using_2014; @sternberg_memory-scanning_1969]. These reaction time experiments may or may not have minuscule measurement uncertainty, depending on the experimental setup [@holden_accuracy_2019; @crocetta_problem_2015; @ohyanagi_solution_2010]. This is not to say that cognitive scientists do not care about measurement uncertainty, as moving towards more sophisticated measurement methods is an ongoing endeavor. For instance, using better and more sophisticated computers to measure reaction times [@crocetta_problem_2015]. Minimizing this kind of uncertainty most often revolves around getting better tools to measure the variable(s) of interest. 


The next level of uncertainty arises when calculations are done on data. This is true for calculations done on measures with and without measurement uncertainty. This uncertainty will be referred to as estimation uncertainty. Estimation uncertainty is often quantified by the statistical model, such as in the form of a standard error of a correlation coefficient (CC) or the width of a posterior distribution of a parameter. Estimation uncertainty is always present and influenced by the measurement uncertainty. Minimizing estimation uncertainty is a primary concern for scientists, particularly as many cognitive science experiments center around null hypothesis testing. This null hypothesis testing will typically involve testing whether parameter estimates includes a particular value, typically, 0. Therefore, minimizing estimation uncertainty serves to highlight underlying effects. The standard approach to minimize estimation uncertainty is to get more data, provided it originates from the same population. In cognitive science this often includes increasing the number of trials and/or subjects to get more precise estimates, thereby reducing estimation uncertainty. However, this method of minimizing estimation uncertainty is not without its drawbacks or inherent uncertainties. Increasing the number of trials in a cognitive task may inadvertently exacerbate estimation uncertainty. This can occur due to various factors, including boredom, habituation, fatigue, and lack of engagement, particularly when experimental tasks become very long [@meier_is_2024; @jeong_exhaustive_2023]. Increasing the number of trials could also make participants more prone to switching between cognitive strategies. If these switches are not properly accounted for in the analysis, they might be interpreted as additional noise by the model and its parameters. Increasing the number of subjects will decrease estimation uncertainty on the population level estimates, if the sample population is homogeneous. The trade off between subjects and trials in an experiment, is therefore quite important to minimize estimation uncertainty, but also minimize the overuse of resources. In addition to these traditional approaches, there are alternative methods for minimizing estimation uncertainty, such as modifying the task design or including more information in the model [@baldi_antognini_new_2023; @stone_using_2014]. This modification or optimization of the task design involves individualizing the task design, to maximize the informativeness of each presented stimulus. This task design optimization is frequently used in psycho-physical experiments, where adaptive algorithms like PSI, QUEST and ADOPY are used to select stimuli that minimizes the uncertainty in the estimated parameters [@watson_quest_2017; @yang_adopy_2021; @prins_psi-marginal_2013]. 

The final layer of uncertainty to be addressed, is the test retest uncertainty, which arises from the variability of parameter estimates over time. In cognitive science the additional uncertainty on retesting the estimates of a parameter stems from the fact that humans vary over time. This variation is influenced by various behavioral factors like learning, but also psychological factors such as mood and arousal [@schurr_dynamic_2024].

## **Measurement uncertainty in a correlational design**

In order to demonstrate and account for these types of uncertainties, the thesis will here show how it is possible to add measurement uncertainty to a correlation analysis. This will be accomplished by using simulations. A correlation analysis is chosen as the example for three main reasons. Firstly, significant portion of published literature in Cognitive Science revolves around conducting correlational analyses on measures that have quantifiable uncertainties. These measures typically involve estimated parameters, structural properties of the brain or even reactions times [@wu_neurobiological_2021; @de_berker_computations_2016; @luijcks_influence_2015]. Secondly, this example can be generalized, such that instead of estimating a CC on data with measurement uncertainty, it could equivalently be done for more complex models. Supplementary analysis 1 shows how these uncertainties interact in a linear model, in a test retest paradigm. Lastly, this type of analysis is going to be used extensively throughout the thesis serving as a primer for the upcoming sections. 

Initially, a the special case where little to no uncertainty exists in the data is examined. This case allows for comprehension of the estimation uncertainty of the CC in isolation, without the influence of measurement uncertainty. Analytical solutions exist to calculate this estimation uncertainty and is incorporated in most statistical softwares [@R-correlation]. However, this can also be shown by simulations, or more accurately by re-sampling. To estimate the uncertainty in the CC, the data is re-sampled with replacement, a technique known as bootstrapping [@efron_estimating_1983]. Iterating this process of resampling gives a distribution of CCs, which with enough iterations will converge towards the analytic solution. It is generally recommended to have least 30 data points, to ensure convergence to the analytical solution [@efron_estimating_1983; @efron_introduction_1994; @wu_jackknife_1986]. For the simplest case of recalculating the CC (without measurement uncertainty) and its uncertainty, the process might seem tedious compared to using the direct analytic solution. However, once implemented and understood, this approach allows for adding and propergating all types of uncertainty, coming from various kinds of distributions. One of the advantages of having an analytic solution to the case of estimating the uncertainty of the CC is to ensure proper setup of code and scripts. This first step therefore serves as a validation step, before venturing into areas where analytic solutions are scarce or nonexistent.


The initial step is therefore to demonstrate that the two approaches of simulating and analytically estimating the uncertainty of the CC is identical across ranges of correlations and sample sizes. To achieve this, simulated data from a multivariate normal distribution with the following parameters are produced.


$$
\begin{equation} 
  \begin{pmatrix}
  x_i \\
  y_i 
  \end{pmatrix} \sim \mathcal{N}\left(
  \begin{pmatrix}
  \mu_x \\
  \mu_y 
  \end{pmatrix}, \begin{bmatrix}
  \sigma_x^2 & \sigma_x \cdot \sigma_y \cdot \rho_{xx} \\
  \sigma_x \cdot \sigma_y \cdot \rho_{xx} & \sigma_y^2 
  \end{bmatrix} 
  \right)
\end{equation} 
$$

Where

$$
\mu_x = 50, \quad \mu_y = 100, \quad \Sigma = \begin{bmatrix}
10^2 & 10 \cdot 10 \cdot \rho_{xy} \\
10 \cdot 10 \cdot \rho_{xy} & 10^2 
\end{bmatrix}
$$


The multinormal distribution produces random variables with a means $μ_x$ a standard deviation $σ_x$ and crucially, with a CC between the random variables $ρ_{xy}$. This distribution is ideal for understanding how the CC changes as it is a parameter of the distribution. Demonstrating the equivalence of bootstrapping and the analytic solution to the estimation uncertainty, involves simulating CCs from the set $\rho_{xy} = \in \{ -0.9, -0.8, \ldots, 0.8, 0.9\}$ with the total number of samples per random variable being $N \in \{50, 100, \ldots, 500\}$
[@correlationPackage; @R2024]. See supplementary Figure 1 for demonstration of the similarity of these two approaches.

After having established the equivalence between the two approaches, one can now proceed to add measurement uncertainty to each observation. To add measurement uncertainty, one can instead of randomly re-sampling pairs of data points from the original data-set, as done in the case without measurement uncertainty, one can re-sample these pairs using an error distribution. The original data is then inserted as the mean of this error distribution and the uncertainty (standard deviation) of this distribution, is the measurement uncertainty. A straightforward choice of error distribution would be the normal distribution, which would reflect that the uncertainty is assumed to be bidirectional, with no preferred direction. This can be formulated as the following:

$$
\begin{equation}
\begin{pmatrix}
  \hat{x}_i \\
  \hat{y}_i
\end{pmatrix} \sim \mathcal{N}\left(
\begin{pmatrix}
  x_i \\
  y_i
\end{pmatrix}, \begin{bmatrix}
  m_x^2 & m_x^2\cdot m_y^2\cdot\rho_{m} \\
  m_x^2\cdot m_y^2\cdot\rho_{m} & m_y^2
\end{bmatrix}
\right)
\end{equation}
$$

where $\hat{x}_i$ and $\hat{y}_i$ represent the observed estimates of x and y on a particular simulation given their measurement uncertainty $m_x$ and $m_y$ and the correlation between them $\rho_m$. Of note, is that one might re-sample the original data from other error distributions. For instance if values are strictly positive, then simulating from a truncated normal or strictly positive distributions like a lognormal, would be preferred to avoid sampling values that cannot be obtained i.e. negative reaction time values. 


Figure 1 A, shows a scatter plot of $x_i$ and $y_i$, these might represent different measurements from a Cognitive Science experiment, say reaction times and time spent awake. For visualization purposes only measurement uncertainty was added to the y-coordinates meaning that from the above equation $m_x = 0$ and $m_y \in \{ 1, 2, \ldots, 10 \}$, here this uncertainty is depicted as error bars on individual data points. Figure 1 B displays how the estimated CC distribution obtained by bootstrapping changes based on these measurement uncertainties. The figure demonstrates that the CC estimated via bootstrapping is attenuated, while its distribution widens with increasing measurement uncertainty, mirroring what one would find using analytical solutions [@saccenti_corruption_2020].

```{r figure1, fig.width = 7, fig.height = 6, warning = F, message = F, echo = F, fig.cap = "**Figure 1 Measurement uncertainty on correlation coefficient**  (A) Displays a scatterplot with varying amounts of measurement uncertainty. (B) displays how the correlation coefficient distribution, obtained through bootstrapping changes with increasing measurement uncertainty. Vertical line is the simulated correlation coeficient without uncertainty."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_3_measurement_uncertainty.png")), scale = 1)
```

Here it was shown with normally distributed noise that decreased the size and width of the CC, later non trivial types of noise is added where the simulation approach used here, is necessary to properly propagate the uncertainty. This example does not demonstrate how test re-test uncertainty interacts with these lower levels i.e. measurement and estimation uncertainty. One can imagine measuring the CC from figure 1 B twice, and getting different results, even if the measurements were infinity precise, and there was no uncertainty in the estimation process due to infinity many data-points. For a more concrete and elaborate example, see supplementary analysis 1. The main message here, is that to get reliable estimates and in the end, to make reliable inference one needs to account for all sorts of uncertainties and the lower in the hierarchy you move the more fundamental and important they become. Having a parameter estimate that is stable over time, will not matter if you cannot estimate or measure it reliably in the first place.

