# Introduction

Most scientific inquiry revolves around measurements of the physical world; may that be the time it takes for a cup to fall to the ground or the time it takes for a person to react to a visual stimulus on a computer screen. These measurements of the physical world will be associated with uncertainty as repeatedly measuring the same thing, will result in different measurements. This means that one of the most fundamental things that scientific inquiry and theories rests on is uncertainty and it is one of the roles of science to quantify such uncertainties.
In this thesis I will investigate shortcomings in uncertainty handling in cognitive science, while providing ways to properly account for these uncertainties when analyses are conducted. To do this I will rely on Monte Carlo simulations which provides a robust method for accounting for uncertainties when analyses and models are deployed. In particular the thesis will introduce a partially novel way of testing and validating the parameters of models in cognitive science. The thesis will demonstrate this with a focus on the psychometric function, a commonly used cognitive model in cognitive science. It will be shown that the parameters of the model and their uncertainties can be reduced by several different interventions and assumptions, including optimization of the experimental design, but also by incorporating additional information already available in most experiments like reaction times.

To further reiterate the utility of jointly modeling additional variables the thesis will re-analyze published data using a psychometric function. Here it will be demonstrated how carefully using the known structure of the data can greatly improve session by session correlation between parameters, i.e. minimizing uncertainty. Lastly using this reanalysis, the thesis will highlight opportunities to conduct power analyses utilizing a novel modeling framework that accounts for the uncertainty in model parameters as well as sampling variability of the effect investigated. Comparison will be made to popular tools such as G\*power highlighting the need for rigorous methods, when conducting power analyses but also in model validation.

## *Uncertainties in science and examples from physics*

<!-- This section still needs work -->

Science is a systematic way to organize knowledge in hierarchies, leading to testable hypotheses. Knowledge can be hard to define, but most often it is something that is achieved though experience. Imaging a cup being dropped, most will have the knowledge that it will fall towards the ground and reach our foot at a particular speed, because of our previous experiences with dropping a cup. This is to say that knowledge is the relationships that we believe to be true with differing amounts of certainty. The reality is that even though we might say that we are completely certain of events, i.e. know, that the cup will fall towards the ground and reach it at a particular speed, this is in an assumption that is true only most of the time. Given that the natural world is bounded on probabilities, complete certainty is unwarranted, both in the assumption of the cup hitting our foot, but especially the speed at which it hits our foot. Most of the time the probabilistic nature of the natural world stems from the uncertainties during measurement or perhaps unseen events. The interest here is not in the unseen events, but instead in the predictability and (un)certainty of the expected. Taking the falling cup as an example, in science we would normally not be interested in the probability that the cup will hit the ground, but instead in the acceleration of the cup and the uncertainty in this estimate. What scientists have shown is that objects dropped on earth will accelerate towards the ground with an acceleration of $9.81\frac{m}{s^2}$ [@johannes_fundamentals_2009]. However, this number does not mean anything without an estimate of the uncertainty, while also accounting for the assumptions that are entailed with these numbers. The first proposition is well studied and the 95% confidence interval of the value is estimated to be $[9.78 ; 9.84]\frac{m}{s^2}$ [@johannes_fundamentals_2009]. The second proposition is also quite well studied as we know that the density of the medium that the cup is dropped in is important, but also the shape and weight of the cup if dropped outside of a vacuum. In order to estimate this constant acceleration, measurements have to be made of the distance a falling cup travels and the time it takes to reach the ground. With these measures of distance and time, uncertainty is introduced and propagated to get the an estimate for the acceleration, but also the uncertainty associated with it. There are 2 main points of the example which this thesis will explore, firstly uncertainties are organized in hierarchies and are just as important as beliefs as without one, the other is meaningless. Secondly, taking these uncertainties seriously and herein estimating and propagating them should not be a choice or something that can be avoided, but a necessity of all scientific endeavors, where they can be quantified or at least approximated. Taking its outset in the published literature, the thesis will establish issues regarding uncertainty handling and propagation and use simulations to highlight the problems with neglecting a proper account of uncertainty in statistical models particularly in cognitive science. After highlighting these potential issues, the thesis will provide possible ways of dealing with the shortcomings by means of simulations. The goal of this thesis is to illuminate the often-overlooked uncertainties in the data collected on human behavior and cognition while providing ways of accounting for it. This would therefore entail that the uncertainty reported in the published literature more accurately reflects the (un)certainty in the results. 

The thesis will argue that accounting for uncertainties is more important than ever, especially in research of complex systems such as humans, as computational resources have made it possible to easily develop more sophisticated analyses and models that have dependencies on lower-level analyses. The dependent structure makes the need for proper uncertainty handling even more imperative, as without propagating uncertainties the resulting estimate will be overly confident. Furthermore, these computational resources do allow for uncertainty propagation without a deep understanding of the underlying mathematics, making it accessible to most researchers with some coding experience. To effectively communicate both the statistical models as well as the underlying uncertainties associated with doing computations on data, the thesis will start by exploring different types of uncertainty in cognitive science. Next the thesis will be investigating a particular cognitive model, the psychometric function, used in many sub fields of cognitive science. This model will be examined and used to show how a novel way of validating cognitive models can be used to gain more information about the structure of the uncertainty in the parameters of the model.

## *Levels of uncertainty and uncertainty propagation*

I will here broadly define 3 different types of uncertainty which are going to be used throughout the thesis, measurement, estimation, and test-retest reliability uncertainty see figure 1 and 2 for a visualization. These definitions are not exhaustive and will be centered around how experimental studies in cognitive science are conducted, from data collection to data analysis. However, before examining these three types of uncertainty it is to acknowledge that uncertainties can be defined in hierarchies and that uncertainty propagates through these hierarchies. This uncertainty propagation means that as calculations are done based on measures with uncertainty, the uncertainty propagates to the results of the calculations. In this thesis I will be using simulations to show how uncertainty propagation can be understood and handled without a need for rigorous mathematical proofs. For a more mathematical treatment see [@saccenti_corruption_2020].

The lowest level of uncertainty is in the measurements themselves i.e. measurement uncertainty. Measurement uncertainty reflects the uncertainty in how well one can for instance measure the reaction time on a computer or the time it took a falling cup to reach the ground. This level of uncertainty is often neglected in cognitive science when applying statistical models, because they are thought to be minuscule as in the case of reaction time tasks, which may or may not be true given the experiment setup [@holden_accuracy_2019; @crocetta_problem_2015; @ohyanagi_solution_2010]. This is not to say that cognitive scientists do not care about them, as moving towards more sophisticated measurement methods is an ongoing endeavor. For instance, using better and more sophisticated computers to measure reaction times commonly found in cognitive science experiments [@crocetta_problem_2015]. Minimizing this kind of uncertainty most often revolves around getting better tools to measure the variable(s) of interest. Interestingly some variables of interest in cognitive science do not have a explicit quantification of measurement uncertainty. One of the main instances coming to mind is the use of questionnaires. Many of these questionnaires try to quantify a latent construct such as mental health conditions i.e. depression or anxiety. Many of the main questionnaires used to assess these latent constructs use several questions that are then added together to give a score of said mental health condition without a quantification of the uncertainty on the latent construct because no uncertainty is quantified on each question [@xiao_psychometric_2023; @cohen_perceived_1994; @kroenke_phq-9_2001; @johnson_psychometric_2019]. 

In order to contextualize these forms of uncertainty i will consider an idealized example of a fictional researcher wanting to understand the relationship between reaction times and stress. To do this the researcher conducts an experiment where participants are measured several times under different conditions to introduce stress. A questionnaire is used to access the stress of the participant and a computer based experiment is used to determine their reaction time. In this example both of these measures have associated uncertainty, measurement uncertainty, see the individual data points in figure 1. The relationship between reaction times and stress is determined by the slope of the regression line depicted in Figure 1.

```{r figure1, fig.width = 7, fig.height = 4, warning = F, message = F, echo = F, fig.cap = "**Figure 1 Measurement and Estimation uncertainty;** the figure displays a linear regression between two measurements for instance reaction time and stress with measurement uncertainty depicted as vertical and horizontal error bars on individual points. The mean of the regression line with and without propagated uncertainty is highlighted in grey and dark green respectively. Lastly a prediction interval is depicted as the shaded area around the mean of the regression line with and without propagated uncertainty again in grey and green respectively."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_1_measurement_uncertainty.png")), scale = 1)
```

The next level of uncertainty is when a particular model is fit to some data, or more broadly when calculations are done on data with uncertainty. In cognitive science we achieve parameter estimates from our collected data and these parameter estimates have uncertainty associated with them, this uncertainty will be referred to as estimation uncertainty. Estimation uncertainty is most often quantified by the statistical model be that the standard error of a regression coefficient or the width of a posterior distribution of a parameter in a Bayesian framework. Taking the example of the researcher investigating reaction times and stress this is the uncertainty in the parameter estimates achieved by fitting a linear model to the data see the linear model in figure 1. Minimizing estimation uncertainty is partly what scientists care about, as inevitably most cognitive science experiments revolve around null hypothesis testing, which in most cases will involve testing whether the parameter estimate includes a particular value, mostly, 0. Therefore if there is an underlying effect i.e. the parameter is not zero, then minimizing estimation uncertainty will reveal this effect as being statistically significant. To minimize this type of uncertainty the standard approach is to get more data, given that it comes from the same population and produces similar results. In cognitive science this might include increasing the number of trials or subjects to get a more precise estimate of interest i.e. minimizing estimation uncertainty. This way of minimizing estimation uncertainty is however not free or free of uncertainty itself. Firstly, increasing the number of trials in a cognitive task might even increase the estimation uncertainty itself. This can happen for several reasons, but boredom, habituation, fatigue and lack of engagement can become big contributors when experimental tasks become very long [@meier_is_2024; @jeong_exhaustive_2023]. Secondly for some cognitive science experiments massively increasing the number of trials could make subjects more prone to switching between cognitive strategies and if not properly accounted for in the analysis might be interpreted as additional noise by the model and its parameters. Next increasing the number of subjects included in a study will many times decrease estimation uncertainty on the population level estimates, if the sample population is homogeneous. The trade off between subjects and trials in an experiment is therefore quite important to minimize estimation uncertainty, but also minimize the overuse of resources. Beyond these traditional approaches to minimizing estimation uncertainty there are other ways, for instance changing the task design such that responses will give more information on parameter values of interest [@baldi_antognini_new_2023; @stone_using_2014]. This optimization strategy involves individualizing the task design such that each presented stimulus is the most informative it can be. This task design optimization is frequently used in psycho-physical experiments where adaptive algorithms are used to select the upcoming stimuli such that it minimizes the uncertainty in the estimated parameter values. See for example algorithms like PSI, QUEST and ADOPY [@watson_quest_2017; @yang_adopy_2021; @prins_psi-marginal_2013].  From the example of the researcher investigating reaction time and stress this might involve selecting interventions that will produce varying levels of stress to increase the precision of the parameters of the model, again minimizing estimation uncertainty. A practical example of how these algorithms work to minimize uncertainty in the parameter estimates, see the section about Adaptive design optimization. The next level, and last level presented here, of uncertainty stems from the fact that these parameter estimates will vary over time, as humans vary over time. This variation stems from both behavioral factors like learning, but also psychological factors such as mood and arousal [@schurr_dynamic_2024]. This type of uncertainty will be referred to as test-retest uncertainty. Again with offset in the example, participants in the researchers study on reaction times and stress might be tested twice on different days to understand how stable the relationship is over time. As the relationship is measured by the parameters of the model the stability of the relationship is measured by the stability of the parameters. One might imagine that the amount of sleep acquired before the experimental day could influence both measures of the task i.e. reaction time and susceptibility to stress and perhaps even their relationship. Figure 2 displays how the parameter estimates of the same model as presented in Figure 1 with and without accounting for uncertainty propagation change based on the propergation of uncertainty. As can be seen from Figure 1 accounting for the measurement uncertainty does not change much the prediction made by the model, however when propagating these extra uncertainties into the next analysis of the parameters i.e. from session to session in Figure 2 the change in results become more pronounced. The main effect for the current linear model is that the residual variance is underestimated without error propagation and the slope parameter is overestimated.



```{r figure2, fig.width = 7, fig.height = 4, warning = F, message = F, echo = F, fig.cap = "**Figure 2 Test retest uncertainty;** displays the results of fitting the linear regression in Figure 1 twice with and without accounting for measurement uncertainty. Each facet represents one of the three parameters of the linear model, the intercept the residual uncertainty and the slope respectively from left to right. Colors represented weather the measurement uncertainty was proporgated or not."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_2_test_retest.png")), scale = 1)
```

The main message here is that to get reliable estimates and, in the end, to make reliable inference one needs to account for all these sorts of uncertainties and the lower in the hierarchy you move the more fundamental and important they become, when doing higher level calculations with them. Having a parameter estimate that is stable over time won't matter if you cannot estimate it, or measure it, reliably in the first place.
