# Introduction

Most scientific inquiry revolves around measurements of the physical world; may that be the time it takes for a cup to fall to the ground or the time it takes for a person to react to a visual stimulus on a computer screen. These measurements will be associated with uncertainty as repeatedly measuring the same thing, will result in different measurements. This means that one of the fundamental things that scientific inquiry and theories rests on is uncertainty. It is therefore also one of the roles of science to quantify such uncertainties.

In this thesis I will investigate uncertainty handling in cognitive science, while providing ways to properly account for these uncertainties when analyses are conducted. To do this I will rely on Monte Carlo simulations which provides a robust method for accounting for uncertainties when analyses and models are deployed. In particular the thesis will introduce a partially novel way of testing and validating the parameters of models in cognitive science. The thesis will further demonstrate this, with a focus on the psychometric function, a commonly used cognitive model in cognitive science. It will be shown that the parameters of the model and their uncertainties can be reduced by several different interventions and assumptions, including optimization of the experimental design, but also by incorporating additional information already available in most experiments. The thesis will demonstrate how modeling and incorporation of such assumptions can improve estimation of parameters of already published data. Lastly using this re-analysis of published data, the thesis will highlight opportunities to conduct power analyses utilizing a novel modeling framework that can help make  power analyses for a particular model more rigorous by incorporating the propagation of uncertainty. Comparison of this novel power analysis framework will be compared to popular tools such as G\*power.

## *Uncertainties in science*

<!-- This section still needs work -->

Science can be thought of as a systematic way to organize knowledge in hierarchies, leading to testable hypotheses. Knowledge can be hard to define, but most often it is something that is achieved though experience. Imaging a cup being dropped, most people will have the knowledge that it will fall towards the ground and reach our foot at a particular speed, because of our previous experiences with dropping a cup. This is to say that knowledge is the relationships that we believe to be true with differing amounts of certainty. The reality is that even though we might say that we are completely certain of events, i.e. know, that the cup will fall towards the ground and reach it at a particular speed, this is in an assumption that is true only most of the time. Given that the natural world is bounded on probabilities, complete certainty is unwarranted, both in the assumption of the cup hitting our foot, but especially the speed at which it hits our foot. Most of the time the probabilistic nature of the natural world stems from the uncertainties during measurement or perhaps unseen events. The interest here is not in the unseen events, but instead in the predictability and (un)certainty of the expected. Taking the falling cup as an example, in science we would normally not be interested in the probability that the cup will hit the ground, but instead in the acceleration of the cup and the uncertainty in this estimate. What scientists have shown is that objects dropped on earth will accelerate towards the ground with an acceleration of $9.81\frac{m}{s^2}$ [@johannes_fundamentals_2009]. However, this number does not mean anything without an estimate of the uncertainty, while also accounting for the assumptions that are entailed with these numbers. The first proposition is well studied and the 95% confidence interval of the value is estimated to be $[9.78 ; 9.84]\frac{m}{s^2}$ [@johannes_fundamentals_2009]. The second proposition is also quite well studied as we know that the density of the medium that the cup is dropped in is important, but also the shape and weight of the cup if dropped outside of a vacuum. In order to estimate this acceleration, measurements have to be made. These measurements include the distance the cup travels and the time it takes. With these measures of distance and time, uncertainty is introduced and propagated to get the an estimate for the acceleration, but also the uncertainty associated with it. There are 2 main points of the example which this thesis will explore, firstly uncertainties are organized in hierarchies and are just as important as beliefs as without one, the other is meaningless. Secondly, taking these uncertainties seriously and herein estimating and propagating them should not be a choice or something that can be avoided, but a necessity of all scientific endeavors, where they can be quantified or at least approximated. The thesis will show how issues could arise in different aspects of cognitive science, if uncertainty is not properly handled. After highlighting these potential issues, the thesis will provide possible ways incorporating and minimizing these uncertainties by means of simulations. The goal of this thesis is to illuminate the often overlooked uncertainties in the data collected on human behavior and cognition while providing ways of accounting for it.

The thesis will argue that accounting for uncertainties is more important than ever, especially in research of complex systems such as humans, as computational resources have made it possible to easily develop more sophisticated analyses and models that have dependencies on lower-level analyses. The dependent structure makes the need for proper uncertainty handling even more imperative, as without propagating uncertainties the resulting estimates / beliefs will be overly confident. Furthermore, these computational resources do allow for uncertainty propagation without a deep understanding of the underlying mathematics, making it accessible to most researchers with some coding experience. To effectively communicate both the statistical models as well as the underlying uncertainties associated with doing computations on data, the thesis will start by exploring different types of uncertainty. Next the thesis will be investigating a particular cognitive model, the psychometric function, used in many sub fields of cognitive science. This model will be examined and used to show how a novel way of validating cognitive models can be used to gain more information about the structure of the uncertainty in the parameters of the model.

## *Levels of uncertainty and uncertainty propagation*

I will here broadly define 3 different types of uncertainty which are going to be used throughout the thesis, measurement, estimation, and test-retest  uncertainty. These definitions are not exhaustive and will be centered around how experimental studies in cognitive science are conducted, from data collection to data analysis. Before examining these three types of uncertainty it is imperative to acknowledge that uncertainties can be defined in hierarchies and that uncertainty propagates through these hierarchies. This uncertainty propagation means that as calculations are done based on measures with uncertainty, the uncertainty propagates to the results of the calculations. In this thesis I will be using simulations to show how uncertainty propagation can be understood and handled without a need for rigorous mathematical proofs. For a more mathematical treatment see [@saccenti_corruption_2020].

The lowest level of uncertainty is in the measurements themselves i.e. measurement uncertainty. Measurement uncertainty reflects the uncertainty in how well one can for instance measure the reaction time on a computer or the time it took a falling cup to reach the ground. This level of uncertainty is often neglected in cognitive science when applying statistical models, because they are sometimes thought to be minuscule as in the case of reaction time tasks, which may or may not be true given the experiment setup [@holden_accuracy_2019; @crocetta_problem_2015; @ohyanagi_solution_2010]. This is not to say that cognitive scientists do not care about them, as moving towards more sophisticated measurement methods is an ongoing endeavor. For instance, using better and more sophisticated computers to measure reaction times commonly found in cognitive science experiments [@crocetta_problem_2015]. Minimizing this kind of uncertainty most often revolves around getting better tools to measure the variable(s) of interest. 

The next level of uncertainty is when calculations are done on data with uncertainty or when statistical metrics are computed in general, this uncertainty will be referred to as estimation uncertainty. Estimation uncertainty is most often quantified by the statistical model be that the standard error of a correlation coefficient or the width of a posterior distribution of a parameter. This type of uncertainty is always present and is highly influenced by the measurement uncertainty of the data that it is based on. Minimizing estimation uncertainty is partly what scientists care about, as inevitably most cognitive science experiments revolve around null hypothesis testing, which in most cases will involve testing whether the parameter estimate includes a particular value, mostly, 0. Therefore if there is an underlying effect i.e. the parameter is different from zero, then minimizing estimation uncertainty will reveal this effect as being statistically significant. To minimize this type of uncertainty the standard approach is to get more data, given that it comes from the same population and produces similar results. In cognitive science this might include increasing the number of trials or subjects to get a more precise estimate of interest i.e. minimizing estimation uncertainty. This way of minimizing estimation uncertainty is however not free or free of uncertainty itself. Firstly, increasing the number of trials in a cognitive task might even increase the estimation uncertainty itself. This can happen for several reasons, but boredom, habituation, fatigue and lack of engagement can become big contributors when experimental tasks become very long [@meier_is_2024; @jeong_exhaustive_2023]. Secondly for some cognitive science experiments massively increasing the number of trials could make subjects more prone to switching between cognitive strategies and if not properly accounted for in the analysis might be interpreted as additional noise by the model and its parameters. Next increasing the number of subjects included in a study will many times decrease estimation uncertainty on the population level estimates, if the sample population is homogeneous. The trade off between subjects and trials in an experiment is therefore quite important to minimize estimation uncertainty, but also minimize the overuse of resources. Beyond these traditional approaches to minimizing estimation uncertainty there are other ways, for instance changing the task design [@baldi_antognini_new_2023; @stone_using_2014]. This optimization strategy involves individualizing the task design such that each presented stimulus is the most informative it can be. This task design optimization is frequently used in psycho-physical experiments where adaptive algorithms are used to select the upcoming stimuli such that it minimizes the uncertainty in the estimated parameter values. See for example algorithms like PSI, QUEST and ADOPY [@watson_quest_2017; @yang_adopy_2021; @prins_psi-marginal_2013]. The thesis will explore how these algorithms work and can be implemented to minimize estimation uncertainty. 

The next level, and here last level of uncertainty (presented here) stems from the fact that the parameter estimates will vary over time. In cognitive science the extra uncertainty on retesting the estimates of a parameter stems from the fact that humans vary over time. This variation stems from both behavioral factors like learning, but also psychological factors such as mood and arousal [@schurr_dynamic_2024]. This type of uncertainty will be referred to as test-retest uncertainty. 


In order to demonstrate and account for these types of uncertainties, the thesis will here introduce how it is possible to add measurement uncertainty to a correlation analysis. This will be done by using simulations, which therefore does not require high level mathematics, but instead some coding experience. A correlation analysis is chosen as the example for three main reasons. Firstly, a non trivial part of the published litterature in cognitive science revolves around conducting correlational analyses on measures that have quantifiable and considerable uncertainties. These measures typically involve estimated parameters, structural properties of the brain or even reactions times [@wu_neurobiological_2021; @de_berker_computations_2016; @luijcks_influence_2015]. Secondly, the example can be generalized such that instead of being estimating a correlation coefficient on data with measurement uncertainty, it could just as well be estimating a correlation coefficient based on other measures with uncertainty, or even more elaborate models, see supplementary analysis 1 for an example of how these types of uncertainties interact in a linear model, also commonly used in the cognitive science literature. third and lastly, this type of analysis is going to be used extensively throughout the thesis and it serves as a primer for the upcoming sections. 


Firstly we might consider the special case where no or minimal uncertainty is present in the data to understand the uncertainty of the correlation coefficient in a vaccum, i.e. its estimation uncertainty without being influenced by measurement uncertainty. 

Matematical analytical solutions exist to calculate this type of uncertainty and is incorporated in most statistical softwares [@R-correlation], however this can also be shown by simulations or more accuractely by re-sampling.


Below is an explanation of using re-sampling to evaluate uncertainty in a general case and thereafter its implementation for adding measurement uncertainty. The way to estimate the uncertainty in the correlation coefficient is to re-sample the collected data with replacement i.e. bootstrapping  and then recalculate the test statistic of interest [@efron_estimating_1983]. Iterating this process gives a distribution of test statistics which with enough iterations will converge towards the analytic solution with recommendations of having at least 30 data points to begin with [@efron_estimating_1983; @efron_introduction_1994; @wu_jackknife_1986]. For the simplest case of recalculating the correlation coefficient (without measurement uncertainty) and its uncertainty it is somewhat tedious compared to taking the direct analytic solution, as this is already implemented in most statistical softwares and packages [@correlationPackage]. However, once implemented and understood this approach allows for adding and propergating all types of uncertainty coming from many different kinds of distributions. One of the advantages of having an analytic solution to this simple case of recalculating the uncertainty of the correlation coefficient is to ensure that the code and scripts are properly set up. This therefore serves as a validation step before exploring territories where analytic solutions are scarce or nonexistent.


The first step is therefore to show that the two approaches of simulating and analytically estimating the uncertainty of the correlation coefficient is identical across different ranges of correlations and sample sizes. To do this, simulated data from a multivariate normal distribution with the following parameters are produced.


$$
\begin{equation} 
  \begin{pmatrix}
  x_i \\
  y_i 
  \end{pmatrix} \sim \mathcal{N}\left(
  \begin{pmatrix}
  \mu_x \\
  \mu_y 
  \end{pmatrix}, \begin{bmatrix}
  \sigma_x^2 & \sigma_x \cdot \sigma_y \cdot \rho_{xx} \\
  \sigma_x \cdot \sigma_y \cdot \rho_{xx} & \sigma_y^2 
  \end{bmatrix} 
  \right)
\end{equation} 
$$

Where

$$
\mu_x = 50, \quad \mu_y = 100, \quad \Sigma = \begin{bmatrix}
10^2 & 10 \cdot 10 \cdot \rho_{xy} \\
10 \cdot 10 \cdot \rho_{xy} & 10^2 
\end{bmatrix}
$$


The multinormal distribution produces random variables with a means $μ_x$ a standard deviation $σ_x$ and crucially with a correlation coefficient between the random variables $ρ_{xy}$. This distribution is perfect for understanding how the correlation coefficient changes as it is a parameter of the distribution. Now demonstrating that bootstrapping and the analytic solution to the estimation uncertainty implemented in R are identical, is a matter of simulating correlation coefficients ranging from -0.9 to 0.9 in increments of 0.1 with the total number of samples per random variable being between 50 and 500 in increments of 50 [@correlationPackage; @R2024]. See supplementary Figure 1 for demonstration of the similarity of these two approaches.

Having shown that the two approaches are identical (or close to) we can add measurement uncertainty to each observation. To add measurement uncertainty to the measurements we can instead of randomly re-sampling pairs of data points from the original data, as done for the case without measurement uncertainty. Instead one re-samples these pairs as means of an error distribution where the uncertainty (standard deviation) of this distribution is the measurement uncertainty. A mindless choice of error distribution would be the normal distribution which would reflect the fact that the directionality of the uncertainty is assumed to be bidirectional i.e. with no preferred direction. This can be fomulated as the following:

$$
\begin{equation}
\begin{pmatrix}
  \hat{x}_i \\
  \hat{y}_i
\end{pmatrix} \sim \mathcal{N}\left(
\begin{pmatrix}
  x_i \\
  y_i
\end{pmatrix}, \begin{bmatrix}
  m_x^2 & m_x^2\cdot m_y^2\cdot\rho_{m} \\
  m_x^2\cdot m_y^2\cdot\rho_{m} & m_y^2
\end{bmatrix}
\right)
\end{equation}
$$

where $\hat{x}_i$ and $\hat{y}_i$ are the observed estimates of x and y on a particular simulation given their measurement uncertainty $m_x$ and $m_y$ and their correlation between them $\rho_m$. Of note here is that one might re-sample the original data from other error distributions. For instance if values are strictly positive or bounded in other ways then simulating from a truncated normal or strictly positive distributions like a lognormal, would be preferred to avoid sampling values that cannot be obtained i.e. negative reaction time values. 

To demonstrate this type of uncertainty error bars on individual data points is usually depicted see figure 1 A. Here a scatter plot of two types of data is depicted $x_i$ and $y_i$, these might represent different measurements from a cognitive science experiment, say reaction times and time spent awake. For visualization purposes only measurement uncertainty was added to the y-coordinates meaning that from the above equation $m_x = 0$ and $m_y \in \{ 1, 2, \ldots, 10 \}$.Figure 1 B displays how the estimated correlation coefficient distribution obtained by bootstrapping changes based on these measurement uncertainties. It should be noted that the correlation coefficient simulated in this case was 0.8, as indicated by the vertical line in figure 3 (B). it is clear that the estimated correlation coefficient using bootstrapping is being attenuated in size but also that the width of the correlation coefficient distribution is increasing with increasing measurement uncertainty, mimicking what can be shown using the analytical solutions [@saccenti_corruption_2020]. This demonstration serves as a primer for the rest of the thesis, in how uncertainty on the data going into a correlational analysis can shape the correlation coefficient. Here it was shown with normally distributed noise that decreased the size and width of the correlation coefficient, later non trivial types of noise is added where the simulation approach used here is necessary to properly propagate the uncertainty. Note this example does not demonstrate how test re-test uncertainty interacts with these lower levels i.e. measurement and estimation uncertainty, but one can imagine measuring the correlation coefficient from figure 1 twice and getting different results, even if the measurements were infinity precise and there was no uncertainty in the estimation process due to infinity many data-points. For a more concrete and elaborate example supplementary analysis 1.


```{r figure1, fig.width = 7, fig.height = 7, warning = F, message = F, echo = F, fig.cap = "**Figure 1 Measurement uncertainty on correlation coefficient**  (A) Displays a scatterplot with varying amounts of measurement uncertainty. (B) displays how the correlation coefficient distribution, obtained through bootstrapping changes with increasing measurement uncertainty. Vertical line is the simulated correlation coeficient without uncertainty."}
cowplot::ggdraw() +
    cowplot::draw_image(magick::image_read(here::here("Figures","figure_3_measurement_uncertainty.png")), scale = 1)
```

The main message here is that to get reliable estimates and in the end, to make reliable inference one needs to account for sorts of uncertainties and the lower in the hierarchy you move the more fundamental and important they become, when doing higher level calculations with them. Having a parameter estimate that is stable over time will not matter if you cannot estimate it, or measure it, reliably in the first place. For a complete set of scripts and parameters used for above demonstration see [github](https://github.com/JesperFischer/Master-thesis/blob/main/plot%20scripts/Measurement%20error%20visualization.Rmd).

